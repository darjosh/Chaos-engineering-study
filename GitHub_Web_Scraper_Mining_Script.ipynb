{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2bd3ef-cfba-4a9b-977e-65572b445f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install requests\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install pandas\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb339445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e46232d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c6811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>#stars on Github</th>\n",
       "      <th>#forks on Github</th>\n",
       "      <th>Year</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>search string</th>\n",
       "      <th>Repository Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>Great Expectations</td>\n",
       "      <td>8000</td>\n",
       "      <td>1200</td>\n",
       "      <td>2019</td>\n",
       "      <td>what are the most popular data quality test to...</td>\n",
       "      <td>filename:great_expectations extension:.yml</td>\n",
       "      <td>https://github.com/great-expectations/great_ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>deequ-pydeequ</td>\n",
       "      <td>2665</td>\n",
       "      <td>470</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>filename:deequ extension:.jar,deequ extension:...</td>\n",
       "      <td>https://github.com/awslabs/deequ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>Pandera</td>\n",
       "      <td>1978</td>\n",
       "      <td>162</td>\n",
       "      <td>2018</td>\n",
       "      <td>what are the most popular data quality test to...</td>\n",
       "      <td>pandera extension:.py,pandera extension:.ipynb</td>\n",
       "      <td>https://github.com/pandera-dev/pandera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>Apache Griffin</td>\n",
       "      <td>999</td>\n",
       "      <td>585</td>\n",
       "      <td>2017</td>\n",
       "      <td>what are the most popular data quality test to...</td>\n",
       "      <td>apachegriffin extension:.yml</td>\n",
       "      <td>https://github.com/apache/griffin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>TensorFlow Data Validation</td>\n",
       "      <td>699</td>\n",
       "      <td>150</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tensorflow_data_validation extension:.py</td>\n",
       "      <td>https://github.com/tensorflow/data-validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>DBT</td>\n",
       "      <td>6600</td>\n",
       "      <td>1200</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test filename:dbt_project extension:.yml</td>\n",
       "      <td>https://github.com/dbt-labs/dbt-core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source                        Name  #stars on Github  #forks on Github  \\\n",
       "0  GitHub          Great Expectations              8000              1200   \n",
       "1  GitHub               deequ-pydeequ              2665               470   \n",
       "2  GitHub                     Pandera              1978               162   \n",
       "3  GitHub              Apache Griffin               999               585   \n",
       "4  GitHub  TensorFlow Data Validation               699               150   \n",
       "5  GitHub                         DBT              6600              1200   \n",
       "\n",
       "   Year                                            Keyword  \\\n",
       "0  2019  what are the most popular data quality test to...   \n",
       "1  2018                                                NaN   \n",
       "2  2018  what are the most popular data quality test to...   \n",
       "3  2017  what are the most popular data quality test to...   \n",
       "4  2018                                                NaN   \n",
       "5  2021                                                NaN   \n",
       "\n",
       "                                       search string  \\\n",
       "0         filename:great_expectations extension:.yml   \n",
       "1  filename:deequ extension:.jar,deequ extension:...   \n",
       "2     pandera extension:.py,pandera extension:.ipynb   \n",
       "3                       apachegriffin extension:.yml   \n",
       "4           tensorflow_data_validation extension:.py   \n",
       "5           test filename:dbt_project extension:.yml   \n",
       "\n",
       "                                     Repository Link  \n",
       "0  https://github.com/great-expectations/great_ex...  \n",
       "1                   https://github.com/awslabs/deequ  \n",
       "2             https://github.com/pandera-dev/pandera  \n",
       "3                  https://github.com/apache/griffin  \n",
       "4      https://github.com/tensorflow/data-validation  \n",
       "5               https://github.com/dbt-labs/dbt-core  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5761da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cad58-19df-463e-a858-5d27dd0ce2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93150b1c-d205-451e-8f3b-a91e4f900ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e476385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tool_list_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m login_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSign in\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9C\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m93\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogin\u001b[39m\u001b[38;5;124m'\u001b[39m: email,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m: password\n\u001b[0;32m     16\u001b[0m }\n\u001b[0;32m     18\u001b[0m all_tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(tool_list_dict\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m5\u001b[39m]]:\n\u001b[0;32m     20\u001b[0m     session \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m session \u001b[38;5;28;01mas\u001b[39;00m s:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tool_list_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f3dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45e6a69f-ab3a-4cca-b95a-8f99d7eb222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69444efc-ffd2-4714-ac65-df1ac90da58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tool: ChaosMonkey\n",
      "Searching for: chaosmonkey\n",
      "Found 0 results for chaosmonkey\n",
      "No repositories found. Creating an empty DataFrame.\n",
      "Results saved to tool-repo-list-ChaosMonkey.xlsx\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8520208-df0e-46dc-85a7-993ddd915a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search URL: https://github.com/search?q=chaosmonkey&type=code\n",
      "Response Status: 200\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc74f53-b82a-4382-992e-ea5937ff122d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b229109-a5a4-4da6-b523-bb7002b45d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tool: Chaos Monkey\n",
      "Searching for: chaosmonkey\n",
      "Search URL: https://github.com/search?q=chaosmonkey&type=repositories&p=1\n",
      "Response Status: 200\n",
      "Found 0 repositories on page 1\n",
      "Results saved to tool-repo-list-ChaosMonkey.xlsx\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c13955b-fcb2-4124-a296-785b57b7158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tool: Chaos Monkey\n",
      "Searching for: chaosmonkey\n",
      "Search URL: https://github.com/search?q=chaosmonkey&type=repositories&p=1\n",
      "Response Status: 200\n",
      "Found 0 repositories on page 1\n",
      "Results saved to tool-repo-list-ChaosMonkey.xlsx\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d560d6d8-e025-4f7a-8f35-7d993385e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tool: Chaos Monkey\n",
      "Searching for: chaosmonkey\n",
      "Search URL: https://github.com/search?q=chaosmonkey&type=repositories&p=1\n",
      "Response Status: 200\n",
      "Found 0 repositories on page 1\n",
      "Results saved to tool-repo-list-ChaosMonkey.xlsx\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bdff8c90-4c4c-4e59-8a95-2525b85bf3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/9.7 MB 2.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/9.7 MB 4.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/9.7 MB 4.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/9.7 MB 6.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/9.7 MB 7.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/9.7 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.6/9.7 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.1/9.7 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.6/9.7 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.0/9.7 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/9.7 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.4/9.7 MB 9.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.8/9.7 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.3/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.8/9.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.2/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.1/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.6/9.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.7 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.7 kB ? eta -:--:--\n",
      "   ------------------------------- ------- 389.1/481.7 kB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 481.7/481.7 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.0/63.0 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: wsproto, websocket-client, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-24.2.0 outcome-1.3.0.post0 selenium-4.27.1 trio-0.27.0 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec0b1856-0ad3-415a-be2e-0bc8dae0a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 100 repositories on page 2\n",
      "Fetched 100 repositories on page 3\n",
      "Fetched 100 repositories on page 4\n",
      "Fetched 100 repositories on page 5\n",
      "Fetched 6 repositories on page 6\n",
      "Results saved to tool-repo-list-ChaosMonkey.xlsx\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "308832a4-3eb9-4fb3-b590-338af07a135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 87 repositories on page 1\n",
      "Results saved to tool-repo-list-ChaosMonkey-with-details.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URL\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"chaosmonkey\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        all_repos.append({\n",
    "            \"name\": repo[\"name\"],\n",
    "            \"url\": repo[\"html_url\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"description\": repo.get(\"description\", \"No description\")\n",
    "        })\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    \n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Save to Excel\n",
    "df = pd.DataFrame(all_repos)\n",
    "output_filename = \"tool-repo-list-ChaosMonkey-with-details.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76e3af26-33d0-4624-8ad1-d30a29cb403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 100 repositories on page 2\n",
      "Fetched 100 repositories on page 3\n",
      "Fetched 100 repositories on page 4\n",
      "Fetched 100 repositories on page 5\n",
      "Fetched 6 repositories on page 6\n",
      "Results saved to tool-repo-list-ChaosMonkey-detailed.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"chaos monkey\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"]\n",
    "        }\n",
    "\n",
    "        # Additional API requests for detailed information\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        # Contributors\n",
    "        contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "        if contributors_response.status_code == 200:\n",
    "            contributors = contributors_response.json()\n",
    "            repo_details[\"contributers\"] = len(contributors)\n",
    "        else:\n",
    "            repo_details[\"contributers\"] = \"N/A\"\n",
    "\n",
    "        # All Languages\n",
    "        languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "        if languages_response.status_code == 200:\n",
    "            languages = languages_response.json()\n",
    "            repo_details[\"allLanguages\"] = \", \".join(languages.keys())\n",
    "        else:\n",
    "            repo_details[\"allLanguages\"] = \"N/A\"\n",
    "\n",
    "        # Commits (First and Last Commit, Total Commits)\n",
    "        commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 1})\n",
    "        if commits_response.status_code == 200:\n",
    "            commits_data = commits_response.json()\n",
    "            if commits_data:\n",
    "                repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "                # Fetch first commit (pagination required)\n",
    "                first_commit_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 1, \"page\": 9999})\n",
    "                if first_commit_response.status_code == 200:\n",
    "                    first_commit_data = first_commit_response.json()\n",
    "                    repo_details[\"firstCommit\"] = first_commit_data[0][\"commit\"][\"committer\"][\"date\"] if first_commit_data else \"N/A\"\n",
    "                else:\n",
    "                    repo_details[\"firstCommit\"] = \"N/A\"\n",
    "            else:\n",
    "                repo_details[\"lastCommit\"] = \"N/A\"\n",
    "                repo_details[\"firstCommit\"] = \"N/A\"\n",
    "        else:\n",
    "            repo_details[\"lastCommit\"] = \"N/A\"\n",
    "            repo_details[\"firstCommit\"] = \"N/A\"\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Save to Excel\n",
    "df = pd.DataFrame(all_repos)\n",
    "output_filename = \"tool-repo-list-ChaosMonkey-detailed.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fc634c7-2f64-4b59-8792-5df06d61ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1 repositories on page 1\n",
      "Results saved to tool-repo-detailed.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"ChatAbstractions\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"]\n",
    "        }\n",
    "\n",
    "        # Commits: Fetch total commits, first commit, and last commit\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "        commits_url = commits_url_template.format(owner=owner, repo=repo_name)\n",
    "\n",
    "        # Fetch commits\n",
    "        try:\n",
    "            # Fetch the first page of commits\n",
    "            commits_response = requests.get(commits_url, headers=headers, params={\"per_page\": 1, \"page\": 1})\n",
    "            if commits_response.status_code == 200:\n",
    "                commits_data = commits_response.json()\n",
    "                if commits_data:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                # Fetch the last page of commits to get the first commit\n",
    "                first_commit_response = requests.get(commits_url, headers=headers, params={\"per_page\": 1, \"page\": 9999})\n",
    "                if first_commit_response.status_code == 200:\n",
    "                    first_commit_data = first_commit_response.json()\n",
    "                    if first_commit_data:\n",
    "                        repo_details[\"firstCommit\"] = first_commit_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "                    else:\n",
    "                        repo_details[\"firstCommit\"] = \"N/A\"\n",
    "                else:\n",
    "                    repo_details[\"firstCommit\"] = \"N/A\"\n",
    "\n",
    "                # Fetch total commits count\n",
    "                commits_count_response = requests.get(repo[\"url\"], headers=headers)\n",
    "                if commits_count_response.status_code == 200:\n",
    "                    repo_details[\"numberOfCommits\"] = commits_count_response.json().get(\"size\", \"N/A\")\n",
    "                else:\n",
    "                    repo_details[\"numberOfCommits\"] = \"N/A\"\n",
    "            else:\n",
    "                repo_details[\"lastCommit\"] = \"N/A\"\n",
    "                repo_details[\"firstCommit\"] = \"N/A\"\n",
    "                repo_details[\"numberOfCommits\"] = \"N/A\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching commit data for {repo_name}: {e}\")\n",
    "            repo_details[\"lastCommit\"] = \"N/A\"\n",
    "            repo_details[\"firstCommit\"] = \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = \"N/A\"\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Save to Excel\n",
    "df = pd.DataFrame(all_repos)\n",
    "output_filename = \"tool-repo-detailed.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f89f0b8-4259-42a5-9a4d-399b4daba627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1 repositories on page 1\n",
      "Results saved to cc.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"ChatAbstractions\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"N/A\",\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = \", \".join(languages.keys())\n",
    "\n",
    "            # Fetch last and first commit\n",
    "            page = 1\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"cc.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa12a0f9-b71c-4b37-abe7-b2298ee6134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 14 repositories on page 1\n",
      "Results saved to cccc.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"kube-monkey\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": [],\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = list(languages.keys())  # Store as a list of languages\n",
    "\n",
    "            # Fetch last and first commit\n",
    "            page = 1\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"cccc.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0714c7c0-d11d-473f-b36d-3eb464cefa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7553bc25-9f99-4395-88ed-5cb7a2316341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 repositories on page 1\n",
      "Results saved to chaos-mesh-3b.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"chaosmesh\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaos-mesh-3b.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0aeb132b-5d68-426c-9399-bcb9f263dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 75 repositories on page 1\n",
      "Results saved to chaosblade-4.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"chaosblade\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaosblade-4.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "80298a1c-03f3-4272-ba1e-80ab2c6252ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 72 repositories on page 1\n",
      "Results saved to litmuschaos-5a.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"litmus_chaos\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"litmuschaos-5a.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2bb6534d-7457-475c-a8e9-73c8aa85edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 100 repositories on page 2\n",
      "Fetched 98 repositories on page 3\n",
      "Results saved to pumba-6.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"pumba\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"pumba-6.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1df068-08a4-44f8-9852-594378def9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 85 repositories on page 2\n",
      "Results saved to chaostoolkit-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"chaostoolkit\" OR \"chaos toolkit\" OR \"chaos_toolkit\" OR \"chaos-toolkit\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaostoolkit-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aca4122c-f353-4251-96ef-a317758eb5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 6 repositories on page 1\n",
      "Results saved to powerfulseal-8.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"powerfulseal\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"powerfulseal-8.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d212ee6d-e48b-4226-9a89-6b3bcf96b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 8 repositories on page 1\n",
      "Results saved to chaoskube-9b.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"chaoskube\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaoskube-9b.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5a83cc7-5339-4a0d-a4e0-32a41a7dd773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 14 repositories on page 1\n",
      "Results saved to kube_monkey2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"kube_monkey\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"kube_monkey2.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9c7c9-47b3-4fe9-8816-1b2fa7f018d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\"q\": \"chaoskube\", \"per_page\": 100, \"page\": 1}\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaoskube.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43a664-0d9f-49fa-9836-6adbe8fec5a6",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eccdfa3-e435-435f-8c7a-54dd7dcb6844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to chaostoolkitt-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"litmuschaos extension:yaml\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaostoolkitt-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3814c2a3-905e-4700-bf41-eb230211a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 100 repositories on page 2\n",
      "Fetched 100 repositories on page 3\n",
      "Fetched 100 repositories on page 4\n",
      "Fetched 47 repositories on page 5\n",
      "Results saved to chaosmonkey-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": ' \"chaosmonkey\" OR  \"chaos_monkey\" OR \"chaos-monkey\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaosmonkey-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae58f8b-b28a-409e-8d97-1f46f0063c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"chaos_kube\" OR \"chaoskube\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaoskube-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61d0923-aa97-442b-9c61-0c3edcb97a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 6 repositories on page 1\n",
      "Results saved to powerfulseal-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"powerfulseal\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"powerfulseal-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0650027-aeb3-4c34-94d8-717990bbe89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"chaosmonkey\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"pumba-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3926484f-fd46-4cf9-bff0-876cd799f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 21 repositories on page 1\n",
      "Results saved to kubemonkey-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"kubemonkey\" OR \"kube_monkey\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"kubemonkey-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ac4bd3-aa2d-45aa-ac9b-0d0e34e22f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 23 repositories on page 2\n",
      "Results saved to litmuschaos-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": ' \"litmuschaos\" OR \"litmus_chaos\" OR \"litmus-chaos\" OR \"litmus chaos\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"litmuschaos-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86672fc-8126-4c09-ad18-c4c05142df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 78 repositories on page 1\n",
      "Results saved to chaosblade-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"chaosblade\" OR \"chaos-blade\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaosblade-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af7ac2f-64d2-4376-88ab-3e9d66e27da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 21 repositories on page 2\n",
      "Results saved to chaosmesh-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": ' \"chaos mesh\" OR \"chaos-mesh\" OR \"chaos_mesh\" OR \"chaosmesh\" ',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaosmesh-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5843a417-5388-4e6c-9cf4-95ce9838b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories on page 1\n",
      "Fetched 100 repositories on page 2\n",
      "Fetched 100 repositories on page 3\n",
      "Fetched 100 repositories on page 4\n",
      "Fetched 47 repositories on page 5\n",
      "Results saved to chaosmonkey-new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": ' \"chaosmonkey\" OR  \"chaos_monkey\" OR \"chaos-monkey\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaosmonkey-new.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33da33e2-434a-408d-bbf7-46e17096a40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 93 repositories on page 1\n",
      "Results saved to toxiproxy.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"toxiproxy\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"toxiproxy.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6170db6f-bf4b-4459-8353-e8d9ad3f0256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to chaos_test.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"t {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "# params = {\"q\": \"chaostoolkit\", \"per_page\": 100, \"page\": 1}\n",
    "# params = {\"q\": '\"chaostoolkit\" OR \"chaos toolkit\"', \"per_page\": 100, \"page\": 1}\n",
    "params = {\n",
    "    \"q\": '\"chaosmesh filename:.yaml\"',\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        # Basic repository details\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",  # Placeholder\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(commits_url_template.format(owner=owner, repo=repo_name), headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaos_test.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f375011-b88d-4c2d-969d-09b649baba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 repositories on page 1\n",
      "No repositories found on page 2.\n",
      "Results saved to chaos_test.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API URLs\n",
    "search_url = \"https://api.github.com/search/repositories\"\n",
    "contributors_url_template = \"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "languages_url_template = \"https://api.github.com/repos/{owner}/{repo}/languages\"\n",
    "commits_url_template = \"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "\n",
    "# Authentication (personal access t recommended)\n",
    "t = ''\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search parameters\n",
    "params = {\n",
    "    \"q\": \"chaosmesh in:path.yaml\",\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "all_repos = []\n",
    "\n",
    "def handle_rate_limit(response):\n",
    "    \"\"\"Handle GitHub rate-limiting by waiting until the reset time.\"\"\"\n",
    "    if response.status_code == 403 and \"X-RateLimit-Reset\" in response.headers:\n",
    "        reset_time = int(response.headers[\"X-RateLimit-Reset\"])\n",
    "        wait_time = reset_time - int(time.time())\n",
    "        if wait_time > 0:\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds.\")\n",
    "            time.sleep(wait_time + 1)\n",
    "\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    \n",
    "    # Handle rate-limiting\n",
    "    if response.status_code == 403:\n",
    "        handle_rate_limit(response)\n",
    "        continue\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    repos = data.get(\"items\", [])\n",
    "    if not repos:\n",
    "        print(f\"No repositories found on page {params['page']}.\")\n",
    "        break\n",
    "\n",
    "    for repo in repos:\n",
    "        repo_details = {\n",
    "            \"repositoryID\": repo[\"id\"],\n",
    "            \"repositoryName\": repo[\"name\"],\n",
    "            \"ownerLogin\": repo[\"owner\"][\"login\"],\n",
    "            \"ownerType\": repo[\"owner\"][\"type\"],\n",
    "            \"repositoryDescription\": repo.get(\"description\", \"No description\"),\n",
    "            \"topics\": \", \".join(repo.get(\"topics\", [])),\n",
    "            \"URL\": repo[\"html_url\"],\n",
    "            \"license\": repo[\"license\"][\"name\"] if repo[\"license\"] else \"No license\",\n",
    "            \"primaryLanguage\": repo[\"language\"],\n",
    "            \"repositoryCreation\": repo[\"created_at\"],\n",
    "            \"repositoryLastUpdate\": repo[\"updated_at\"],\n",
    "            \"stars\": repo[\"stargazers_count\"],\n",
    "            \"forks\": repo[\"forks_count\"],\n",
    "            \"issues\": repo[\"open_issues_count\"],\n",
    "            \"watchers\": repo[\"watchers_count\"],\n",
    "            \"contributors\": 0,\n",
    "            \"allLanguages\": \"[]\",\n",
    "            \"lastCommit\": \"N/A\",\n",
    "            \"firstCommit\": \"N/A\",\n",
    "            \"numberOfCommits\": \"N/A\"\n",
    "        }\n",
    "\n",
    "        # Fetch additional details\n",
    "        owner = repo[\"owner\"][\"login\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "\n",
    "        try:\n",
    "            # Fetch contributors\n",
    "            contributors_response = requests.get(contributors_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if contributors_response.status_code == 200:\n",
    "                contributors = contributors_response.json()\n",
    "                repo_details[\"contributors\"] = len(contributors)\n",
    "\n",
    "            # Fetch all languages\n",
    "            languages_response = requests.get(languages_url_template.format(owner=owner, repo=repo_name), headers=headers)\n",
    "            if languages_response.status_code == 200:\n",
    "                languages = languages_response.json()\n",
    "                repo_details[\"allLanguages\"] = str(list(languages.keys()))\n",
    "\n",
    "            # Fetch last and first commit and total commits\n",
    "            page = 1\n",
    "            total_commits = 0\n",
    "            first_commit_date = None\n",
    "            while True:\n",
    "                commits_response = requests.get(\n",
    "                    commits_url_template.format(owner=owner, repo=repo_name),\n",
    "                    headers=headers,\n",
    "                    params={\"per_page\": 100, \"page\": page}\n",
    "                )\n",
    "                if commits_response.status_code == 403:\n",
    "                    handle_rate_limit(commits_response)\n",
    "                    continue\n",
    "\n",
    "                if commits_response.status_code != 200 or not commits_response.json():\n",
    "                    break\n",
    "\n",
    "                commits_data = commits_response.json()\n",
    "                if page == 1:\n",
    "                    repo_details[\"lastCommit\"] = commits_data[0][\"commit\"][\"committer\"][\"date\"]\n",
    "\n",
    "                first_commit_date = commits_data[-1][\"commit\"][\"committer\"][\"date\"]\n",
    "                total_commits += len(commits_data)\n",
    "                page += 1\n",
    "\n",
    "            repo_details[\"firstCommit\"] = first_commit_date if first_commit_date else \"N/A\"\n",
    "            repo_details[\"numberOfCommits\"] = total_commits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching additional data for {repo_name}: {e}\")\n",
    "\n",
    "        # Append to results\n",
    "        all_repos.append(repo_details)\n",
    "\n",
    "    print(f\"Fetched {len(repos)} repositories on page {params['page']}\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Arrange fields in the specified order\n",
    "df = pd.DataFrame(all_repos, columns=[\n",
    "    \"repositoryID\", \"repositoryName\", \"ownerLogin\", \"ownerType\", \"repositoryDescription\",\n",
    "    \"topics\", \"URL\", \"license\", \"primaryLanguage\", \"allLanguages\", \"repositoryCreation\",\n",
    "    \"repositoryLastUpdate\", \"contributors\", \"watchers\", \"stars\", \"forks\", \"issues\",\n",
    "    \"lastCommit\", \"firstCommit\", \"numberOfCommits\"\n",
    "])\n",
    "\n",
    "# Save to Excel\n",
    "output_filename = \"chaos_test.xlsx\"\n",
    "df.to_excel(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238367e-c63c-4b76-a964-d1765303ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4d102bd-d1fc-464c-89fb-340ac91fb346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 6 results on page 1.\n",
      "No more results found.\n",
      "Results saved to code_files_search.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub API URL for Code Search\n",
    "search_url = \"https://api.github.com/search/code\"\n",
    "\n",
    "# Authentication (Personal Access t)\n",
    "t = \"\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Search query\n",
    "query = 'path:helm/chaos-mesh filename:values.yaml OR filename:Chart.yaml'\n",
    "params = {\n",
    "    \"q\": query,\n",
    "    \"per_page\": 100,\n",
    "    \"page\": 1\n",
    "}\n",
    "\n",
    "# Fetch results\n",
    "all_results = []\n",
    "\n",
    "while True:\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    items = data.get(\"items\", [])\n",
    "    \n",
    "    if not items:\n",
    "        print(\"No more results found.\")\n",
    "        break\n",
    "\n",
    "    for item in items:\n",
    "        result = {\n",
    "            \"Repository\": item[\"repository\"][\"full_name\"],\n",
    "            \"File Name\": item[\"name\"],\n",
    "            \"File Path\": item[\"path\"],\n",
    "            \"File URL\": item[\"html_url\"],\n",
    "            \"Repository URL\": item[\"repository\"][\"html_url\"]\n",
    "        }\n",
    "        all_results.append(result)\n",
    "\n",
    "    print(f\"Fetched {len(items)} results on page {params['page']}.\")\n",
    "    params[\"page\"] += 1\n",
    "\n",
    "# Save results to Excel\n",
    "if all_results:\n",
    "    df = pd.DataFrame(all_results)\n",
    "    output_filename = \"code_files_search.xlsx\"\n",
    "    df.to_excel(output_filename, index=False)\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "else:\n",
    "    print(\"No matching files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3337be9e-d5f3-4c48-a35f-d402d4a91d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "466300cf-5494-4047-8e85-61f727b27aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: 30 results\n",
      "Page 2: 30 results\n",
      "Page 3: 30 results\n",
      "Page 4: 30 results\n",
      "Page 5: 30 results\n",
      "Data saved to chaos_tools_repositories.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    " # Replace with your GitHub t\n",
    "\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\" \n",
    "HEADERS = {\"Authorization\": f\"Bearer {t}\"}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=30, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"per_page\": per_page,\n",
    "            \"page\": page,\n",
    "        }\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch repository metadata\n",
    "def fetch_repo_metadata(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch metadata for {owner}/{repo}\")\n",
    "        return {}\n",
    "\n",
    "# Search for ChaosMonkey repositories\n",
    "query = \"filename:chaosmonkey extension:java OR filename:chaosmonkey extension:groovy\"\n",
    "\n",
    "# query = \"com.netflix.simianarmy in:file\"\n",
    "results = search_github(query, per_page=30, max_pages=5)\n",
    "\n",
    "# Collect repository metadata\n",
    "repositories = []\n",
    "for item in results:\n",
    "    repo_owner = item['repository']['owner']['login']\n",
    "    repo_name = item['repository']['name']\n",
    "    metadata = fetch_repo_metadata(repo_owner, repo_name)\n",
    "    repositories.append({\n",
    "        \"name\": metadata.get(\"name\"),\n",
    "        \"description\": metadata.get(\"description\"),\n",
    "        \"url\": metadata.get(\"html_url\"),\n",
    "        \"stars\": metadata.get(\"stargazers_count\"),\n",
    "        \"forks\": metadata.get(\"forks_count\"),\n",
    "        \"language\": metadata.get(\"language\"),\n",
    "        \"created_at\": metadata.get(\"created_at\"),\n",
    "        \"updated_at\": metadata.get(\"updated_at\"),\n",
    "    })\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_tools_repositories.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3bd7310d-4f79-4822-b1e6-8497f09b1feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting: https://api.github.com/search/code with params: {'q': 'chaoskube filename:makefile', 'per_page': 10, 'page': 1}\n",
      "Page 1: 10 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'chaoskube filename:makefile', 'per_page': 10, 'page': 2}\n",
      "Page 2: 10 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'chaoskube filename:makefile', 'per_page': 10, 'page': 3}\n",
      "Page 3: 10 results\n",
      "Fetching details for freebsd/freebsd-ports\n",
      "Fetching details for linki/chaoskube\n",
      "Fetching details for DragonFlyBSD/DPorts\n",
      "Fetching details for Zirias/zfbsd-ports\n",
      "Fetching details for pfsense/FreeBSD-ports\n",
      "Fetching details for Jehops/freebsd-ports-legacy\n",
      "Fetching details for bsdlabs/ports\n",
      "Fetching details for notsuoholi/note-taker\n",
      "Fetching details for freebsd/freebsd-ports-gnome\n",
      "Fetching details for freebsd/freebsd-ports\n",
      "Fetching details for yzgyyang/freebsd-ports\n",
      "Fetching details for evadot/freebsd-ports\n",
      "Fetching details for Jehops/freebsd-ports-legacy\n",
      "Fetching details for truenas/ports\n",
      "Fetching details for patmaddox/test-subtree\n",
      "Fetching details for ghostbsd/ghostbsd-ports\n",
      "Fetching details for lgcosta/pfsense-ports\n",
      "Fetching details for bsdlabs/ports\n",
      "Fetching details for HardenedBSD/ports\n",
      "Fetching details for freebsd/freebsd-ports-gnome\n",
      "Fetching details for Zirias/zfbsd-ports\n",
      "Fetching details for pfsense/FreeBSD-ports\n",
      "Fetching details for lgcosta/pfsense-ports\n",
      "Fetching details for opnsense/ports\n",
      "Fetching details for freebsd/freebsd-ports-kde\n",
      "Fetching details for ghostbsd/ghostbsd-ports\n",
      "Fetching details for HardenedBSD/ports\n",
      "Fetching details for truenas/ports\n",
      "Fetching details for yzgyyang/freebsd-ports\n",
      "Fetching details for evadot/freebsd-ports\n",
      "Data saved to mum.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        \n",
    "        # Debugging: Print the URL and query parameters\n",
    "        print(f\"Requesting: {url} with params: {params}\")\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    # Fetch basic repo details\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json()\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    # Return collected data\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Search for repositories with ChaosMonkey\n",
    "#query = \"filename:chaosmonkey extension:yml OR filename:chaosmonkey extension:toml\"\n",
    "\n",
    "#query = \"filename:chaosmonkey extension:go\"\n",
    "\n",
    "query = \"chaoskube filename:makefile\"\n",
    "\n",
    "\n",
    "\n",
    "results = search_github(query, per_page=10, max_pages=3)\n",
    "\n",
    "# Collect metadata\n",
    "repositories = []\n",
    "for item in results:\n",
    "    owner = item['repository']['owner']['login']\n",
    "    repo = item['repository']['name']\n",
    "    print(f\"Fetching details for {owner}/{repo}\")\n",
    "    repo_details = fetch_repository_details(owner, repo)\n",
    "    if repo_details:\n",
    "        repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"mum.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e93e48e1-5c8b-4fbd-9c52-a9a487f1b05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting: https://api.github.com/search/code with params: {'q': 'chaoskube filename:makefile', 'per_page': 100, 'page': 1}\n",
      "Page 1: 33 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'chaoskube filename:makefile', 'per_page': 100, 'page': 2}\n",
      "Fetching details for freebsd/freebsd-ports\n",
      "Fetching details for linki/chaoskube\n",
      "Fetching details for DragonFlyBSD/DPorts\n",
      "Fetching details for Zirias/zfbsd-ports\n",
      "Fetching details for pfsense/FreeBSD-ports\n",
      "Fetching details for Jehops/freebsd-ports-legacy\n",
      "Fetching details for bsdlabs/ports\n",
      "Fetching details for notsuoholi/note-taker\n",
      "Fetching details for freebsd/freebsd-ports-gnome\n",
      "Fetching details for freebsd/freebsd-ports\n",
      "Fetching details for yzgyyang/freebsd-ports\n",
      "Fetching details for evadot/freebsd-ports\n",
      "Fetching details for Jehops/freebsd-ports-legacy\n",
      "Fetching details for truenas/ports\n",
      "Fetching details for patmaddox/test-subtree\n",
      "Fetching details for ghostbsd/ghostbsd-ports\n",
      "Fetching details for lgcosta/pfsense-ports\n",
      "Fetching details for bsdlabs/ports\n",
      "Fetching details for HardenedBSD/ports\n",
      "Fetching details for freebsd/freebsd-ports-gnome\n",
      "Fetching details for Zirias/zfbsd-ports\n",
      "Fetching details for pfsense/FreeBSD-ports\n",
      "Fetching details for lgcosta/pfsense-ports\n",
      "Fetching details for opnsense/ports\n",
      "Fetching details for freebsd/freebsd-ports-kde\n",
      "Fetching details for ghostbsd/ghostbsd-ports\n",
      "Fetching details for HardenedBSD/ports\n",
      "Fetching details for truenas/ports\n",
      "Fetching details for yzgyyang/freebsd-ports\n",
      "Fetching details for evadot/freebsd-ports\n",
      "Fetching details for patmaddox/test-subtree\n",
      "Fetching details for opnsense/ports\n",
      "Fetching details for freebsd/freebsd-ports-kde\n",
      "Data saved to c1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        \n",
    "        # Debugging: Print the URL and query parameters\n",
    "        print(f\"Requesting: {url} with params: {params}\")\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    \n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    # Fetch basic repo details\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    # all_languages = \", \".join(languages_response.json().keys())\n",
    "    all_languages = list(languages_response.json().keys())  # Changed to list\n",
    "\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json()\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    # Return collected data\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Search for repositories with ChaosMonkey or ChaosMesh\n",
    "query = \"filename:chaoskube \"  # Replace with your search query\n",
    "\n",
    "# Fetch results from GitHub\n",
    "results = search_github(query, per_page=100, max_pages=10)\n",
    "\n",
    "# Collect metadata\n",
    "repositories = []\n",
    "for item in results:\n",
    "    owner = item['repository']['owner']['login']\n",
    "    repo = item['repository']['name']\n",
    "    print(f\"Fetching details for {owner}/{repo}\")\n",
    "    repo_details = fetch_repository_details(owner, repo)\n",
    "    if repo_details:\n",
    "        repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"c1.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fb89f4-dd04-41a4-8a67-89c398bae9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml', 'per_page': 100, 'page': 1}\n",
      "Page 1: 48 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml', 'per_page': 100, 'page': 2}\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for openebs-archive/e2e-tests\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for chaosiq/chaosiq\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for VadimShtukan/otus_homework\n",
      "Fetching details for chaosiq/chaosiq\n",
      "Fetching details for ledgifi/currencies\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for openebs/dynamic-localpv-provisioner\n",
      "Fetching details for openebs-archive/cstor-operators\n",
      "Fetching details for openebs-archive/jiva-operator\n",
      "Fetching details for abhinavjha126/openebs\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Data saved to re11.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        \n",
    "        # Debugging: Print the URL and query parameters\n",
    "        print(f\"Requesting: {url} with params: {params}\")\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    \n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    # Fetch basic repo details\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json()\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    # Return collected data\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Search for repositories with ChaosMonkey or ChaosMesh\n",
    "#query = \"chaos filename:pyproject extension:.toml\"  # Replace with your search query\n",
    "# query = \"chaos-mesh filename:makefile\"\n",
    "#query = \"litmus filename:litmus-installation extension:.yaml\" \n",
    "query = \"pumba filename:pumba extension:.yaml\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fetch results from GitHub\n",
    "results = search_github(query, per_page=100, max_pages=10)\n",
    "\n",
    "# Collect metadata\n",
    "repositories = []\n",
    "for item in results:\n",
    "    owner = item['repository']['owner']['login']\n",
    "    repo = item['repository']['name']\n",
    "    print(f\"Fetching details for {owner}/{repo}\")\n",
    "    repo_details = fetch_repository_details(owner, repo)\n",
    "    if repo_details:\n",
    "        repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"re11.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f40ad3-0b69-489d-a410-4d49a02be071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7d6bd2-85e3-4955-9ef9-0b1f51607d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>#stars on Github</th>\n",
       "      <th>#forks on Github</th>\n",
       "      <th>Year</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>search string</th>\n",
       "      <th>Repository Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>Great Expectations</td>\n",
       "      <td>8000</td>\n",
       "      <td>1200</td>\n",
       "      <td>2019</td>\n",
       "      <td>what are the most popular data quality test to...</td>\n",
       "      <td>filename:great_expectations extension:.yml</td>\n",
       "      <td>https://github.com/great-expectations/great_ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>deequ-pydeequ</td>\n",
       "      <td>2665</td>\n",
       "      <td>470</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>filename:deequ extension:.jar,deequ extension:...</td>\n",
       "      <td>https://github.com/awslabs/deequ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>Pandera</td>\n",
       "      <td>1978</td>\n",
       "      <td>162</td>\n",
       "      <td>2018</td>\n",
       "      <td>what are the most popular data quality test to...</td>\n",
       "      <td>pandera extension:.py,pandera extension:.ipynb</td>\n",
       "      <td>https://github.com/pandera-dev/pandera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>Apache Griffin</td>\n",
       "      <td>999</td>\n",
       "      <td>585</td>\n",
       "      <td>2017</td>\n",
       "      <td>what are the most popular data quality test to...</td>\n",
       "      <td>apachegriffin extension:.yml</td>\n",
       "      <td>https://github.com/apache/griffin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>TensorFlow Data Validation</td>\n",
       "      <td>699</td>\n",
       "      <td>150</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tensorflow_data_validation extension:.py</td>\n",
       "      <td>https://github.com/tensorflow/data-validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GitHub</td>\n",
       "      <td>DBT</td>\n",
       "      <td>6600</td>\n",
       "      <td>1200</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test filename:dbt_project extension:.yml</td>\n",
       "      <td>https://github.com/dbt-labs/dbt-core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source                        Name  #stars on Github  #forks on Github  \\\n",
       "0  GitHub          Great Expectations              8000              1200   \n",
       "1  GitHub               deequ-pydeequ              2665               470   \n",
       "2  GitHub                     Pandera              1978               162   \n",
       "3  GitHub              Apache Griffin               999               585   \n",
       "4  GitHub  TensorFlow Data Validation               699               150   \n",
       "5  GitHub                         DBT              6600              1200   \n",
       "\n",
       "   Year                                            Keyword  \\\n",
       "0  2019  what are the most popular data quality test to...   \n",
       "1  2018                                                NaN   \n",
       "2  2018  what are the most popular data quality test to...   \n",
       "3  2017  what are the most popular data quality test to...   \n",
       "4  2018                                                NaN   \n",
       "5  2021                                                NaN   \n",
       "\n",
       "                                       search string  \\\n",
       "0         filename:great_expectations extension:.yml   \n",
       "1  filename:deequ extension:.jar,deequ extension:...   \n",
       "2     pandera extension:.py,pandera extension:.ipynb   \n",
       "3                       apachegriffin extension:.yml   \n",
       "4           tensorflow_data_validation extension:.py   \n",
       "5           test filename:dbt_project extension:.yml   \n",
       "\n",
       "                                     Repository Link  \n",
       "0  https://github.com/great-expectations/great_ex...  \n",
       "1                   https://github.com/awslabs/deequ  \n",
       "2             https://github.com/pandera-dev/pandera  \n",
       "3                  https://github.com/apache/griffin  \n",
       "4      https://github.com/tensorflow/data-validation  \n",
       "5               https://github.com/dbt-labs/dbt-core  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Get owner-repo list\n",
    "repo_list = pd.read_excel('DataQualityToolsList1.xlsx')\n",
    "repo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f2e20e-b05c-4fb6-b342-4132e3d54829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: filename:great_expectations extension:.yml\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml', 'per_page': 100, 'page': 1}\n",
      "Page 1: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml', 'per_page': 100, 'page': 2}\n",
      "Page 2: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml', 'per_page': 100, 'page': 3}\n",
      "Page 3: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml', 'per_page': 100, 'page': 4}\n",
      "Page 4: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml', 'per_page': 100, 'page': 5}\n",
      "Page 5: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml', 'per_page': 100, 'page': 6}\n",
      "Page 6: 58 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml', 'per_page': 100, 'page': 7}\n",
      "Fetching details for dagster-io/dagster\n",
      "Fetching details for jacopotagliabue/you-dont-need-a-bigger-boat\n",
      "Fetching details for josephmachado/data_engineering_best_practices\n",
      "Fetching details for erwinpaillacan/kedro-great-expectations-example\n",
      "Fetching details for mkerschbaumer/rb-data-smell-detection\n",
      "Fetching details for Sage-Bionetworks/snowflake\n",
      "Fetching details for CardosoJr/bootcamp\n",
      "Fetching details for dunghoang369/feature-store\n",
      "Fetching details for santiagoprado12/titanic_effort\n",
      "Fetching details for Manuindukuri/Streamlit_GreatExpectations\n",
      "Fetching details for artefactory/xhec-mlops-crashcourse\n",
      "Fetching details for ichsanulamal/notes\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for JuanRuizIng/GlobalTerrorismAnalysis_ETL\n",
      "Fetching details for iakgk/Enterprise_Data_Governance_and_Quality_Enhancement_Initiative\n",
      "Fetching details for spbail/data-quality-tools\n",
      "Fetching details for Iznia/Restaurant-Customer-Satisfaction-Analysis\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Eliie58/movie_recommendation_system\n",
      "Fetching details for mlops-2425q1-mds-upc/MLOps-2425q1-demos\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Aura-healthcare/seizure_detection_pipeline\n",
      "Fetching details for agrawalnaveen/wine-quality-mlops\n",
      "Fetching details for satriotn/Adidas-Sales-Pipeline\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Mopheshi/DataEngineeringSpecialization\n",
      "Fetching details for meredithoopis/Capstone_DE_project\n",
      "Fetching details for BigDataIA-Spring2023-Team-01/Assignment1\n",
      "Fetching details for nuvita97/dsp-final-project\n",
      "Fetching details for davidkhachikov/MLOps-for-music-popularity-prediction\n",
      "Fetching details for deep-diver/githubaction-demo\n",
      "Fetching details for Tatshini/insyd_mlops\n",
      "Fetching details for mfiqihalayubi/Batch-data-processing-and-periodical-travel-insurance-market-analysis\n",
      "Fetching details for mikepars/automated-ds-jobs-monitoring\n",
      "Fetching details for BigDataIA-Summer2022-1/Assignment1\n",
      "Fetching details for piyush-an/DAMG7245-Spring23\n",
      "Fetching details for YkpDeMiR/data_testing\n",
      "Fetching details for IU-MLOps-project-2024/mlops-final-project-iu-2024\n",
      "Fetching details for Negi97Mohit/INFO7374-32925-Algorithmic-Digital-Marketing\n",
      "Fetching details for lightshifted/MLOPS\n",
      "Fetching details for giovales/GIOVANNA_VALES_DDF_TECH_072024\n",
      "Fetching details for MLOps-essi-upc/MLOps_WhereIsWally\n",
      "Fetching details for ckevinhill/great_expectations\n",
      "Fetching details for mhabedank/dwd-weather-data-ingestion\n",
      "Fetching details for joaoPedroMota20230454/mlops_prject_2\n",
      "Fetching details for fembrioni/fer-gx\n",
      "Fetching details for IU-MLOps-project-2024/mlops-final-project-iu-2024\n",
      "Fetching details for PetrovskiBojan/Payment-Recognition\n",
      "Fetching details for sustertine/iis-projekt\n",
      "Fetching details for sustertine/mbajk-api\n",
      "Fetching details for GrozdaniTanja/gold-price-prediction\n",
      "Fetching details for Nitheshmanimaran/bs_test\n",
      "Fetching details for caioFiorini/CAIO_FIORINI_DDF_TECH_072024\n",
      "Fetching details for franciscojavierarceo/Python\n",
      "Fetching details for gumartinm/dbt-sts-ge\n",
      "Fetching details for AntonVoronkoPM/skillscounter\n",
      "Fetching details for taed2-2425q1-gced-upc/TAED2_BytesWithoutBorders\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for kktiwa/ds-de-examples-playground\n",
      "Fetching details for jogoodma/county-health\n",
      "Fetching details for dunghoang369/feature-store\n",
      "Fetching details for roblim/great_expectations_ifttt_action\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for HungNguyen501/data-quality-great-expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for TFMV/DataSanity\n",
      "Fetching details for eavanvalkenburg/GX\n",
      "Fetching details for hieuvp/robust-data-pipeline\n",
      "Fetching details for Azankiev/Kedro-pipeline\n",
      "Fetching details for asm0dey/great-expectations-sample\n",
      "Fetching details for ZelaskoMichal/de-cf-pyrogai-op-pipelines-zelasko\n",
      "Fetching details for ETIQ-AI/ml-testing\n",
      "Fetching details for khungCU/prestashop-casestudy\n",
      "Fetching details for idealista/great_expectations_talk\n",
      "Fetching details for turntabl-sentiment-analysis/ml-pipeline\n",
      "Fetching details for MuhammadRozzaaq/My_Project\n",
      "Fetching details for NFIT95/etetsfm\n",
      "Fetching details for greatexpectationslabs/ge_demo\n",
      "Fetching details for andrefilipefmsilva/MLOps_Full_Deployment\n",
      "Fetching details for MLOps-essi-upc/taed2-ML-Alphas\n",
      "Fetching details for provectus/data-quality-gate\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/Assignment_2\n",
      "Fetching details for josephmachado/data-quality-w-greatexpectations\n",
      "Fetching details for Tomas-vilte/streamingPipelineMercadoLibre\n",
      "Fetching details for shpolina/great_expectations_samples\n",
      "Fetching details for anagasperin/IIS\n",
      "Fetching details for jdimatteo/great_expectations_tutorial_custom_expectation\n",
      "Fetching details for nayakatul/Stack-Overflow-AI-Assistant\n",
      "Fetching details for sumaniitm/open-source-etl\n",
      "Fetching details for cstirry/bnia-tutorial-great-expectations\n",
      "Fetching details for josejnra/great-expectations\n",
      "Fetching details for adrian-pasek-prv/bestbuy-product-catalog\n",
      "Fetching details for Adriana031991/Great-Expectations-exercise\n",
      "Fetching details for dagster-io/dagster\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Education-Model\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Template\n",
      "Fetching details for Androidown/dagster\n",
      "Fetching details for josephmachado/data_engineering_best_practices_log\n",
      "Fetching details for levyvix/data_engineering_best_practices\n",
      "Fetching details for zdoryk/lakehouse\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for HungNguyenDev1511/Capstone-Project-Data-Engineer\n",
      "Fetching details for provectus/data-quality-gate\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/Assignment_3\n",
      "Fetching details for Sukruthmothakapally/AI-Powered-StackOverflow-App\n",
      "Fetching details for Vedant-Deshpande/Vedant-Deshpande_StackAI\n",
      "Fetching details for BigDataIA-Summer2023-Team3/final_project\n",
      "Fetching details for jacopotagliabue/you-dont-need-a-bigger-boat\n",
      "Fetching details for nupsea/af-wlm\n",
      "Fetching details for meltano/squared\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/DAMG_7245\n",
      "Fetching details for flyteorg/flytesnacks\n",
      "Fetching details for samurai-py/weather-traffic-etl\n",
      "Fetching details for aal-v-ess/MLOps-Mini-Framework\n",
      "Fetching details for santiagxf/mlproject-sample\n",
      "Fetching details for BirdiD/BirdiDQ\n",
      "Fetching details for Marca01/DWH\n",
      "Fetching details for datarootsio/tutorial-great-expectations\n",
      "Fetching details for astronomer/airflow-provider-great-expectations\n",
      "Fetching details for tylkahn/mlops-project\n",
      "Fetching details for sp1thas/criticker-dataset\n",
      "Fetching details for oceanwave09/test-ci-cd\n",
      "Fetching details for anagasperin/IIS_projekt\n",
      "Fetching details for baptdav/5MLDE_PROJ\n",
      "Fetching details for taed2-2425q1-gced-upc/TAED2_BytesWithoutBorders\n",
      "Fetching details for CesarMitja/IIS_2\n",
      "Fetching details for sulcer/electricity_predictor\n",
      "Fetching details for astronomer/airflow-data-quality-demo\n",
      "Fetching details for hnawaz007/pythondataanalysis\n",
      "Fetching details for RantusaLara/IIS\n",
      "Fetching details for undersfx/dewp\n",
      "Fetching details for vittoriopolverino/great-expectations-lambda\n",
      "Fetching details for trannhatnguyen2/NYC_Taxi_Data_Pipeline\n",
      "Fetching details for DaveRuijter/presentations\n",
      "Fetching details for opendatadiscovery/odd-great-expectations-demo\n",
      "Fetching details for deep-diver/mlops-demo\n",
      "Fetching details for johnkatua/data-validation-gx-cloud\n",
      "Fetching details for jernej10/napovedna_storitev\n",
      "Fetching details for merobi-hub/lantern-library-online\n",
      "Fetching details for sarveshwar-s/DSP_Project\n",
      "Fetching details for IVproger/MLops-project\n",
      "Fetching details for KunalBhoyar/Geospatial_data_exploration\n",
      "Fetching details for GokuMohandas/data-engineering\n",
      "Fetching details for DaveRuijter/presentations\n",
      "Fetching details for staeff/ge_bike_berlin\n",
      "Fetching details for janinewin/docker_cicd\n",
      "Fetching details for Mopheshi/DataEngineeringSpecialization\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Gundalai-Batkhuu/data-pipeline-AWS\n",
      "Fetching details for jeantardelli/data-engineering-with-python\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for sasongkobgn/datafellowship12_sgkbgn\n",
      "Fetching details for heineken-advanced-analytics/great-expectations-workshop-pydata\n",
      "Fetching details for izalitzilant/mlopsiu_team4\n",
      "Fetching details for ZanPovseGit/RekeVodotok\n",
      "Fetching details for lanaben/MBajk-prediction-app\n",
      "Fetching details for NikolaVilar/IIS\n",
      "Fetching details for GuinsooLab/sheenflow\n",
      "Fetching details for ismaildawoodjee/Great-Expectations-for-JSON\n",
      "Fetching details for JiangYuxuan233/Intern_Yuxuan\n",
      "Fetching details for albakoehler/taed2\n",
      "Fetching details for MLOps-essi-upc/taed2-Food_Classification\n",
      "Fetching details for HuhtaLauri/az-ge\n",
      "Fetching details for JanHuntersi/IIS_NEW\n",
      "Fetching details for sumithsingh/dsp-skyprix\n",
      "Fetching details for kundigagandeep/kedro-sentiment-analysis\n",
      "Fetching details for CJSanon/S3-Data-Warehouse-ETL-Redshift\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for YkpDeMiR/data_testing\n",
      "Fetching details for bartosz25/python-playground\n",
      "Fetching details for lassebenni/makelaarsland-gh-action\n",
      "Fetching details for se4ai2122-cs-uniba/CT-COVID\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Kaastor/ml-ops\n",
      "Fetching details for SinghAbhishek07/Birddq\n",
      "Fetching details for g2a-world/great_expectation_de_validations\n",
      "Fetching details for devopsmarc/osullivan-ai\n",
      "Fetching details for sasongkobgn/datafellowship12_sgkbgn\n",
      "Fetching details for alyamutiara/DataFellowship12\n",
      "Fetching details for astronomer/airflow-snowpark-containers-demo\n",
      "Fetching details for samueldatasci/MDSAA_MLOPS\n",
      "Fetching details for ind4zy/Data-Engineering-for-Machine-Learning\n",
      "Fetching details for Debargho99/Data-Enginnering\n",
      "Fetching details for abhishek-1131/BERTNest\n",
      "Fetching details for AChowdhury1211/mlops_project\n",
      "Fetching details for alyamutiara/DataFellowship12\n",
      "Fetching details for merlinepedra/DAGSTER\n",
      "Fetching details for shanku007/pro-orch\n",
      "Fetching details for CJSanon/S3-Data-Warehouse-ETL-Redshift\n",
      "Fetching details for Fabrizio250/test_ct_covid\n",
      "Fetching details for khuyentran1401/Data-science\n",
      "Fetching details for pdefusco/cde_3rdp_airflow_providers\n",
      "Fetching details for taed2-2425q1-gced-upc/TAED2-2425q1-demos\n",
      "Fetching details for dgg32/mgrast_ge\n",
      "Fetching details for mozilla/great_expectations_demo\n",
      "Fetching details for deep-diver/githubaction-tester\n",
      "Fetching details for se4ai2324-uniba/GHIPrediction\n",
      "Fetching details for danielbeach/GreatExpectationsWithSpark\n",
      "Fetching details for datakind/Data-Observation-Toolkit\n",
      "Fetching details for datarootsio/notion-dbs-data-quality\n",
      "Fetching details for spbail/dag-stack\n",
      "Fetching details for luanans/ada-analytics-engineering\n",
      "Fetching details for khadkarajesh/wine-prediction\n",
      "Fetching details for rcmoynihan/modern-data-eng\n",
      "Fetching details for manugarri/elt-spike\n",
      "Fetching details for MLOps-essi-upc/TAED2-clothing-reviews\n",
      "Fetching details for Vedant-Deshpande/DAMG7245Vedant\n",
      "Fetching details for rylativity/container-analytics-platform\n",
      "Fetching details for piyush-an/DAMG7245-Spring23\n",
      "Fetching details for wmde/wikibase-metadata\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for mkgmels/Data-Engineering-Portfolio\n",
      "Fetching details for erwinpaillacan/kedro-great-expectations-example\n",
      "Fetching details for adhyarya51/Census-Income-Dataset\n",
      "Fetching details for inesartero95/Energinet-GX\n",
      "Fetching details for stwins60/great_expectations\n",
      "Fetching details for astronomer/gx-airflow-pipeline\n",
      "Fetching details for gamberooni/imdb-etl\n",
      "Fetching details for sichgeis/data-pipeline-testing-demo\n",
      "Fetching details for mrdavidlaing/software-releases-dwh\n",
      "Fetching details for gilbertk27/Superstore-Data-Pipeline\n",
      "Fetching details for matevzsemprimoznik/solar-power-forecast\n",
      "Fetching details for blake-enyart/ge_deployment_practice\n",
      "Fetching details for NanaOKA/spaceship-tutorial\n",
      "Fetching details for Hexy00123/Sum2024MLOps\n",
      "Fetching details for CardosoJr/bootcamp\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/Assignment_2\n",
      "Fetching details for Coolbreeze151/BackendRoadmap\n",
      "Fetching details for strimban/datahub_challenge\n",
      "Fetching details for zenml-io/apidocs-fork\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/Assignment_3\n",
      "Fetching details for jared-rimmer/great-expectations-demo\n",
      "Fetching details for oceanwave09/test-ci-cd\n",
      "Fetching details for Condielj/great-expectations-intro\n",
      "Fetching details for Condielj/great-expectations-intro\n",
      "Fetching details for BigDataIA-Spring2023-Team-05/Assignment-03\n",
      "Fetching details for RaulBarbaRojas/DataMining\n",
      "Fetching details for RaulBarbaRojas/DataMining\n",
      "Fetching details for hieuvp/robust-data-pipeline\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Education-Model\n",
      "Fetching details for stackabletech/example-dags\n",
      "Fetching details for devopsmarc/osullivan-ai\n",
      "Fetching details for se4ai2324-uniba/CropClassification\n",
      "Fetching details for ismaildawoodjee/GreatEx\n",
      "Fetching details for samusyrjanen/notes\n",
      "Fetching details for luchonaveiro/open-source-data-stack\n",
      "Fetching details for oceord/aml\n",
      "Fetching details for ashishtele/MLOps\n",
      "Fetching details for Swati17293/newAdult_DataAugment\n",
      "Fetching details for Joshua-omolewa/Stock_streaming_pipeline_project\n",
      "Fetching details for rishimo/msds680-project\n",
      "Fetching details for ChiefPatrik/trafficPredictions\n",
      "Fetching details for trdin/BTCRNN\n",
      "Fetching details for ROL-1/5MLDE-soutenace\n",
      "Fetching details for igarciamontoya/mlops_project\n",
      "Fetching details for Resistance-Lab/resistancelab_data\n",
      "Fetching details for owshq-live/ge-data-quality-workflow\n",
      "Fetching details for mbakunze/great_expectation_sandbox\n",
      "Fetching details for Qualytics/qualytics-examples\n",
      "Fetching details for Th0usandSunny/Automation-and-Visualization-for-Marketing-Report-Using-Airflow-and-kibana\n",
      "Fetching details for PetrovskiBojan/IIS\n",
      "Fetching details for ryankarlos/dagster-expectations\n",
      "Fetching details for jplaulau14/mlops-python\n",
      "Fetching details for rokrozman321/IIS_naloga1\n",
      "Fetching details for RizqiSeijuuro/learn-great_expectations\n",
      "Fetching details for APaliakou/GE\n",
      "Fetching details for datahub-project/datahub\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for flyteorg/flytekit\n",
      "Fetching details for mihail911/fake-news\n",
      "Fetching details for great-expectations/great_expectations_action\n",
      "Fetching details for provectus/data-quality-gate\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for antonindurieux/MLOps_template\n",
      "Fetching details for MetaphorData/connectors\n",
      "Fetching details for makingbigdato/prjctr-02-mlop-infrastructure\n",
      "Fetching details for diogo-pires-github/MLOps\n",
      "Fetching details for AdrianSzymczyk/running-ml-project\n",
      "Fetching details for latchbio/flaightkit\n",
      "Fetching details for adhyarya51/Project-CV\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/Assignment_3\n",
      "Fetching details for mesikkera/DailyCoding\n",
      "Fetching details for maheshkshi400/juypter\n",
      "Fetching details for amanjaiswalofficial/medium-articles\n",
      "Fetching details for hanifarieff/python\n",
      "Fetching details for oceanwave09/test-ci-cd\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Template\n",
      "Fetching details for merlinepedra/DAGSTER\n",
      "Fetching details for shanku007/pro-orch\n",
      "Fetching details for Abhishekop722/datahub\n",
      "Fetching details for anthonyhungnguyen/datahub-ghn\n",
      "Fetching details for raft-tech/datahub-metadata-day-2022\n",
      "Fetching details for stagebo/my_datahub\n",
      "Fetching details for Kartik-153/fake_news_detector\n",
      "Fetching details for actions-marketplace-validations/great-expectations_great_expectations_action\n",
      "Fetching details for MajdAlAjlaniiii/Credit_Fraud_Detector\n",
      "Fetching details for jacopotagliabue/you-dont-need-a-bigger-boat\n",
      "Fetching details for syalanuj/youtube\n",
      "Fetching details for outerbounds/full-stack-ML-metaflow-tutorial\n",
      "Fetching details for NameArtem/deployml_course\n",
      "Fetching details for yussaaa/great_expectatisons_runtime_demo\n",
      "Fetching details for luchonaveiro/great-expectations-postgres-tutorial\n",
      "Fetching details for astronomer/kedro-ge-airflow\n",
      "Fetching details for AnastasiyaSilivonchyk/great_expectations_dq\n",
      "Fetching details for BigDataIA-Spring2023-Team-05/Assignment-02\n",
      "Fetching details for harshit82/great_expectations_test\n",
      "Fetching details for boushphong/geospatial-analysis-pipeline\n",
      "Fetching details for ErokhinE/MLOps\n",
      "Fetching details for tanle8/dsp-fake-bill-pred\n",
      "Fetching details for meredithoopis/Capstone_DE_project\n",
      "Fetching details for MLOps-essi-upc/MLOps-SentiBites\n",
      "Fetching details for shahparth0007/Big-Data-Systems-Intelligence-Analytics-Labs-Summer-2022\n",
      "Fetching details for data-engineering-helpers/data-contracts\n",
      "Fetching details for RafaParkoureiro/Automated-Pipeline-Infrastructure\n",
      "Fetching details for Nitheshmanimaran/ml_automation\n",
      "Fetching details for Condielj/great-expectations-intro\n",
      "Fetching details for phcngyn99/online-retail\n",
      "Fetching details for ozacas/asxtrade\n",
      "Fetching details for bwhom2000/DiabetesPrediction\n",
      "Fetching details for NanaOKA/tutorial_template\n",
      "Fetching details for fembrioni/fer-gx-validator\n",
      "Fetching details for igarciamontoya/mlops_labs\n",
      "Fetching details for Manuindukuri/Streamlit_GreatExpectations\n",
      "Fetching details for RAMYA-V-7/Data_quality_great_expectations\n",
      "Fetching details for KunalBhoyar/Geospatial_API_as_a_service\n",
      "Fetching details for istiyaksiddiquee/MagnumOpus\n",
      "Fetching details for karthikshankar98/Kedro-Data-Pipeline\n",
      "Fetching details for thornandberry/intwv-20220926\n",
      "Fetching details for ragingbal/gx-demo-dq-checks\n",
      "Fetching details for mlincon/coursera-courses\n",
      "Fetching details for HermannJoel/enr_portfolio_modeling\n",
      "Fetching details for meredithoopis/Capstone_DE_project\n",
      "Fetching details for MLOps-essi-upc/TAED2-Falsettos\n",
      "Fetching details for tamsanh/kedro-great\n",
      "Fetching details for lanaben/news-prediction-app\n",
      "Fetching details for Light-House-AI/Lighthouse\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for piyush-an/DAMG7245-Fall2023\n",
      "Fetching details for DatakinHQ/demo\n",
      "Fetching details for xrsrke/prodgpt\n",
      "Fetching details for punsiriboo/data-quality-with-apache-airflow\n",
      "Fetching details for Jhopcel/Middleware-ETL-APT\n",
      "Fetching details for DrunkTeam/ApartmentPrice\n",
      "Fetching details for dixon2678/el-snowflake\n",
      "Fetching details for alperbingol/master-thesis\n",
      "Fetching details for morphatic/gx-fluent-pandas-asset-name-bug-demo\n",
      "Fetching details for MrPyroTek/5MLOPS\n",
      "Fetching details for gsajko/tweetfeed\n",
      "Fetching details for paket2004/Stock-market-prediction\n",
      "Fetching details for BigDataIA-Spring2023-Team-05/Assignment-03\n",
      "Fetching details for rohanrb302/End-to-End-Movie-Recommendation--Service\n",
      "Fetching details for astronomer/gx-tutorial\n",
      "Fetching details for Foehammer82/tech-terrarium\n",
      "Fetching details for nai1ka/MLOpsProject\n",
      "Fetching details for MLOps-essi-upc/taed2-PedestrianDetection\n",
      "Fetching details for UCBoulder/oda_ds_cookiecutter_template\n",
      "Fetching details for ishwarpawar21/healthcare-etl-v4\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Amirka-Kh/TM5MLOops\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for naveeagrawal/mlops\n",
      "Fetching details for provectus/from_ge_to_allure_mapper\n",
      "Fetching details for opendevstack/ods-quickstarters\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for GokuMohandas/follow\n",
      "Fetching details for MetaphorData/connectors\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for octave-ati/MLOps-Text-Classification\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Jason-Clark-FG/OpenMetadata-Mirror\n",
      "Fetching details for jacopotagliabue/you-dont-need-a-bigger-boat\n",
      "Fetching details for NameArtem/deployml_course\n",
      "Fetching details for astronomer/custom-xcom-backend-tutorial\n",
      "Fetching details for greatexpectationslabs/astro-ge\n",
      "Fetching details for astronomer/azure-great-expectations\n",
      "Fetching details for BigDataIA-Spring2023-Team-05/Assignment-03\n",
      "Fetching details for igarciamontoya/mlops_labs\n",
      "Fetching details for NuwanCW/mlops_test\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Jason-Clark-FG/OpenMetadata-FG\n",
      "Fetching details for surenganne/OpenMetadata-1.2.0\n",
      "Fetching details for jblim0125/openmetadata-2\n",
      "Fetching details for surenganne/OpenMetadata\n",
      "Fetching details for josephmachado/efficient_data_processing_spark\n",
      "Fetching details for StevenMMortimer/ge-sklearn-pipeline-example\n",
      "Fetching details for eliasbenaddou/personal_finance_de_project\n",
      "Fetching details for twoutrigger/Data-Science-Projects\n",
      "Fetching details for karoteeni110/Coursera-Data-Engineering-Specialization\n",
      "Fetching details for BigDataIA-Spring2023-Team-01/Assignment-3\n",
      "Fetching details for DSP-on-Wine/red-wine-quality\n",
      "Fetching details for varshahindupur09/Real_Time_Weather_Data\n",
      "Fetching details for HuhtaLauri/actual-mlops\n",
      "Fetching details for MajorDaxx/crba-etl\n",
      "Fetching details for UTDT-TD7/practicas\n",
      "Fetching details for uche-madu/gx-testrun\n",
      "Fetching details for josejnra/apache-airflow\n",
      "Fetching details for 26nikhilkumar/healthcare-pipeline\n",
      "Fetching details for trisdoan/Marketing_project\n",
      "Fetching details for lassebenni/makelaarsland-gh-action\n",
      "Fetching details for chuhsuanlee/literate-guide\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Androidown/dagster\n",
      "Fetching details for modupeotayo/dsp_project\n",
      "Fetching details for luisgogu/SentimentAnalysis-TAED\n",
      "Fetching details for finesse-fingers/airflow-playground\n",
      "Fetching details for greatexpectationslabs/airflow_meetup_demo\n",
      "Fetching details for MLOps-essi-upc/MLOps2023Course-demo\n",
      "Fetching details for PbVrCt/time-series-pipeline\n",
      "Fetching details for phatnguyen080401/real-estate-sale-analytics\n",
      "Fetching details for nasa-petal/petal-labeler-data-pipeline\n",
      "Fetching details for Ability014/MDS-for-Quality-Pipelines\n",
      "Fetching details for Posfay/Data-Quality-Project\n",
      "Fetching details for sulcer/intelligent_systems_engineering\n",
      "Fetching details for AmineMrabet12/DSP-Project\n",
      "Fetching details for lksanterre/mlops_full_pipeline\n",
      "Fetching details for lksanterre/ml_ops_project\n",
      "Fetching details for celicelopes/ADA1011-Analytics\n",
      "Fetching details for cwbennie/FinalProject_MLOps\n",
      "Fetching details for trdin/RNNSistem\n",
      "Fetching details for chekanskiy/covid-19-great-expectations\n",
      "Fetching details for joeip0411/GroceryAnalytics\n",
      "Fetching details for SinghAbhishek07/Birddq\n",
      "Fetching details for piyush-an/DAMG7245-Summer23\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/ResearcHub\n",
      "Fetching details for BigDataIA-Spring2023-Team-04/BigDataIA-Assignment-02\n",
      "Fetching details for sleepo-chin/TrainingAI\n",
      "Fetching details for meredithoopis/de_project\n",
      "Fetching details for alej0909/ETL-1\n",
      "Fetching details for JelmerVanNuss/AlcoholPredictor\n",
      "Fetching details for gryBox/prefect_guide\n",
      "Fetching details for SaschaDittmann/dbt-demo\n",
      "Fetching details for eyaltrabelsi/my-notebooks\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for nico-fi/semi-supervised-dcec\n",
      "Fetching details for freireoliveira/great_expectations_project\n",
      "Fetching details for BigDataIA-Spring2023-Team-01/Assignment-3\n",
      "Fetching details for Foehammer82/tech-terrarium\n",
      "Fetching details for MattTriano/quarto_blog\n",
      "Fetching details for BigDataIA-Spring2023-Team-09/DAMG_7245\n",
      "Fetching details for PrefectHQ/prefect-great-expectations\n",
      "Fetching details for qxf2/newsletter_automation\n",
      "Fetching details for JuanCampbsi/analytics_engineering_airbnb\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for rahul-theorem/flytekit-theorem\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Education-Model\n",
      "Fetching details for Androidown/dagster\n",
      "Fetching details for BigDataIA-Spring2023-Team-05/Assignment-01\n",
      "Fetching details for Condielj/great-expectations-intro\n",
      "Fetching details for sleepo-chin/TrainingAI\n",
      "Fetching details for taed2-2425q1-gced-upc/TAED2-2425q1-demos\n",
      "Fetching details for meredithoopis/de_project\n",
      "Fetching details for BigDataIA-Spring2023-Team-01/Assignment-2\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Template\n",
      "Fetching details for merlinepedra/DAGSTER\n",
      "Fetching details for shanku007/pro-orch\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for Sage-Bionetworks/recover\n",
      "Fetching details for oss-compass/compass-projects-information\n",
      "Fetching details for piyush-an/NYC-Restaurant-Inspection\n",
      "Fetching details for Aleksandra-Vainilovich/DQEMentoringProgram\n",
      "Fetching details for julian-west/e4ds-snippets\n",
      "Fetching details for luongphambao/nyc-taxi-feature-store\n",
      "Fetching details for codekid/disaster_data\n",
      "Fetching details for KiwiBrezo/slovenia-weather-forecast-inteligent-system\n",
      "Fetching details for LapitskayaN/DQE-Mentoring-Program-\n",
      "Fetching details for microsoft/devsquad-mlops\n",
      "Fetching details for philippschmalen/etl_spark_airflow_emr\n",
      "Fetching details for shivasaicharanruthala/DAMG7425-Summer23-Labs\n",
      "Fetching details for vnugny/DataModel-GXX\n",
      "Fetching details for datakind/data-confidence\n",
      "Fetching details for cbdq-io/datasets\n",
      "Fetching details for perkzen/mbajk-ml-web-service\n",
      "Fetching details for scoyne2/data-manager\n",
      "Fetching details for saadkhalid-git/heart-disease-pridiction-pipeline\n",
      "Fetching details for spothapragada/NYCTaxi-DE\n",
      "Fetching details for diogobarros02/Machine-Learning-Operations---Predict-Job-Change-\n",
      "Fetching details for intanmuktif/Melbourne-House-Sales-Transaction-with-Airflow\n",
      "Fetching details for nirfana/automate-fraud-monitoring\n",
      "Fetching details for Ediashta-Narendra/Electronic-Add-Ons-for-Product-Sales-Optimization\n",
      "Fetching details for BigDataIA-Summer2022Team2/Assignment2\n",
      "Fetching details for RuthwikBg/Data-Quality-Assessment-and-Summarization-Tool\n",
      "Fetching details for FTDS-assignment-bay/p2-final-project-group-002\n",
      "Fetching details for Nit31/AliExpress_Price_Prediction\n",
      "Fetching details for Palandr123/MLOps-Project\n",
      "Fetching details for MLOps-essi-upc/MLOps-TeamBeans\n",
      "Fetching details for noklam/kedro-example\n",
      "Fetching details for MoukthikaM/DAMG7245-Assignment3\n",
      "Fetching details for salmaelyagoubi/error418\n",
      "Fetching details for BigDataIA-Spring2023-Team02/StockRecommendationSystem\n",
      "Fetching details for Nitheshmanimaran/Data-Science-Complete-In-Production-Project\n",
      "Fetching details for morn1n9st4r/bookswarehouse\n",
      "Fetching details for albutz/de-movies\n",
      "Fetching details for artefactory/xhec-mlops-crashcourse\n",
      "Fetching details for lksanterre/ml_ops\n",
      "Fetching details for JanHuntersi/strava_prediction\n",
      "Fetching details for MattTriano/analytics_data_where_house\n",
      "Fetching details for se4ai2223-uniba/architectural-style-recognition\n",
      "Fetching details for morn1n9st4r/medical_machine_learning\n",
      "Fetching details for AirelRibeiro/ana_eng_project\n",
      "Fetching details for BigDataIA-Spring2023-Team-03/Stock_Analysis_Summarizer\n",
      "Fetching details for KiwiBrezo/air-pollution-inteligent-system\n",
      "Fetching details for BigDataIA-Fall2023-Team7/Assignment1-PDF-Processing-Application\n",
      "Fetching details for greatexpectationslabs/great_expectations_example_load_npi_file_into_mysql\n",
      "Fetching details for OpenLineage/OpenLineage\n",
      "Fetching details for kristjansuligoj/iis-vaja\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for snowplow/data-models\n",
      "Fetching details for espositoandrea/Dementia-Detection\n",
      "Fetching details for se4ai2122-cs-uniba/Drone-CrowdCounting\n",
      "Fetching details for 007kakashi/MlOps-course\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for merlinepedra/DAGSTER\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for flynn3103/data-stack\n",
      "Fetching details for santiagxf/trunkbased-mlops\n",
      "Fetching details for yashjain02/Loan-Prediction-TheVectors\n",
      "Fetching details for anateresamaia/MLOpsProject\n",
      "Fetching details for BigDataIA-Fall2023-Team3/Data-Quality-Assessment-and-Summarization-Tool\n",
      "Fetching details for Akshathapatil1998/Assignment-1\n",
      "Fetching details for RuthwikBg/PDF_Analyzer\n",
      "Fetching details for Nitheshmanimaran/Data-Science-Complete-In-Production-Project\n",
      "Fetching details for oriol-marco/202306_MLOps_ML_projects_Classifier\n",
      "Fetching details for shanku007/pro-orch\n",
      "Fetching details for HungNguyenDev1511/Capstone-Project-Data-Engineer\n",
      "Fetching details for PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices\n",
      "Fetching details for callmeeric5/DSP_Final\n",
      "Fetching details for Sage-Bionetworks/agora-data-tools\n",
      "Fetching details for se4ai2223-uniba/glassDetection\n",
      "Fetching details for roderickmajoor/BD-Project\n",
      "Fetching details for seve-martinez/Coinraker\n",
      "Fetching details for data-corentinv/mlops-training\n",
      "Fetching details for SELRHIABI/DataPipeline\n",
      "Fetching details for YkpDeMiR/data_testing\n",
      "Fetching details for Slaiby/dsp-wsms\n",
      "Fetching details for GeorgeAzaru/GreatExpectations\n",
      "Fetching details for mikepars/autobuddy-llm-chatbot\n",
      "Fetching details for Rishabhsingh11/AIssueFlow\n",
      "Fetching details for dinhanhthi/mooc-de\n",
      "Fetching details for OpenLineage/OpenLineage\n",
      "Fetching details for jitsejan/great-expectations-explore\n",
      "Fetching details for paulf-999/demos\n",
      "Fetching details for BigDataIA-Spring2023-Team-04/Final-Project-Playground\n",
      "Processing query: filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py', 'per_page': 100, 'page': 1}\n",
      "Page 1: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py', 'per_page': 100, 'page': 2}\n",
      "Page 2: 54 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py', 'per_page': 100, 'page': 3}\n",
      "Fetching details for data-science-on-aws/data-science-on-aws\n",
      "Fetching details for alipay/alipay-sdk-python-all\n",
      "Fetching details for aws-samples/spark-on-aws-lambda\n",
      "Fetching details for hanrach/p2d_fast_solver\n",
      "Fetching details for microsoft/Auto-Validate-by-History\n",
      "Fetching details for zBrainiac/streaming-flink\n",
      "Fetching details for xplot/imeet\n",
      "Fetching details for LUH-DBS/Matelda-Baselines\n",
      "Fetching details for lep511/spark-on-lambda\n",
      "Fetching details for pflun/advancedAlgorithms\n",
      "Fetching details for krishnainfoblox/data_dreamers\n",
      "Fetching details for Matthew-Burnett-450/Physical-Property-Modeling-Using-NMR-Spectra-and-Functional-Group-Representation\n",
      "Fetching details for several27/ProphecyDataQualityDemo\n",
      "Fetching details for depaulatiago/bootcampDIOPython\n",
      "Fetching details for kcirtapfromspace/database_thing\n",
      "Fetching details for alexbprr/Modelagem-Computacional\n",
      "Fetching details for Yazdish/PyLearnAssignments\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for natetongwong/SparkPythonDeequ\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for Luis20matias/Data_engineer_projects\n",
      "Fetching details for robson14br/Python-AI-Backend-Developer\n",
      "Fetching details for bssrdf/pyleet\n",
      "Fetching details for apostolos1927/DatabrickDataQuality\n",
      "Fetching details for GMLC-TDC/HELICS-Use-Cases\n",
      "Fetching details for koking0/LuffyCity\n",
      "Fetching details for YaraRossi/QuantifyRotationErrors\n",
      "Fetching details for sakamomo554101/sample_data_quality\n",
      "Fetching details for CamiloMWizeline/arcxp-interview\n",
      "Fetching details for JuanFer200402/POO\n",
      "Fetching details for jbernec/data-quality\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for himnsuk/Python-Practice\n",
      "Fetching details for alipay/alipay-sdk-python-all\n",
      "Fetching details for Ymiros0/al-python\n",
      "Fetching details for SebCorbin/netbluemind\n",
      "Fetching details for hanrach/p2d_solver\n",
      "Fetching details for yanvoi/leetcode_solutions\n",
      "Fetching details for databand-ai/dbnd\n",
      "Fetching details for alipay/alipay-sdk-python-all\n",
      "Fetching details for aws-samples/amazon-deequ-glue\n",
      "Fetching details for SheldonErasmus/Masters-Project\n",
      "Fetching details for sayofthelor/grade-equity\n",
      "Fetching details for data-science-on-aws/data-science-on-aws\n",
      "Fetching details for D7S5/_Spark\n",
      "Fetching details for chandru-2177/deequ_project\n",
      "Fetching details for aws-samples/data-science-on-aws\n",
      "Fetching details for chandru-2177/deequ_project\n",
      "Fetching details for leeFonzo/Practica-POO\n",
      "Fetching details for alexbprr/Modelagem-Computacional\n",
      "Fetching details for HERBETON25/AtividadeDe-Casa\n",
      "Fetching details for kcirtapfromspace/database_thing\n",
      "Fetching details for deka014/100-Days-of-DSA-and-React\n",
      "Fetching details for Sun-Zhiyuan/GVL\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for hsnr-data-science/SEDAR\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for xplot/imeet\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for krishnainfoblox/data_dreamers\n",
      "Fetching details for natetongwong/SparkPythonDeequ\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for alexbprr/Modelagem-Computacional\n",
      "Fetching details for kcirtapfromspace/database_thing\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for databand-ai/dbnd\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for canimus/cuallee\n",
      "Fetching details for rbuffat/pyidf\n",
      "Fetching details for SourceryAI/pydeequ3\n",
      "Fetching details for siddhant-deepsource/pydeequ3\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for several27/ProphecyDataQualityDemo\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for several27/ProphecyDataQualityDemo\n",
      "Fetching details for kcirtapfromspace/database_thing\n",
      "Fetching details for MADS508/labs\n",
      "Fetching details for tuhinsharma121/aws-sagemaker-quickstart\n",
      "Fetching details for jennyluciav/llm-finetuning\n",
      "Fetching details for oonisim/books-datascience-on-aws\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2021-06-28\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2021-05-15\n",
      "Fetching details for YushinJung-ATG/SageMaker-Workshop\n",
      "Fetching details for vijay-khanna/data-science-on-aws-sagemaker\n",
      "Fetching details for leoalexand/aws-ds-NLP-BERT\n",
      "Fetching details for weharris/data-science-on-AWS\n",
      "Fetching details for HadiMaqbool/workshp29\n",
      "Fetching details for bwcx-aws/data-science-on-aws-workshop\n",
      "Fetching details for anupam3693/data-science-on-aws\n",
      "Fetching details for sureindia-in/data-science-on-aws-workshop\n",
      "Fetching details for sureindia-in/aws-sagemaker-workshop-jun2021\n",
      "Fetching details for sureindia-in/data-science-on-aws-latest-2021\n",
      "Fetching details for ichen20/oreilly_book\n",
      "Fetching details for hydrobot004/datascience-on-aws\n",
      "Fetching details for esenebenjamin/SageMaker\n",
      "Fetching details for shumshersubashgautam/DataScience-AWS\n",
      "Fetching details for mike-maclaverty/data-science-on-aws\n",
      "Fetching details for cyourth-cognonic/aws-workshop\n",
      "Fetching details for Matthew-Burnett-450/Physical-Property-Modeling-Using-NMR-Spectra-and-Functional-Group-Representation\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Fetching details for koking0/LuffyCity\n",
      "Fetching details for koking0/LuffyCity\n",
      "Fetching details for koking0/LuffyCity\n",
      "Fetching details for bhaveshbendale/data-science-on-aws\n",
      "Fetching details for MADS508/labs\n",
      "Fetching details for lucentcosmos/awsds\n",
      "Fetching details for artemponomarevjetski/aws-ml-data-pipeline-deep-learniing\n",
      "Fetching details for oonisim/books-datascience-on-aws\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2021-06-28\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2021-05-15\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200425\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200523\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200822\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2020-09-26\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200919\n",
      "Fetching details for vijay-khanna/data-science-on-aws-sagemaker\n",
      "Fetching details for leoalexand/aws-ds-NLP-BERT\n",
      "Fetching details for YushinJung-ATG/SageMaker-Workshop\n",
      "Fetching details for weharris/data-science-on-AWS\n",
      "Fetching details for HadiMaqbool/workshp29\n",
      "Fetching details for bwcx-aws/data-science-on-aws-workshop\n",
      "Fetching details for anupam3693/data-science-on-aws\n",
      "Fetching details for sureindia-in/data-science-on-aws-workshop\n",
      "Fetching details for sureindia-in/aws-sagemaker-workshop-jun2021\n",
      "Fetching details for sureindia-in/data-science-on-aws-latest-2021\n",
      "Fetching details for ichen20/oreilly_book\n",
      "Fetching details for hydrobot004/datascience-on-aws\n",
      "Fetching details for esenebenjamin/SageMaker\n",
      "Fetching details for shumshersubashgautam/DataScience-AWS\n",
      "Fetching details for mike-maclaverty/data-science-on-aws\n",
      "Fetching details for bhupen/aws_datascience_workshop\n",
      "Fetching details for flavia-sosa/data-science-on-aws-pipelineai\n",
      "Fetching details for vijay-khanna/data-science-on-aws-workshop\n",
      "Fetching details for Chalcym/AWS_datascience_workshop\n",
      "Fetching details for aditya-chaturvedi/pipeline.ai-workshop\n",
      "Fetching details for sureindia-in/data-science-on-aws-archive-good\n",
      "Fetching details for Arijit-datascience/aws_workshop\n",
      "Fetching details for demonhawk007/AWS-BERT-Workshop\n",
      "Fetching details for cyourth-cognonic/aws-workshop\n",
      "Fetching details for BrianHHough/generative-ai-amazon-sagemaker-development-and-deployment\n",
      "Fetching details for gordonchen1989/data-science-on-aws\n",
      "Fetching details for tongliuTL/LLMs_SageMaker\n",
      "Fetching details for asikhsingh/chat-summarization-app\n",
      "Fetching details for vrishbhanusingh/nasa-ads-sagemaker\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Processing query: pandera extension:.py,pandera extension:.ipynb\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb', 'per_page': 100, 'page': 1}\n",
      "Page 1: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb', 'per_page': 100, 'page': 2}\n",
      "Page 2: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb', 'per_page': 100, 'page': 3}\n",
      "Page 3: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb', 'per_page': 100, 'page': 4}\n",
      "Page 4: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb', 'per_page': 100, 'page': 5}\n",
      "Page 5: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb', 'per_page': 100, 'page': 6}\n",
      "Page 6: 61 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb', 'per_page': 100, 'page': 7}\n",
      "Fetching details for unionai-oss/pandera\n",
      "Fetching details for khuyentran1401/Data-science\n",
      "Fetching details for kwinkunks/notebooks\n",
      "Fetching details for manual123/Nacho-Jupyter-Notebooks\n",
      "Fetching details for fugue-project/tutorials\n",
      "Fetching details for 74th/test-python\n",
      "Fetching details for a0th/a0th.github.io\n",
      "Fetching details for smmartprogrammer/modernpython\n",
      "Fetching details for Branden-Kang/Python-practice\n",
      "Fetching details for aeturrell/coding-for-economists\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for panaverse/learn-modern-python\n",
      "Fetching details for dayton-dynamic/dayton-dynamic.github.com\n",
      "Fetching details for jhrcook/conference-notes\n",
      "Fetching details for baniasbaabe/delightful-data-science\n",
      "Fetching details for Agusboliv/TP-POD\n",
      "Fetching details for fullstackwd/Data_engineer\n",
      "Fetching details for Avanthika19/JLNA-podsearch\n",
      "Fetching details for PUC-RecSys-Class/RecSysPUC-2024-2\n",
      "Fetching details for OPERA-Cal-Val/tile-mate\n",
      "Fetching details for therafiali/learn_python\n",
      "Fetching details for sashakaralchuk/ninja-move\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for jorisvandenbossche/ICES-python-data\n",
      "Fetching details for zetvzb/Predicting_MLB_Batted_Balls_Workflow_Python\n",
      "Fetching details for SamEdwardes/pycascades-2023-do-everything-with-pydantic\n",
      "Fetching details for AleksC999/newcode\n",
      "Fetching details for ecamo19/sureau_ecos_py\n",
      "Fetching details for MariaZharova/recsys_internship\n",
      "Fetching details for RT-Fridge/Applied-Deep-Learning-UVM\n",
      "Fetching details for siddharth-sen/ML-Exhibits\n",
      "Fetching details for abdulsami13/Panaverse--Meta-course-\n",
      "Fetching details for isasans/alguns-projetos-dados\n",
      "Fetching details for AndersonGabrielCalasans/BootCamp-SoulCode-Engenharia-Dados\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for londogard/londogard\n",
      "Fetching details for ftiosso/dio-curso-etl\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for Ribeiro20214543/Projetos_Data\n",
      "Fetching details for kalyani-subbiah/amazon-recommender\n",
      "Fetching details for DEE-GEMSTONE/ADVENTURE_WORKS\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for MLOPS-IE7374-Fall2024-G9/mlops-project\n",
      "Fetching details for csekankan/MTP-Recommender\n",
      "Fetching details for nabil0n/MachineLearning-Isak-Andersson\n",
      "Fetching details for shimkoji/ml_project_template\n",
      "Fetching details for UBC-MDS/DSCI_524_collab-sw-dev\n",
      "Fetching details for CertifaiAI/data-science-fundamentals\n",
      "Fetching details for posit-conf-2024/ds-workflows-python\n",
      "Fetching details for matteoferla/Fragment-hit-follow-up-chemistry\n",
      "Fetching details for derinman/L_Python-Note\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for matteoferla/EV-D68-3C-protease\n",
      "Fetching details for JhonCostaDev/Analytics-Engineering---Data-Quality-Dados-Airbnb\n",
      "Fetching details for hugocool/kedro-mlflow-bentoml\n",
      "Fetching details for JhonCostaDev/Analytics-Engineering---Data-Quality-Dados-Airbnb\n",
      "Fetching details for kh4r00n/Engenharia-de-Dados-_Projeto-Final-Soul-Code-Academy\n",
      "Fetching details for ngandlau/learning\n",
      "Fetching details for OLeandroRosa/Data-Analytics_and_Data-Science\n",
      "Fetching details for ohmamp/geo_arrete_peril_amp\n",
      "Fetching details for GBLONE/Fundamentos-de-ETL-com-Jupyter-e-Python\n",
      "Fetching details for Sarmad426/Python\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for MatsMoll/aligned-example-otovo\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for chonalchendo/effective-data-engineering\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for Leiryanny/Projetos-de-ETL-Analise-de-Dados\n",
      "Fetching details for matteoferla/EV-D68-3C-protease\n",
      "Fetching details for vreima/tie-dashboard\n",
      "Fetching details for Wania-Kazmi/Modern-Python\n",
      "Fetching details for MieleSantos/fundamentos_etl_python\n",
      "Fetching details for NatanMish/data_validation\n",
      "Fetching details for paulzuradzki/cddb-data-cleaning-project\n",
      "Fetching details for aqeel-spec/python-learning\n",
      "Fetching details for extralit/extralit\n",
      "Fetching details for andremrezende/bootcamp-data-engineer\n",
      "Fetching details for faranbutt/Practice_Python\n",
      "Fetching details for LauWyeRock/RecommendationSystem\n",
      "Fetching details for Wania-Kazmi/Modern-Python\n",
      "Fetching details for Jesshuan/Jedha-certification\n",
      "Fetching details for nicolasbini/tps-pod-nico-bini\n",
      "Fetching details for jhermann/jupyter-by-example\n",
      "Fetching details for Riniga/python\n",
      "Fetching details for ijaz0007/Python_programming\n",
      "Fetching details for kvnkho/demos\n",
      "Fetching details for KnowledgeCaptureAndDiscovery/SMA\n",
      "Fetching details for rocarva/analises-\n",
      "Fetching details for CDN2024/Projeto_Integrador2024\n",
      "Fetching details for MariaZharova/recsys_internship\n",
      "Fetching details for UBC-MDS/DSCI_522_Group36_taxi_fare_predictor\n",
      "Fetching details for landge/mri_protocols\n",
      "Fetching details for dmiglino/python_exercises\n",
      "Fetching details for mauriciopicirillo/Fundamentos-de-ETL-com_Python\n",
      "Fetching details for realdanizilla/Bootcamp-python\n",
      "Fetching details for Kinzermir/Learn_Python\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for manual123/Nacho-Jupyter-Notebooks\n",
      "Fetching details for PUC-RecSys-Class/RecSysPUC-2022\n",
      "Fetching details for denisparra/RecSysPUC-2024\n",
      "Fetching details for jonathanmagliano/etl_jupyter\n",
      "Fetching details for OLeandroRosa/Data-Analytics_and_Data-Science\n",
      "Fetching details for UBC-MDS/DSCI_524_collab-sw-dev\n",
      "Fetching details for UBC-MDS/DSCI_524_collab-sw-dev\n",
      "Fetching details for UBC-MDS/DSCI_524_collab-sw-dev\n",
      "Fetching details for moreymat/geo-arretes\n",
      "Fetching details for GBLONE/Fundamentos-de-ETL-com-Jupyter-e-Python\n",
      "Fetching details for KnowledgeCaptureAndDiscovery/SMA\n",
      "Fetching details for vbelz/Outliers_detection\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for khuyentran1401/Efficient_Python_tricks_and_tools_for_data_scientists\n",
      "Fetching details for dm-fedorov/pandas_basic\n",
      "Fetching details for mauriciopicirillo/Fundamentos-de-ETL-com_Python\n",
      "Fetching details for KelvinCorrea/Projeto_Integrador01\n",
      "Fetching details for fanyuy2/LLM-RL4Rec\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for ijaz0007/Python_programming\n",
      "Fetching details for DINAMOHMD/Movielens_Recommender_System-Project\n",
      "Fetching details for matteoferla/Zika-NS2B-NS3-elaborations\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for MariaZharova/recsys_internship\n",
      "Fetching details for ecamo19/sureau_ecos_py\n",
      "Fetching details for UBC-MDS/group04\n",
      "Fetching details for fullstackwd/Data_engineer\n",
      "Fetching details for enumerbs/Project3-Group6\n",
      "Fetching details for dac-sar/prep-flow\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for sotte/polars-tutorial\n",
      "Fetching details for Dinghow/MyRoadToMachineLearning\n",
      "Fetching details for constantine77/schema-inf\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for HAbuns/recommendation-system\n",
      "Fetching details for rezabmirzaei/python-bootcamp\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for cal-itp/ntd-modernization\n",
      "Fetching details for Matialevi/tp-pod\n",
      "Fetching details for csekankan/MTP-Recommender\n",
      "Fetching details for Lyarkh/Bootcamps_e_Intensivoes\n",
      "Fetching details for rosacarla/DIO-cloud-data-engineer\n",
      "Fetching details for baniasbaabe/delightful-data-science\n",
      "Fetching details for Igor-R-Amorim/Soulcode-Academy\n",
      "Fetching details for Guiils/analise_de_dados\n",
      "Fetching details for Lord-Kelsier/RecSys-Project\n",
      "Fetching details for DAGWorks-Inc/hamilton-tutorials\n",
      "Fetching details for cosmicBboy/cosmicBboy.github.io\n",
      "Fetching details for rafagitaccount/desafio-pandas\n",
      "Fetching details for dosumis/siletti_hacking\n",
      "Fetching details for Wania-Kazmi/Modern-Python\n",
      "Fetching details for PUC-RecSys-Class/RecSysPUC-2024-2\n",
      "Fetching details for javierIA/apibosch\n",
      "Fetching details for carlos-sojo/tp-pod\n",
      "Fetching details for AndersonGabrielCalasans/AnaliseDados-BootCampGeracaoTech-DIO\n",
      "Fetching details for UBC-MDS/DSCI_524_collab-sw-dev\n",
      "Fetching details for DemikFR/Oracle_Database\n",
      "Fetching details for catalyst-cooperative/pudl\n",
      "Fetching details for heyibad/learning-python\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for hiukiwong/erg-score-pipeline\n",
      "Fetching details for flyteorg/flyte-conference-talks\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for danielhocevar/Foundation-Model-for-Soccer\n",
      "Fetching details for arthurvale/Perda-de-percurso-para-Redes-LoRaWAN-em-Ambientes-Compostos\n",
      "Fetching details for lilacostaro/DIO_projrto_ETL\n",
      "Fetching details for edrmonteiro/DataSciencePython\n",
      "Fetching details for MariaZharova/Skillfactory_projects\n",
      "Fetching details for Deee92/journal\n",
      "Fetching details for istiyaksiddiquee/MagnumOpus\n",
      "Fetching details for matteoferla/EV-A71-2A-elaborations\n",
      "Fetching details for SidneyMoreira/bootCampsDIO\n",
      "Fetching details for KnowledgeCaptureAndDiscovery/SMA\n",
      "Fetching details for rajivsam/itsm_retail_examples_r2ds\n",
      "Fetching details for Binjian/pricing\n",
      "Fetching details for hamzaca/DifferentsProjects\n",
      "Fetching details for giobritos/soul_code_projeto_pandas_cenipa\n",
      "Fetching details for ankile/defi-measurement\n",
      "Fetching details for Wesley-Breno/Exercicios.py\n",
      "Fetching details for architkaila/recommenders_aipi590\n",
      "Fetching details for tloeber/MLOps-best-practices\n",
      "Fetching details for andremrezende/bootcamp-data-engineer\n",
      "Fetching details for MariaZharova/recsys_internship\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for ecamo19/sureau_ecos_py\n",
      "Fetching details for azfarsuhail/PythonDataAnalytics\n",
      "Fetching details for jthorvaldur/edelta\n",
      "Fetching details for gabrielcristhie/Estudos\n",
      "Fetching details for soadD/netflix_data\n",
      "Fetching details for tloeber/MLOps-best-practices\n",
      "Fetching details for Luan-r-b/Panderas_Presentation\n",
      "Fetching details for GBLONE/Fundamentos-de-ETL-com-Jupyter-e-Python\n",
      "Fetching details for MariaZharova/recsys_internship\n",
      "Fetching details for urosminoski/All-Digital-RF-Transmitter-in-FPGA-master-\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for PedroPauloMR/unifinance_credit_prediction\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for sashakaralchuk/ninja-move\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for AzizNadirov/datatest\n",
      "Fetching details for khuyentran1401/Efficient_Python_tricks_and_tools_for_data_scientists\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for ankile/surveillance-metric\n",
      "Fetching details for dagster-io/dagster\n",
      "Fetching details for apandey306/SD4DS\n",
      "Fetching details for khuyentran1401/Efficient_Python_tricks_and_tools_for_data_scientists\n",
      "Fetching details for 74th/test-python\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for kvnkho/demos\n",
      "Fetching details for AimonShakil/modern-python\n",
      "Fetching details for pedroachagas/energy_demand\n",
      "Fetching details for LauWyeRock/RecommendationSystem\n",
      "Fetching details for MariaZharova/recsys_internship\n",
      "Fetching details for Branden-Kang/Python-practice\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for posit-conf-2023/ds-workflows-python\n",
      "Fetching details for Imran691/python_practice\n",
      "Fetching details for d-diaz/lidar_plot_registration\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for arnaudguibbert/dataiku-analysis\n",
      "Fetching details for emilydolson/alife-phylogeny-tutorial\n",
      "Fetching details for aws-samples/modern-time-series-forecasting-on-aws\n",
      "Fetching details for kuatroka/visualisation_training\n",
      "Fetching details for JhonCostaDev/Analytics-Engineering---Data-Quality-Dados-Airbnb\n",
      "Fetching details for IvanNardini/demos\n",
      "Fetching details for Hikmat344/ML\n",
      "Fetching details for AegoCausalML/RecSysDataAugmentation\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for yukikongju/Miscellaneous-Projects\n",
      "Fetching details for tjhunter/climate-trace-handbook\n",
      "Fetching details for panaverse/learn-modern-python\n",
      "Fetching details for liviasantos08/ETL-com-Python\n",
      "Fetching details for Vizzuality/skytruth-30x30\n",
      "Fetching details for muhammadahmad1857/Python-all-assignments-and-practices\n",
      "Fetching details for Vizzuality/landgriffon\n",
      "Fetching details for kyaiooiayk/MLOps-Machine-Learning-Operations\n",
      "Fetching details for JhonCostaDev/Analytics-Engineering---Data-Quality-Dados-Airbnb\n",
      "Fetching details for csekankan/MTP-Recommender\n",
      "Fetching details for thaisgulias/DataAnalytics_student\n",
      "Fetching details for GBLONE/Fundamentos-de-ETL-com-Jupyter-e-Python\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for Kinzermir/Learn_Python\n",
      "Fetching details for ianozsvald/mot_pandas2_polars_dask\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for MieleSantos/fundamentos_etl_python\n",
      "Fetching details for mstromsnes/plotter\n",
      "Fetching details for muhammadahmad1857/Python-all-assignments-and-practices\n",
      "Fetching details for gabrielcristhie/Estudos\n",
      "Fetching details for oliveryh/toybox\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for aqeel-spec/python-learning\n",
      "Fetching details for Etistar/MACHINELEARNING_TITANIC\n",
      "Fetching details for smmartprogrammer/modernpython\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for ecamo19/sureau_ecos_py\n",
      "Fetching details for abdulsami13/Panaverse--Meta-course-\n",
      "Fetching details for mehulfollytobevice/Price_Alchemy\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for ui4054/Cuadernos\n",
      "Fetching details for fabiangunzinger/fabiangunzinger.github.io\n",
      "Fetching details for matteoferla/EV-A71-2A-elaborations\n",
      "Fetching details for Msaleemakhtar/Python-with-typehints\n",
      "Fetching details for sivasairam1437/Ineuron-Assignments\n",
      "Fetching details for Ribeiro20214543/Projetos_Data\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for architkaila/recommenders_aipi590\n",
      "Fetching details for AimonShakil/modern-python\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for MuhammadYaxir/Python\n",
      "Fetching details for azfarsuhail/PythonDataAnalytics\n",
      "Fetching details for JhonCostaDev/Analytics-Engineering---Data-Quality-Dados-Airbnb\n",
      "Fetching details for edrmonteiro/DataSciencePython\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for DAGWorks-Inc/hamilton-tutorials\n",
      "Fetching details for duc-hai/GreenFeast\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for Creative-Victor/phytonkursuppgifter\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for AbdullahKhetran/learning-modern-python\n",
      "Fetching details for wasim3399/batch-62\n",
      "Fetching details for MariaZharova/recsys_internship\n",
      "Fetching details for 74th/test-python\n",
      "Fetching details for vkola-lab/iscience2023\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for GuinsooLab/sheenflow\n",
      "Fetching details for merlinepedra/DAGSTER\n",
      "Fetching details for shanku007/pro-orch\n",
      "Fetching details for Kkdutta/Softwaredesign\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for tjhunter/climate-trace-handbook\n",
      "Fetching details for AbdullahKhetran/learning-modern-python\n",
      "Fetching details for MuhammadYaxir/Python\n",
      "Fetching details for muhammadahmad1857/python_class_17\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for khuyentran1401/reproducible-data-science\n",
      "Fetching details for h2es-institute/data-management-with-python\n",
      "Fetching details for CertifaiAI/data-science-fundamentals\n",
      "Fetching details for conda-forge/by-the-numbers\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for matteoferla/Fragment-hit-follow-up-chemistry\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for Precis-Digital/pandabear\n",
      "Fetching details for JesperDramsch/data-science-gui.de\n",
      "Fetching details for rajivsam/Rinse_and_Repeat\n",
      "Fetching details for thaisgulias/DataAnalytics_student\n",
      "Fetching details for kalyani-subbiah/amazon-recommender\n",
      "Fetching details for Kinzermir/Learn_Python\n",
      "Fetching details for Polonez1/taxis-analysis\n",
      "Fetching details for AidanG1/stat413-boxoffice\n",
      "Fetching details for GabrielfOtero/PythonPy\n",
      "Fetching details for unionai-oss/union-cloud-templates\n",
      "Fetching details for akamlani/keebler-studio\n",
      "Fetching details for kuatroka/visualisation_training\n",
      "Fetching details for dosumis/siletti_hacking\n",
      "Fetching details for rafaelladuarte/Python_Pandas\n",
      "Fetching details for matteoferla/EV-D68-3C-protease\n",
      "Fetching details for OPERA-Cal-Val/tile-mate\n",
      "Fetching details for Guiils/analise_de_dados\n",
      "Fetching details for matteoboh/urbem\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for Ivanbtz9/Master2_datascience\n",
      "Fetching details for Sentient-Sports/Strain-of-Success\n",
      "Fetching details for souhiab/reco-systems\n",
      "Fetching details for t-silvers/bifido_summary\n",
      "Fetching details for Tahirialaouiyassine/recommender-syst\n",
      "Fetching details for sccrosby/handy_data_tools\n",
      "Fetching details for Saarthakkj/bayesian_machine_learning\n",
      "Fetching details for OLeandroRosa/Data-Analytics_and_Data-Science\n",
      "Fetching details for KnowledgeCaptureAndDiscovery/SMA\n",
      "Fetching details for tjhunter/climate-trace-handbook\n",
      "Fetching details for biosustain/incawrapper\n",
      "Fetching details for danielfordfc/python-playground\n",
      "Fetching details for mauriciopicirillo/Fundamentos-de-ETL-com_Python\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for UBC-MDS/522-wine-quality-32\n",
      "Fetching details for decisionmechanics/lt539j\n",
      "Fetching details for Luan-r-b/Panderas_Presentation\n",
      "Fetching details for SidneyMoreira/bootCampsDIO\n",
      "Fetching details for Imran691/python_practice\n",
      "Fetching details for angelmaxxx/angelmaxxx\n",
      "Fetching details for posit-conf-2023/ds-workflows-python\n",
      "Fetching details for gerlichlab/spoc\n",
      "Fetching details for LauWyeRock/RecommendationSystem\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for flyteorg/flyte-conference-talks\n",
      "Fetching details for vibhorjoshi/-CHECK\n",
      "Fetching details for Wesley-Breno/Exercicios.py\n",
      "Fetching details for kuatroka/visualisation_training\n",
      "Fetching details for Guiils/analise_de_dados\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for matteoferla/Fragment-hit-follow-up-chemistry\n",
      "Fetching details for whanyu1212/predicting-default-of-a-public-firm\n",
      "Fetching details for thaisgulias/DataAnalytics_student\n",
      "Fetching details for y1u0d2a1i/youtube_view_count_prediction\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for matteoferla/Fragment-hit-follow-up-chemistry\n",
      "Fetching details for catalyst-cooperative/pudl\n",
      "Fetching details for facuvulcano/Arquitectura-y-Sist-Operativos\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for Zakaria-Boukeffa/Data-Cleaning-Guide\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for GhanshyamVerma/Explainable-Recommender-System\n",
      "Fetching details for kyaiooiayk/MLOps-Machine-Learning-Operations\n",
      "Fetching details for matteoferla/ASAP-dataset-matching\n",
      "Fetching details for MuhammadAhsaanAbbasi/python_PracticeSession\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for csekankan/MTP-Recommender\n",
      "Fetching details for muhammadahmad1857/python_class_18\n",
      "Fetching details for sunho/goranireader\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for DEE-GEMSTONE/ADVENTURE_WORKS\n",
      "Fetching details for markusmaresch/py_env_max\n",
      "Fetching details for usunyu/my-awesome-projects\n",
      "Fetching details for matteoferla/EV-D68-3C-protease\n",
      "Fetching details for AI-Researchers/KG-RL_Rec_Sys\n",
      "Fetching details for catherinenelson1/SEforDS\n",
      "Fetching details for panaverse/learn-modern-python\n",
      "Fetching details for jsal13/mlops_toys\n",
      "Fetching details for py-typedlogic/py-typedlogic\n",
      "Fetching details for tuttle-dev/tuttle\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for MuhammadYaxir/Python\n",
      "Fetching details for pone-software/ananke\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for jesrav/mlmodels\n",
      "Fetching details for noklam/kedro-pandera-iris\n",
      "Fetching details for chris-levine/project-3\n",
      "Fetching details for posit-conf-2023/ds-workflows-python\n",
      "Fetching details for ftiosso/dio-curso-etl\n",
      "Fetching details for krishagoti02/Machine-Learning\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for arnaudguibbert/dataiku-analysis\n",
      "Fetching details for WasteLabs/streamlit-gps-clustering-app\n",
      "Fetching details for AimonShakil/modern-python\n",
      "Fetching details for gabrielvlg/CDAF-2024\n",
      "Fetching details for Wesley-Breno/Exercicios.py\n",
      "Fetching details for rosacarla/DIO-cloud-data-engineer\n",
      "Fetching details for mehulfollytobevice/Price_Alchemy\n",
      "Fetching details for sadrahkm/Thesis-project\n",
      "Fetching details for AlfonosRon/TP-POD\n",
      "Fetching details for JhonCostaDev/Analytics-Engineering---Data-Quality-Dados-Airbnb\n",
      "Fetching details for lucaslapazini95/projetos_pandas_soulcode\n",
      "Fetching details for ersilia-os/model-inference-pipeline\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for Wesley-Breno/Exercicios.py\n",
      "Fetching details for UBC-MDS/BankMarketingPreditions-\n",
      "Fetching details for em-dat/em-test\n",
      "Fetching details for shtmjp/abeam_presentation_analysis\n",
      "Fetching details for jordi-zaragoza/Climbing-Route-Recommender\n",
      "Fetching details for Taecpy/validate_exim\n",
      "Fetching details for ijaz0007/Python_programming\n",
      "Fetching details for azfarsuhail/PythonDataAnalytics\n",
      "Fetching details for raphy-maia/raizen_challenge\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for pandera-dev/pandera-blog\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for fourmodern/toc_tutorial_colab\n",
      "Fetching details for DAGWorks-Inc/hamilton-tutorials\n",
      "Fetching details for liviasantos08/ETL-com-Python\n",
      "Fetching details for andreale28/spark-playground\n",
      "Fetching details for unionai-oss/pandera-presentations\n",
      "Fetching details for wasim3399/batch-62\n",
      "Fetching details for mski-iksm/CodenameMaster\n",
      "Fetching details for therafiali/learn_python\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for Moorad/social-network-for-education\n",
      "Fetching details for abdulsami13/Panaverse--Meta-course-\n",
      "Fetching details for kuatroka/visualisation_training\n",
      "Fetching details for rrwabina/RADI603\n",
      "Fetching details for deun115/Recommender\n",
      "Fetching details for faranbutt/Practice_Python\n",
      "Fetching details for jvitorcosta/dio-bootcamp-projects\n",
      "Fetching details for TheCulliganMan/data-science-learning-owen\n",
      "Fetching details for p7k/nu_tyo_lab_analysis\n",
      "Fetching details for rosacarla/DIO-cloud-data-engineer\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for MarcosDemetry/auto_personregister\n",
      "Fetching details for panaverse/learn-modern-python\n",
      "Fetching details for csekankan/MTP-Recommender\n",
      "Fetching details for AlisonZa/ci-cd-machine-learning\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for stephentown42/survival_analysis\n",
      "Fetching details for AbdullahKhetran/learning-modern-python\n",
      "Fetching details for MuhammadYaxir/Python\n",
      "Fetching details for nelisjunior/ETL_Python\n",
      "Fetching details for jonathanmagliano/etl_jupyter\n",
      "Fetching details for AbdullahKhetran/learning-modern-python\n",
      "Fetching details for khuyentran1401/Data-science\n",
      "Fetching details for kvnkho/demos\n",
      "Fetching details for bedrock-gi/bedrock-gi\n",
      "Fetching details for erykml/medium_articles\n",
      "Fetching details for laminlabs/lamindb\n",
      "Fetching details for KausthubK/python_playground\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for lgarzia/library_explorations\n",
      "Fetching details for matteoferla/COVID-Moonshot-metadata-analysis\n",
      "Fetching details for DAGWorks-Inc/hamilton-tutorials\n",
      "Fetching details for datamindedbe/conveyor-samples\n",
      "Fetching details for Xjkslhstxn/BT4222\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for lucaslapazini95/projetos_pandas_soulcode\n",
      "Fetching details for Melanie07-lab/Trabajos-Practicos\n",
      "Fetching details for quan3010/transfer_learning\n",
      "Fetching details for RTIInternational/teehr\n",
      "Fetching details for chonalchendo/transfermarket_scraper\n",
      "Fetching details for matteoferla/Zika-NS2B-NS3-elaborations\n",
      "Fetching details for data-notes/examples\n",
      "Fetching details for vpercuoco/iodptools\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for CatrielD/msscae-tp-final\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for AegoCausalML/CPR_Code\n",
      "Fetching details for posit-conf-2023/ds-workflows-python\n",
      "Fetching details for JhonCostaDev/Analytics-Engineering---Data-Quality-Dados-Airbnb\n",
      "Fetching details for daSpinelli/ml_flow\n",
      "Fetching details for Riniga/python\n",
      "Fetching details for gianni-mana/TP_POD\n",
      "Fetching details for muhammadahmad1857/Python-all-assignments-and-practices\n",
      "Fetching details for sashakaralchuk/ninja-move\n",
      "Fetching details for tamilynpeck/python-playground\n",
      "Fetching details for lilacostaro/DIO_projrto_ETL\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for AI-Researchers/KG-RL_Rec_Sys\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for flyteorg/flyte-conference-talks\n",
      "Fetching details for paulzuradzki/cddb-data-cleaning-project\n",
      "Fetching details for AegoCausalML/RecSysDataAugmentation\n",
      "Fetching details for matteoferla/Fragment-hit-follow-up-chemistry\n",
      "Fetching details for Leticiavalcan/Curso-engenharia-de-dados\n",
      "Fetching details for kmalkow/software-portfolio\n",
      "Fetching details for ianozsvald/mot_pandas2_polars_dask\n",
      "Fetching details for matteoferla/Fragment-hit-follow-up-chemistry\n",
      "Fetching details for JesperDramsch/ml.recipes\n",
      "Fetching details for KnowledgeCaptureAndDiscovery/SMA\n",
      "Fetching details for fabiangunzinger/fabiangunzinger.github.io\n",
      "Fetching details for kh4r00n/Engenharia-de-Dados-_Projeto-Final-Soul-Code-Academy\n",
      "Fetching details for Guiils/analise_de_dados\n",
      "Fetching details for AegoCausalML/RecSysDataAugmentation\n",
      "Fetching details for matteoferla/EV-A71-2A-elaborations\n",
      "Fetching details for lmxhappy/rec_models\n",
      "Fetching details for Lyarkh/Bootcamps_e_Intensivoes\n",
      "Fetching details for deployment-gap-model-education-fund/deployment-gap-model\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for DemikFR/Oracle_Database\n",
      "Fetching details for DanielOberg/cv-to-jobs\n",
      "Fetching details for landge/mri_protocols\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for acheshkov/pytracebugs_pipeline\n",
      "Fetching details for xpander-ai/xpander-agents-hub\n",
      "Fetching details for alancechin/mlflow_project\n",
      "Fetching details for gerlichlab/spoc\n",
      "Fetching details for matteoferla/Fragment-hit-follow-up-chemistry\n",
      "Fetching details for Franklinmoura/WebDeveloping_with_Python\n",
      "Fetching details for MadRapz/Analytics-Data\n",
      "Fetching details for bvgori/tp-pod\n",
      "Fetching details for daaamon84/Pythonkurs-Submission\n",
      "Fetching details for meetup-python-grenoble/meetup-python-grenoble.github.io\n",
      "Fetching details for coobas/robust-pandas-workshop\n",
      "Fetching details for p7k/nu_tyo_lab_analysis\n",
      "Fetching details for florenciasvriz/POD\n",
      "Fetching details for DEE-GEMSTONE/ADVENTURE_WORKS\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for lgarzia/library_explorations\n",
      "Fetching details for matteoferla/COVID-Moonshot-metadata-analysis\n",
      "Fetching details for Ministry-of-Learning-of-Jura-Kites/Originium-Python\n",
      "Fetching details for Vizzuality/mangrove-atlas\n",
      "Fetching details for kvnkho/demos\n",
      "Fetching details for muhammadahmad1857/python_class_17\n",
      "Fetching details for matteoferla/EV-D68-3C-protease\n",
      "Fetching details for fabiangunzinger/fabiangunzinger.github.io\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for ianozsvald/mot_pandas2_polars_dask\n",
      "Fetching details for pollianasilva/Soulcode\n",
      "Fetching details for LauWyeRock/RecommendationSystem\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for Shafqatsarwar/Python-classes\n",
      "Fetching details for noah-pitts/Welcome-to-the-Cloud\n",
      "Fetching details for csekankan/recommender-systems\n",
      "Fetching details for ianozsvald/uplift_experiment\n",
      "Processing query: apachegriffin extension:.yml\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml', 'per_page': 100, 'page': 1}\n",
      "Page 1: 14 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml', 'per_page': 100, 'page': 2}\n",
      "Fetching details for apache/griffin\n",
      "Fetching details for tanxinzheng/docker-image-demo\n",
      "Fetching details for apache/griffin\n",
      "Fetching details for dershov173/Spark-Streaming-Project\n",
      "Fetching details for zcswl7961/apache-griffin-expand\n",
      "Fetching details for ep-infosec/33_apache_griffin\n",
      "Fetching details for 15802519394/griffin\n",
      "Fetching details for rohan-flutterint/griffin\n",
      "Fetching details for rohan-flutterint/griffin-scala\n",
      "Fetching details for zcswl7961/apache-griffin-expand\n",
      "Fetching details for ep-infosec/33_apache_griffin\n",
      "Fetching details for 15802519394/griffin\n",
      "Fetching details for rohan-flutterint/griffin\n",
      "Fetching details for rohan-flutterint/griffin-scala\n",
      "Processing query: tensorflow_data_validation extension:.py\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 1}\n",
      "Page 1: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 2}\n",
      "Page 2: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 3}\n",
      "Page 3: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 4}\n",
      "Page 4: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 5}\n",
      "Page 5: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 6}\n",
      "Page 6: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 7}\n",
      "Page 7: 60 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 8}\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'tensorflow_data_validation extension:.py', 'per_page': 100, 'page': 9}\n",
      "Fetching details for tensorflow/datasets\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for kubeflow/pipelines\n",
      "Fetching details for GoogleCloudPlatform/professional-services\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for JhonVK/DeepLearning\n",
      "Fetching details for oreilly-japan/building-ml-pipelines-ja\n",
      "Fetching details for spotify/spotify-tensorflow\n",
      "Fetching details for ksalama/tfx2-workshop\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for luotigerlsx/tensorflow_tfx\n",
      "Fetching details for Miautawn/Miautawn-Auto-Validate-By-History\n",
      "Fetching details for jinwoo1990/mlops-demo\n",
      "Fetching details for JanetVictorious/tfx-pipeline\n",
      "Fetching details for malikamalik/vertexe2e\n",
      "Fetching details for MLOps-BlueBikes/PedalPulse\n",
      "Fetching details for safdar-tugunet/tensorflow-datasets\n",
      "Fetching details for weichen456/datasets-master\n",
      "Fetching details for Marshallshaddy/Chatbot-with-TensorFlow\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for romanonly/romankazinnik_blog\n",
      "Fetching details for fernandoGitHub/MLOPS_GSD\n",
      "Fetching details for proganalysis/python3_types\n",
      "Fetching details for SmartDeployAI/chicago-taxi-cab\n",
      "Fetching details for elamraniadnane1/scripts\n",
      "Fetching details for ciandt-d1/chicago-taxi-forecast\n",
      "Fetching details for pshah16/KubeflowNotebookPipelineDeployment\n",
      "Fetching details for reflexiveprophecy/twiml_bmlp_practice\n",
      "Fetching details for samuelmoss572/dataflowworkshop\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for cabrossman/ai-kube-pipe\n",
      "Fetching details for salilkanitkar/responsible_ai_hackathon\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for VatsalPatel18/Tprojects\n",
      "Fetching details for ashishramtri/Apache-Beam\n",
      "Fetching details for proganalysis/python3_types\n",
      "Fetching details for elamraniadnane1/scripts\n",
      "Fetching details for HarshaRockzz/TFX-ML-Production-Pipeline-for-Sentiment-Analysis\n",
      "Fetching details for taka-sho/kedro-titanic\n",
      "Fetching details for maggiemhanna/ml_pipeline_tfx_libraries\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for alexander-dev-hub/apache-beam\n",
      "Fetching details for Samagra-Development/machine-learning-platform\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for sukrishnus/Kube-Pipelines\n",
      "Fetching details for GoogleCloudPlatform/mlops-with-vertex-ai\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for KUrushi/advent_calendar_2019\n",
      "Fetching details for YaxitaAmin/LeafDoctorAI\n",
      "Fetching details for fernandoGitHub/MLOPS_GSD\n",
      "Fetching details for hshekhar24/Drift_detection_categorical\n",
      "Fetching details for d68864767/OpenAI-API-Key-11\n",
      "Fetching details for elamraniadnane1/scripts\n",
      "Fetching details for proganalysis/python3_types\n",
      "Fetching details for abdala9512/mlops-specialization-project\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for dlabsai/tfx-helper\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for pvn25/ML-Data-Prep-Zoo\n",
      "Fetching details for hhaggan/LearningTF\n",
      "Fetching details for a0x8o/drill\n",
      "Fetching details for Mastercard/mastercard-labs-ml-pipeline\n",
      "Fetching details for SatwikReddySripathi/Multiclass_Disease_Classification\n",
      "Fetching details for zivschwartz/Data-Quality-Validation-Pipeline\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for pramabhi/some-ops-test\n",
      "Fetching details for christopherohit/MLPipeLine\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for ramonbgc/vertexai_mlops\n",
      "Fetching details for dlabsai/tfx-helper\n",
      "Fetching details for narayana-3005/RAG-based-Financial-Analysis-Chatbot\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for remla2022/stackoverflow-tagger\n",
      "Fetching details for maggiemhanna/ml_pipeline_tfx_libraries\n",
      "Fetching details for lfloretta/tf_ecosystem\n",
      "Fetching details for elamraniadnane1/CSC5382_SP24_FINALPROJECT\n",
      "Fetching details for denisshaf/DSPractice\n",
      "Fetching details for oracle-japan/OCHa_S5_MLOps\n",
      "Fetching details for eaugeas/hilo-tfx\n",
      "Fetching details for FergusCurrie/icloud\n",
      "Fetching details for pshah16/KubeflowNotebookPipelineDeployment\n",
      "Fetching details for MohanKrishna-RC/Explorations_proj\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for 2lambda123/tensorflow-datasets\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for intro-to-ml-with-kubeflow/intro-to-ml-with-kubeflow-examples\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for JanetVictorious/tfx-example-pipeline\n",
      "Fetching details for ra312/credit_score\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for amygdala/code-snippets\n",
      "Fetching details for elamraniadnane1/scripts\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for reevald/tfdf-ft-pipeline\n",
      "Fetching details for TheSwarnim/pipelines\n",
      "Fetching details for czwiessler/sq-llm-docstring-gen-kt\n",
      "Fetching details for TechDom/MLRepositories\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for malikamalik/vertex-pipelines-end-to-end\n",
      "Fetching details for 26Kchiu/vertexpipeline\n",
      "Fetching details for karlerliksson/explainability_transfer\n",
      "Fetching details for proganalysis/python3_types\n",
      "Fetching details for xsm110/Beam15.0\n",
      "Fetching details for pravk03/responsible_ai_hackathon\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for rohan-flutterint/beam\n",
      "Fetching details for LorroWijn/AI_Minor_Tensorflow\n",
      "Fetching details for suez1224/beam\n",
      "Fetching details for CarenRizk/soen6491Dataset\n",
      "Fetching details for proganalysis/python3_types\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for kubeflow-pk/pipelines-0.2.5\n",
      "Fetching details for chironghua/kubeflow-piplines\n",
      "Fetching details for sureindia-in/pipelines-kfp\n",
      "Fetching details for pratibhadeepti/kubeflow-pipeline-gcp\n",
      "Fetching details for Garima1221/MLOps\n",
      "Fetching details for oonisim/python-programs\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for proganalysis/python3_types\n",
      "Fetching details for a0x8o/presto\n",
      "Fetching details for a0x8o/druid\n",
      "Fetching details for pstoerchli/Apache-Beam\n",
      "Fetching details for gimral/beam\n",
      "Fetching details for pamarquez/pipelineHW\n",
      "Fetching details for Parthi101193/Kubeflow-pipeline\n",
      "Fetching details for cdyangzhenyu/kubeflow-example\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for ayman99us/mlops_gcp\n",
      "Fetching details for kartikxsahu/quiccklabs\n",
      "Fetching details for pankaj-saxena/MLOPS\n",
      "Fetching details for Jufz1999/gcp\n",
      "Fetching details for maggiemhanna/ml_pipeline_tfx_libraries\n",
      "Fetching details for JanetVictorious/tfx-pipeline\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for GoogleCloudPlatform/mlops-with-vertex-ai\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for NVIDIA-Merlin/gcp-ml-ops\n",
      "Fetching details for ksalama/ucaip-labs\n",
      "Fetching details for schelterlabs/jenga\n",
      "Fetching details for GoogleCloudPlatform/analytics-componentized-patterns\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for hhaggan/LearningTF\n",
      "Fetching details for tensorflow/tfx-addons\n",
      "Fetching details for rcrowe-google/wit\n",
      "Fetching details for t9k/tutorial-examples\n",
      "Fetching details for abdvl/tensorflow_v2_examples\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for yirutang/insert-id\n",
      "Fetching details for rafiqhasan/auto-tensorflow\n",
      "Fetching details for ahmadhasanuddin10/pipeline\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for JonFillip/lastfm_music_recommender\n",
      "Fetching details for aorursy/old-nb\n",
      "Fetching details for Amrita-TIFAC-Cyber-Blockchain/Coursera-Datascience-and-Machine-Learning\n",
      "Fetching details for Hydracerynitis/Ethical-Framework\n",
      "Fetching details for VatsalPatel18/Tprojects\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for databydepew/mlops\n",
      "Fetching details for safdar-tugunet/tensorflow-datasets\n",
      "Fetching details for toshk9/PredictFutureSales\n",
      "Fetching details for hhaggan/LearningTF\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for juniordiem/pythonProject\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for SatwikReddySripathi/Multiclass_Disease_Classification\n",
      "Fetching details for hhaggan/LearningTF\n",
      "Fetching details for pcavad/ETL-EDA-timeseries\n",
      "Fetching details for reachyu/pipelines-0.1.31\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for adinjayp/anti_money_laundering_project\n",
      "Fetching details for clehensen/tfx\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for aliz-ai/ml-spec-2022\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for adinjayp/anti_money_laundering_project\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for API-Misuse/MisuAPI\n",
      "Fetching details for lfloretta/tf_ecosystem\n",
      "Fetching details for smelly-python/so-tags\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for remla2022/stackoverflow-tagger\n",
      "Fetching details for adinjayp/anti_money_laundering_project\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for ptaraconat/mltools\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for Garima1221/MLOps\n",
      "Fetching details for oonisim/python-programs\n",
      "Fetching details for se-jaeger/data-imputation-paper\n",
      "Fetching details for CUniversityaccount/DataPreperationProject\n",
      "Fetching details for Dittrich-Pascal/Data-Imputation-Thesis\n",
      "Fetching details for ptsialis/MultiColumnCorruption\n",
      "Fetching details for ychernik/bqml-scann\n",
      "Fetching details for fenzhantw/BQ_AUTOML\n",
      "Fetching details for Christopher-Xavier/tfx\n",
      "Fetching details for alexandernh/copy_examplegen\n",
      "Fetching details for Christopher-Xavier/tfx\n",
      "Fetching details for alexandernh/copy_examplegen\n",
      "Fetching details for FernandoDorado/Google-Cloud-MLOps-Training\n",
      "Fetching details for Lorioux/data-analysis-course\n",
      "Fetching details for FernandoDorado/Google-Cloud-MLOps-Training\n",
      "Fetching details for pawantanay/gcp\n",
      "Fetching details for VatsalPatel18/Tprojects\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for guildai/guildai\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for googleapis/python-aiplatform\n",
      "Fetching details for ksalama/ucaip-labs\n",
      "Fetching details for salilkanitkar/responsible_ai_hackathon\n",
      "Fetching details for JJublanc/birds_detection\n",
      "Fetching details for p-chk/uostore\n",
      "Fetching details for bhorkar/geolocation_tensorflow_2.0\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for MichalGasiorowski/tfx-titanic-training\n",
      "Fetching details for MLOPS-Team-7/Telecom-Device-Upgrade-Prediction\n",
      "Fetching details for neelanap1999/MLOPSPROJECT\n",
      "Fetching details for YaxitaAmin/LeafDoctorAI\n",
      "Fetching details for cabrossman/ai-kube-pipe\n",
      "Fetching details for espv/beam-plus-wrapper\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for VatsalPatel18/Tprojects\n",
      "Fetching details for amine1992/Booking-prediction-using-tfx\n",
      "Fetching details for EdwardCuiPeacock/nbcu-metadata-enhancement\n",
      "Fetching details for ksalama/tfx-workshop\n",
      "Fetching details for xuw10/kubeflow-tfx-workshop\n",
      "Fetching details for MLOps-BlueBikes/PedalPulse\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for Sfeir/devfest_mlops\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for API-Misuse/MisuAPI\n",
      "Fetching details for rishav1304/ecommerceprod\n",
      "Fetching details for YT37/Machine-DeepLearning\n",
      "Fetching details for elamraniadnane1/scripts\n",
      "Fetching details for christopherohit/MLPipeLine\n",
      "Fetching details for aorursy/new-nb-6\n",
      "Fetching details for karlerliksson/explainability_transfer\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for chusri/mlrs_ec2\n",
      "Fetching details for neelanap1999/MLOPSPROJECT\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for d68864767/OpenAI-API-Key-7\n",
      "Fetching details for API-Misuse/MisuAPI\n",
      "Fetching details for karlerliksson/explainability_transfer\n",
      "Fetching details for drskaf/EHR-Prediction-Model-with-Tensorflow\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for CRSilkworth/mlp\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for inureyes/TFX-DevSummit-2019\n",
      "Fetching details for weichen456/datasets-master\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for RootCee/nerdiesyndicatebot\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for pshah16/KubeflowNotebookPipelineDeployment\n",
      "Fetching details for rcrowe-google/schemacomponent\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for YaxitaAmin/LeafDoctorAI\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for Saravanan-Arumugam3/Advanced-NLP-Based-Amazon-Reviews-Analytics\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for remla23-team13/model-training\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for epm157/python-projects\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for AhmedShafique313/AutoBiz-Genie\n",
      "Fetching details for Zuboy/Screen.Scout\n",
      "Fetching details for Janice69/AIBC_Project\n",
      "Fetching details for Avhaxx/ChatBot-telegram\n",
      "Fetching details for pravk03/responsible_ai_hackathon\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for dev-agra/IndianSignLanguage\n",
      "Fetching details for manishetty1/ApacheBeam-Learning\n",
      "Fetching details for bhaveshbendale/data-science-on-aws\n",
      "Fetching details for vijay-khanna/data-science-on-aws-workshop\n",
      "Fetching details for Chalcym/AWS_datascience_workshop\n",
      "Fetching details for sureindia-in/data-science-on-aws-archive-good\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for Sfeir/devfest_mlops\n",
      "Fetching details for prakashraj15/dynamo_card\n",
      "Fetching details for arhansuba/crew-ai-cv-generator\n",
      "Fetching details for ayman99us/mlops_gcp\n",
      "Fetching details for kartikxsahu/quiccklabs\n",
      "Fetching details for pankaj-saxena/MLOPS\n",
      "Fetching details for skipper-com/Work_study_examples\n",
      "Fetching details for Trinity1stproject/Avartha.io\n",
      "Fetching details for muskanvip009/Internship-Practice-Task\n",
      "Fetching details for ArmaanSeth/markIT\n",
      "Fetching details for Rabbonos/langhack\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for rcrowe-google/schemacomponent\n",
      "Fetching details for dev-agra/IndianSignLanguage\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for 2lambda123/tensorflow-datasets\n",
      "Fetching details for spotify/spotify-tensorflow\n",
      "Fetching details for tensorflow/datasets\n",
      "Fetching details for NVIDIA-Merlin/gcp-ml-ops\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for hackcheek/UMA-code-demo\n",
      "Fetching details for GoogleCloudPlatform/professional-services\n",
      "Fetching details for intel/intel-xai-tools\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for Najeeb-Aqel/MLOperationSpecialization\n",
      "Fetching details for Arisiru/experiments-tfx-airflow\n",
      "Fetching details for pshah16/KubeflowNotebookPipelineDeployment\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for Miautawn/Miautawn-Auto-Validate-By-History\n",
      "Fetching details for schelterlabs/deml-lab\n",
      "Fetching details for roanbrasil/MGL869-IA\n",
      "Fetching details for jhon2301922021/GCP_Vertex_pipelin\n",
      "Fetching details for Pranathi-Manusanipalli/BigDataPipelines\n",
      "Fetching details for amine-google/vertex_ai\n",
      "Fetching details for amygdala/code-snippets\n",
      "Fetching details for dhavalmj007/model-serving\n",
      "Fetching details for pietro-tanure/tfx_document_ai\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for JanetVictorious/tfx-pipeline\n",
      "Fetching details for amygdala/code-snippets\n",
      "Fetching details for entrpn/entrpn-kfx\n",
      "Fetching details for christopherohit/MLPipeLine\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for MohanKrishna-RC/Explorations_proj\n",
      "Fetching details for kylinsoong/ai-quickstarts\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for weichen456/datasets-master\n",
      "Fetching details for eaugeas/hilo-tfx\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for henriquepeixoto/Data-science-cool-stuff\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for ep-infosec/50_google_nitroml\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for adinjayp/anti_money_laundering_project\n",
      "Fetching details for guravtanvi/Big-Data-Systems-and-Int-Analytics-INFO-7245\n",
      "Fetching details for API-Misuse/MisuAPI\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for ep-infosec/50_google_driblet\n",
      "Fetching details for safdar-tugunet/tensorflow-datasets\n",
      "Fetching details for tensorflow/tfx-addons\n",
      "Fetching details for dhercher/beam-old\n",
      "Fetching details for colingwuyu/mlops\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for pshah16/KubeflowNotebookPipelineDeployment\n",
      "Fetching details for javiow/ml-project-basic-setting\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for chironghua/kubeflow-piplines\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for cabrossman/ai-kube-pipe\n",
      "Fetching details for anifort/vertex-mlops-demo\n",
      "Fetching details for d68864767/OpenAI-API-Key-11\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for arangoml/arangopipe\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for Jwuthri/tfx\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for 2lambda123/tensorflow-datasets\n",
      "Fetching details for amine-google/vertex_ai\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for jth1911/ml-gcp\n",
      "Fetching details for ayman99us/mlops_gcp\n",
      "Fetching details for kartikxsahu/quiccklabs\n",
      "Fetching details for pankaj-saxena/MLOPS\n",
      "Fetching details for skipper-com/Work_study_examples\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for holladileep/CSYE7245-Spring2021-Labs\n",
      "Fetching details for akashmdubey/Big-Data-Intelligent-Analytics\n",
      "Fetching details for Pranathi-Manusanipalli/BigDataPipelines\n",
      "Fetching details for keerti-26/BigDataSystems_CSYE7245_Spring2021_Team2\n",
      "Fetching details for alexandernh/copy_examplegen\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for sureindia-in/pipelines-kfp\n",
      "Fetching details for pratibhadeepti/kubeflow-pipeline-gcp\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for Marshallshaddy/Chatbot-with-TensorFlow\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for spotify/spotify-tensorflow\n",
      "Fetching details for epm157/python-projects\n",
      "Fetching details for apache/beam\n",
      "Fetching details for schelterlabs/jenga\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for p-chk/uostore\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for rajagurunath/mlvajra\n",
      "Fetching details for JJublanc/birds_detection\n",
      "Fetching details for Najeeb-Aqel/MLOperationSpecialization\n",
      "Fetching details for JanetVictorious/tfx-pipeline\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for VatsalPatel18/Tprojects\n",
      "Fetching details for PriyankaM06091994/Big-Data-Systems-and-Int-Analytics\n",
      "Fetching details for avnyadav/aiops_projects\n",
      "Fetching details for jarokaz/vertex-ai-workshop\n",
      "Fetching details for pshah16/KubeflowNotebookPipelineDeployment\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for xxxtrillionarie/KubeFlow_MLOps_Pipelines\n",
      "Fetching details for SatwikReddySripathi/Multiclass_Disease_Classification\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for YaxitaAmin/LeafDoctorAI\n",
      "Fetching details for gabrielwen/LinearModel\n",
      "Fetching details for velascoluis/cnn-sentence-classifier-dev\n",
      "Fetching details for cabrossman/ai-kube-pipe\n",
      "Fetching details for godatadriven/tfx-airflow-code-breakfast\n",
      "Fetching details for GoogleCloudPlatform/analytics-componentized-patterns\n",
      "Fetching details for hannahsophieMWB/Machine_Learning\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for VatsalPatel18/Tprojects\n",
      "Fetching details for JonFillip/lastfm_music_recommender\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for Mushroomjump/IwazolabWebpage\n",
      "Fetching details for TanveerMittal/Sortinghat_Benchmark_Replication\n",
      "Fetching details for dlabsai/tfx-helper\n",
      "Fetching details for Glairly/introduction_to_tensorflow\n",
      "Fetching details for ksalama/tfx-workshop\n",
      "Fetching details for State-of-The-MLOps/MLOps\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for christopherohit/MLPipeLine\n",
      "Fetching details for spotify/spotify-tensorflow\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for se-jaeger/data-imputation-paper\n",
      "Fetching details for CUniversityaccount/DataPreperationProject\n",
      "Fetching details for Dittrich-Pascal/Data-Imputation-Thesis\n",
      "Fetching details for ptsialis/MultiColumnCorruption\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for avnyadav/aiops_projects\n",
      "Fetching details for jarokaz/vertex-ai-workshop\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for juliusvonkohout/pipelines-namespace-isolation\n",
      "Fetching details for raajeshlr/Session6K8sPyTorchKubeFlowHandsOn\n",
      "Fetching details for rahuldsce/EMLO\n",
      "Fetching details for devoure/ml-ops-test\n",
      "Fetching details for dev-agra/IndianSignLanguage\n",
      "Fetching details for ychernik/bqml-scann\n",
      "Fetching details for fenzhantw/BQ_AUTOML\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for sparky0520/CBRE\n",
      "Fetching details for Bharathhh30/Crew-AI\n",
      "Fetching details for RedPill47/Personalized_Learning_with_ChatGPT\n",
      "Fetching details for dwoldeyohannes/Gemini-Explorer\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for ayman99us/mlops_gcp\n",
      "Fetching details for kartikxsahu/quiccklabs\n",
      "Fetching details for pankaj-saxena/MLOPS\n",
      "Fetching details for Jufz1999/gcp\n",
      "Fetching details for czwiessler/sq-llm-docstring-gen-kt\n",
      "Fetching details for TechDom/MLRepositories\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for weichen456/datasets-master\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for MLOPS-Team-7/Telecom-Device-Upgrade-Prediction\n",
      "Fetching details for dvquy13/seq-rec\n",
      "Fetching details for ndtands/mlops\n",
      "Fetching details for saifrahmed/tfx-local\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for robertf99/tfx-e2e\n",
      "Fetching details for tiksharsh/model_drift_marketing_campaign\n",
      "Fetching details for KodeJaiSurya/Vibify\n",
      "Fetching details for elamraniadnane1/CSC5382_SP24_FINALPROJECT\n",
      "Fetching details for sergred/unit-tests-ml-python\n",
      "Fetching details for SatwikReddySripathi/Multiclass_Disease_Classification\n",
      "Fetching details for Marshallshaddy/Chatbot-with-TensorFlow\n",
      "Fetching details for GoogleCloudPlatform/analytics-componentized-patterns\n",
      "Fetching details for gabrielwen/LinearModel\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for YipengHan/vertex_ai_pipeline_e2e\n",
      "Fetching details for cabrossman/ai-kube-pipe\n",
      "Fetching details for maggiemhanna/ml_pipeline_tfx_libraries\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for CardosoJr/ml\n",
      "Fetching details for tensorflow/tfx\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for JonFillip/lastfm_music_recommender\n",
      "Fetching details for tottenjordan/vertex-pipelines-e2e\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for cabrossman/ai-kube-pipe\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for tensorflow/data-validation\n",
      "Fetching details for karlerliksson/explainability_transfer\n",
      "Fetching details for ychernik/bqml-scann\n",
      "Fetching details for fenzhantw/BQ_AUTOML\n",
      "Fetching details for sprydog2964/vertex-pipelines-end-to-end-samples\n",
      "Fetching details for cuebelhoer/mlopse2e\n",
      "Fetching details for kezure/vertex-pipelines-example\n",
      "Fetching details for Boban00S/vertex-ai-test\n",
      "Fetching details for jared-burns/money-gram-mlops-demo\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Fetching details for demetriusengdados/Data_Validation\n",
      "Processing query: test filename:dbt_project extension:.yml\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 1}\n",
      "Page 1: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 2}\n",
      "Page 2: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 3}\n",
      "Page 3: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 4}\n",
      "Page 4: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 5}\n",
      "Page 5: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 6}\n",
      "Page 6: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 7}\n",
      "Page 7: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 8}\n",
      "Page 8: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 9}\n",
      "Page 9: 100 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'test filename:dbt_project extension:.yml', 'per_page': 100, 'page': 10}\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Fetching details for Health-Union/dbt-snowshu-postgres\n",
      "Fetching details for databricks/bundle-examples\n",
      "Fetching details for anna-geller/prefect-dataplatform\n",
      "Fetching details for techindicium/dbt-dag-monitoring\n",
      "Fetching details for barisbilen1/dbt-mini-project_london_biking\n",
      "Fetching details for Lucasouza98/dbtproject\n",
      "Fetching details for kirthanavivekanadan/dbt_test\n",
      "Fetching details for caiocvelasco/project02-docker-dbt-migration-medallion-kimball-postgres\n",
      "Fetching details for g0v/tw_campaign_finance\n",
      "Fetching details for pr0digii/DBT\n",
      "Fetching details for theianchan/dbt-learn-ichan\n",
      "Fetching details for DucPhuocVan/dbt_project_etl_bi\n",
      "Fetching details for NhatTan1232/Transform-visualize-data-project\n",
      "Fetching details for ergest/sql_patterns\n",
      "Fetching details for olikelly00/think-thrift\n",
      "Fetching details for nightscape/dbt-experiments\n",
      "Fetching details for superbolt-x/bolt-dbt-shopify-package\n",
      "Fetching details for Justus-coded/electric_vehicle_de\n",
      "Fetching details for hhigorb/dbt_training\n",
      "Fetching details for inondavidw/dtb-wiztech\n",
      "Fetching details for crassula-ovata/data_and_analytics_dbt_procedures\n",
      "Fetching details for jolares/demo-dbt\n",
      "Fetching details for snowplow/dbt-snowplow-fractribution\n",
      "Fetching details for gliter/dbt-flink-adapter-example\n",
      "Fetching details for edbizarro/dbt-best-practices\n",
      "Fetching details for rakesh-singh-byte/Sales-Analytics\n",
      "Fetching details for FinnCharlton/dbt_fundamentals\n",
      "Fetching details for abasa-cloud/healthcare_semantic\n",
      "Fetching details for barberscott/dbt-learn-sbarber\n",
      "Fetching details for LuccaCoelho/DBT-Pipeline-Project\n",
      "Fetching details for gretalerer3/system-activity-monitoring\n",
      "Fetching details for Mahdi-Moosa/dbt_de_zoomcamp-2023-week-4\n",
      "Fetching details for IsabelTThompson/gz-dbt-repository\n",
      "Fetching details for RacoFernandez/dbt-fundamentals\n",
      "Fetching details for FlipsideCrypto/eclipse-models\n",
      "Fetching details for Ashraf1395/customer_retention_analytics\n",
      "Fetching details for victorcouste/trino-dbt-demo\n",
      "Fetching details for jobeng/dbt_testing\n",
      "Fetching details for Krupal-Bamaniya/Dbt-with-Snowflake-and-power-Bi\n",
      "Fetching details for fivetran-bradmoore/dbt-brad\n",
      "Fetching details for xuanhuyen3011/ecom_analytics_dag\n",
      "Fetching details for dpguthrie/dbt-dynamic-modeling\n",
      "Fetching details for mzdrenka/nauka_dbt\n",
      "Fetching details for ArtemRomanov-FT/dbt-tutorial\n",
      "Fetching details for fivetran/partner_certification_day\n",
      "Fetching details for navikt/dbtinav\n",
      "Fetching details for sidataplus/omop_linkage_dbt\n",
      "Fetching details for bveber/dbt_retail_example\n",
      "Fetching details for Datateer/datateer-dbt\n",
      "Fetching details for matic-insurance/assignment-data-aggregation\n",
      "Fetching details for kvdomingo/analytics-data-engg-play\n",
      "Fetching details for usefusefi/dbt-bigquery\n",
      "Fetching details for meenandm/demo_dbt\n",
      "Fetching details for snowplow-incubator/dbt-template\n",
      "Fetching details for schottj/yukon\n",
      "Fetching details for Aaron-Zhou/dbt-synapse-statistics\n",
      "Fetching details for terateras/dab-dbt-repository\n",
      "Fetching details for riju18/Dashing-Datawarehouse-dbt\n",
      "Fetching details for hypedocs/dbt\n",
      "Fetching details for FlipsideCrypto/terra-models\n",
      "Fetching details for giorgoss198713/brm-marketing\n",
      "Fetching details for TonFLY/NOVADRIVE\n",
      "Fetching details for Lauraalumno23/curso_data_engineering\n",
      "Fetching details for coapacetic/dbt-cloud-northstar\n",
      "Fetching details for marcelopborges/analytics-gcp\n",
      "Fetching details for davemasino/testzone_project\n",
      "Fetching details for Leids-DataLab/orderline-dw-dbt\n",
      "Fetching details for hareeshraina/Hareesh_DBT\n",
      "Fetching details for gpenessot/data_engineering_dbt\n",
      "Fetching details for shesha-phdata/snowflake-reporting-v2\n",
      "Fetching details for Sharat007/ssp_analytics\n",
      "Fetching details for matsonj/mdsinabox\n",
      "Fetching details for drewbanin/dbt-first-time-flow\n",
      "Fetching details for yuanminglee/gen_ai_transform\n",
      "Fetching details for Matatika/dbt-tap-solarvista\n",
      "Fetching details for chinmayeeudupa/dbt_bricks_fundamentals\n",
      "Fetching details for chridubois/advertizing_control_tower\n",
      "Fetching details for jeff-skoldberg-gmds/dbt-demonstration\n",
      "Fetching details for CENTERS-FOR-INTERNATIONAL-PROGRAMS/dmi-etl\n",
      "Fetching details for DegaregeN/Telegram-\n",
      "Fetching details for SaulRamirezCastro/dbt_training\n",
      "Fetching details for marcelo-guimaraes/dbt-fundamentals\n",
      "Fetching details for monimiller/dbt_tutorial\n",
      "Fetching details for Aditya-Tiwari-07/dbt_project\n",
      "Fetching details for rmoff/current-dbt\n",
      "Fetching details for mahevarma/DBT\n",
      "Fetching details for rara-m/fuelWatch\n",
      "Fetching details for szimmer1/dbt-tutorial\n",
      "Fetching details for berni-creze/piloto_dbt\n",
      "Fetching details for jgnog/advworks-dw\n",
      "Fetching details for Pearson-Advance/dbt-aspects-pearson\n",
      "Fetching details for jessiegibson/dbt-transformation\n",
      "Fetching details for tuongviatruong/dbt-tutorial\n",
      "Fetching details for analyticsengineersclub/alewinsky\n",
      "Fetching details for BlockTrekker/decoded-projects-dbt\n",
      "Fetching details for sparsh-ai/recohut\n",
      "Fetching details for Digu01/Data-Engineering-Zoomcamp\n",
      "Fetching details for bwilson668/jaffle_shop\n",
      "Fetching details for DalgoT4D/dbt-superset-usage\n",
      "Fetching details for 21seya/northwind-academy\n",
      "Fetching details for Snowflake-Labs/fhoffa\n",
      "Fetching details for brickmeister/dbt_delta_demo\n",
      "Fetching details for MindaugasN/dbt-ci-cd-data-warehouse\n",
      "Fetching details for KrisMTLA/dbt_fundamentals\n",
      "Fetching details for danilodatabricks/cicd-example\n",
      "Fetching details for anna-geller/dbt-jaffle-shop-template\n",
      "Fetching details for masonmenges/data-platform-tutorial\n",
      "Fetching details for redroy44/prefect-dataplatform\n",
      "Fetching details for fivetran-justinbeausoleil/dbt-learn-justinbeausoleil\n",
      "Fetching details for jwills/nba_monte_carlo\n",
      "Fetching details for rmoff/current-dbt\n",
      "Fetching details for OpenLineage/OpenLineage\n",
      "Fetching details for dagster-io/dagster\n",
      "Fetching details for airbytehq/airbyte-dbt-models\n",
      "Fetching details for the-prairie/source-alberta-re\n",
      "Fetching details for jre247/dbt-forked\n",
      "Fetching details for data-mie/dbt-profiler\n",
      "Fetching details for sparsh-ai/recohut\n",
      "Fetching details for Cannect-Life/dex\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Fetching details for entroopie/dbt_cloud_project\n",
      "Fetching details for fivetran/dbt_hubspot_source\n",
      "Fetching details for Celinamoussaoui/Newwww\n",
      "Fetching details for JarrodWade/fda-adverse-events-pipeline\n",
      "Fetching details for RonikJayakumar/Big-Data---Data-Pipelines-using-Airflow-and-DBT\n",
      "Fetching details for starburstdata/dbt-trino-utils\n",
      "Fetching details for MARDBT/Jaffle_shop_tr\n",
      "Fetching details for SameC137/SensorDataELTPostgress\n",
      "Fetching details for analyticsengineersclub/sample-dbt-project\n",
      "Fetching details for HazemEldabaa/soda-dagster-demo\n",
      "Fetching details for nclaeys/tpcds-dbt-benchmarks\n",
      "Fetching details for cilidon/Sales-Analysis-Using-Modern-Data-pipeline\n",
      "Fetching details for paulf-999/dbt_projects\n",
      "Fetching details for Victoriapm/ny_taxi_rides_zoomcap\n",
      "Fetching details for bmpower/hot-ones-data-engineering-project\n",
      "Fetching details for tnightengale/dbt-activity-schema\n",
      "Fetching details for jpa203/dbt-Snowflake-dw\n",
      "Fetching details for jc1131/redwood-solutions\n",
      "Fetching details for dapomeranz/dbt_example\n",
      "Fetching details for borjavb/dbt-iceberg-poc\n",
      "Fetching details for fernandoabcdataz/wheres-carmen-dbt\n",
      "Fetching details for fivetran/dbt_mixpanel\n",
      "Fetching details for celiacapereira/dbt_dev_pre_snowflake_github\n",
      "Fetching details for lavinomenezes/Desafio_Analytics\n",
      "Fetching details for jmd304-alphiusvictoria/dbt_check\n",
      "Fetching details for ma2ciek/readme-test\n",
      "Fetching details for move-coop/dbt-ngpvan\n",
      "Fetching details for bennieregenold7/dbt-sandbox-snowflake\n",
      "Fetching details for mickaelandrieu/dbt_app\n",
      "Fetching details for benitomartin/de-hotel-reviews\n",
      "Fetching details for kanpinpon/gz_dbt_repository\n",
      "Fetching details for jpayrun/Dimensional-Data-Modeling-Homework\n",
      "Fetching details for Mattadam89/engineering_zoom_camp_homework\n",
      "Fetching details for pluto0007/Microservices\n",
      "Fetching details for nibbleai/dbt-workshop-pernod-ricard\n",
      "Fetching details for AliVaseghnia/spotify-insights\n",
      "Fetching details for Scottyko/dbt-tutorial\n",
      "Fetching details for quannm2k2/mdau_dbt_snowflake\n",
      "Fetching details for sirishkumar/ny_taxi_rides_zoomcamp\n",
      "Fetching details for ekoepplin/dbt-dwh-core\n",
      "Fetching details for caiocvelasco/project04-venv-dbt-from-s3-parquet-to-snowflake-external-tables-scd-type-2\n",
      "Fetching details for tienmanh16/project_data_warehouse\n",
      "Fetching details for amelinvladimir/dbt_course\n",
      "Fetching details for snowplow/dbt-snowplow-web\n",
      "Fetching details for fivetran/dbt_youtube_analytics\n",
      "Fetching details for oleksyii/DBT-Airflow-Snowflake-ETL\n",
      "Fetching details for estuary/estuary_dbt\n",
      "Fetching details for ken6377/dbt_multi_env\n",
      "Fetching details for phamthiminhtu/batch-data-pipeline\n",
      "Fetching details for japerry911/crypto-data-pipeline\n",
      "Fetching details for korbash/dbt_test\n",
      "Fetching details for JJayThanakrit/dw-and-bi\n",
      "Fetching details for airbytehq/airbyte-dbt-models\n",
      "Fetching details for JacobLiLN/Datatalks2024\n",
      "Fetching details for teamdatatonic/dop\n",
      "Fetching details for EdidiongEsu/dataengzoomcamp\n",
      "Fetching details for Csgoodman/dbt-calvin\n",
      "Fetching details for vlasel337/dbt-scooters\n",
      "Fetching details for abdullahsalg/gz-dbt-repository\n",
      "Fetching details for edgarts/data-warehousing\n",
      "Fetching details for amirjonr/data_engeneering_course_summaries\n",
      "Fetching details for sestys/corise_ae_with_dbt\n",
      "Fetching details for carrossoni/dbtvault-databricksDemo\n",
      "Fetching details for moderndatastackpipeline/dbt-model\n",
      "Fetching details for rikkemelander/remote_git_playground\n",
      "Fetching details for BENLTAIEF/dbt\n",
      "Fetching details for luchofit/dbt\n",
      "Fetching details for gabo-hacStyle/fatela-ea\n",
      "Fetching details for timyouell-servian/dbt_demo_updated\n",
      "Fetching details for Penquina/swu-ds525\n",
      "Fetching details for bdatkins/de-2024-macoviddata\n",
      "Fetching details for Adnan-Khanx/Sakila_dbt\n",
      "Fetching details for anwangari/data-modelling-with-dbt\n",
      "Fetching details for rpmcdougall/dbtlearn\n",
      "Fetching details for Ed-Fi-Exchange-OSS/Student-Engagement-Management\n",
      "Fetching details for vergenzt/dbt-personal\n",
      "Fetching details for SabrinaCardoso/dbt_adventureworks\n",
      "Fetching details for MoraChen/dbt-postgresql\n",
      "Fetching details for tfayyaz/marketing_analytics_demo\n",
      "Fetching details for VaLeRiEe37/dbt-lightdash\n",
      "Fetching details for punit-ee/de-journey\n",
      "Fetching details for ArielS-Simplex/dbt_ods\n",
      "Fetching details for huynguyen0257/demo-dbt\n",
      "Fetching details for leogodin217/dbt_demos\n",
      "Fetching details for leogodin217/dbt_demos\n",
      "Fetching details for trevenm/dbt_jinja_test\n",
      "Fetching details for christianboylston/hackathon2023\n",
      "Fetching details for chaimaaAZ/Airbnb_etl_and_analytics_project\n",
      "Fetching details for GuuhRodrigues/northwind-academy\n",
      "Fetching details for OHDSI/Broadsea\n",
      "Fetching details for HemalathaRamanujam2022/DE_ZoomCamp\n",
      "Fetching details for arosenk35/dbt_old_20211101\n",
      "Fetching details for paradime-io/paradime-dagster-dbt\n",
      "Fetching details for mraabhijit/DEZoomcamp2024\n",
      "Fetching details for hjamkhande/Learn_DBT\n",
      "Fetching details for Teradata/airbyte-dbt-jaffle\n",
      "Fetching details for outerbounds/tutorials\n",
      "Fetching details for fivetran/dbt_shopify\n",
      "Fetching details for sungchun12/datafold_demo\n",
      "Fetching details for el-grudge/data-engineering-zoomcamp\n",
      "Fetching details for datakind/Data-Observation-Toolkit\n",
      "Fetching details for sunlao/hello_dbt\n",
      "Fetching details for jugalkishorebhatt/dbt\n",
      "Fetching details for uforodavid/dm_project\n",
      "Fetching details for lightdash/dbt2looker\n",
      "Fetching details for ConstantinoSchillebeeckx/dbt_databricks_demo\n",
      "Fetching details for zyadbenameur/modern-data-pipelines-workshop\n",
      "Fetching details for phdmohamedali/dbt-start\n",
      "Fetching details for Sciance-Inc/core.dashboards_store\n",
      "Fetching details for domenicoboris89/dbt_snowflake\n",
      "Fetching details for Rutivk/Analytics\n",
      "Fetching details for sskashya/ist722dbt\n",
      "Fetching details for airbytehq/open-data-stack\n",
      "Fetching details for cicerojmm/transformacaoDadosDBTAthena\n",
      "Fetching details for gomes0499/Netflix-GCP-Data-Engineering\n",
      "Fetching details for Generation-Data/dbt-tutorials\n",
      "Fetching details for Lidiate5/Curso-Civica\n",
      "Fetching details for fivetran-scottneiger/dbt-tutorial\n",
      "Fetching details for buithehaiuts/bde_assignment\n",
      "Fetching details for durrani271996/saad_repo1\n",
      "Fetching details for ehoanshelt/dbt-test\n",
      "Fetching details for videetm/Zoomcamp_DBT\n",
      "Fetching details for Kevin-Nduati/My-Spotify-Wrapped\n",
      "Fetching details for joweyel/data-engineering-material\n",
      "Fetching details for MeltanoLabs/dbt-kustomer\n",
      "Fetching details for AntonisCSt/DTC_data_engineering_project\n",
      "Fetching details for GOK-656/dezoomcamp_dbt\n",
      "Fetching details for ryandataengineergit/dbt_cloud_demo\n",
      "Fetching details for aec-self-study/ae-club-jeroen\n",
      "Fetching details for srpantano/dbt-bigquery\n",
      "Fetching details for phiisonfire/FSA_Data_Engineering\n",
      "Fetching details for DalgoT4D/dbt_arogya_world\n",
      "Fetching details for dbt-labs/dbt-labs-experimental-features\n",
      "Fetching details for OscartGiles/AirBend\n",
      "Fetching details for v11k/smart24-dbt\n",
      "Fetching details for matyohan/dbtlearn_ondemand\n",
      "Fetching details for dlt-hub/dlt-dbt-stripe\n",
      "Fetching details for community-tech-alliance/dbt-cta\n",
      "Fetching details for itsadityagupta/dezoomcamp2023-dbt\n",
      "Fetching details for cs378-fall2024/snippets\n",
      "Fetching details for ferrap/dbt_transformations_gbq\n",
      "Fetching details for tanhoang1808/AnalyticEngineer\n",
      "Fetching details for LottieJaneDev/usgs_earthquake_data_pipeline\n",
      "Fetching details for AzizAghabayli/DEZoomcamp\n",
      "Fetching details for meenandm/sakila_database\n",
      "Fetching details for Gravity-Data/gravity-dbt\n",
      "Fetching details for rwer81/dbt-tutorial\n",
      "Fetching details for sudoku-committer/iot-elt-airflow-mongo-timescaledb\n",
      "Fetching details for durvalcarvalho/small-dbt-project-for-testing\n",
      "Fetching details for Tonayya/refactoring-sql-for-modularity\n",
      "Fetching details for DestschEng/Morden_DE_Stack\n",
      "Fetching details for myfsuhail/dbt_snowflake_analytics\n",
      "Fetching details for zainulabidin302/dbt-tutorial\n",
      "Fetching details for mahesh-c-pathak/zoomcamp2024\n",
      "Fetching details for marcosfpinheiro/dbt\n",
      "Fetching details for sathyasaikoukantla/CH_test\n",
      "Fetching details for jeniffersanches/adventure-works-fea\n",
      "Fetching details for mahmoud-farah/Airbnb\n",
      "Fetching details for Saikumar8520/saikumar8520\n",
      "Fetching details for eddaouissam/demo_initial\n",
      "Fetching details for johnowusuduah/dbt-demo\n",
      "Fetching details for LinkedInLearning/complete-guide-to-data-lakes-and-lakehouses-3865060\n",
      "Fetching details for Bl3f/dbt-loom-example\n",
      "Fetching details for joshuasprow/dbt-demo\n",
      "Fetching details for martino-lucas95/Practico3.1_BigData\n",
      "Fetching details for Balurc/data_eng_zoomcamp\n",
      "Fetching details for Kalze1/Building_Data_Warehouse_to_store_data_Ethiopian_medical_business\n",
      "Fetching details for JesSchattschneider/project-3-de\n",
      "Fetching details for VictorHugoAmorim/ada_santander\n",
      "Fetching details for DJLemkes/dbt-metric-utils\n",
      "Fetching details for djsweeper/dw-and-bi\n",
      "Fetching details for Dpananos/BARF\n",
      "Fetching details for theodora-tech/dbt-scd2\n",
      "Fetching details for bufferapp/churnado\n",
      "Fetching details for ilya40umov/dbt-athena-sandbox\n",
      "Fetching details for dickyardt/Iykra-Practice-Case\n",
      "Fetching details for boutzoua/Building-a-Lakehouse-Distribution-with-Integration-of-Open-Source-Tools\n",
      "Fetching details for kristin-bagnall/dbt-learn-kbagnall\n",
      "Fetching details for dpguthrie/multi-cluster-multi-tenant-core-project\n",
      "Fetching details for welitonborges/dbt_ecommerce\n",
      "Fetching details for sredfern/dbt-language-server\n",
      "Fetching details for dbt-labs/dbt-learn-jinja\n",
      "Fetching details for graceyudhaaa/osm-data-analysis\n",
      "Fetching details for fkfouri/study_dbt\n",
      "Fetching details for TylerHillery/Stock-Market-Data\n",
      "Fetching details for danlsn/dbt-cicd\n",
      "Fetching details for nagampere/horkew\n",
      "Fetching details for shivangsingh26/automatedELTpipeline\n",
      "Fetching details for tuantran0910/dbt_learn\n",
      "Fetching details for aec-self-study/ae-club-koen\n",
      "Fetching details for ourjune21/stock-watcher\n",
      "Fetching details for OpenLineage/OpenLineage\n",
      "Fetching details for Androidown/dagster\n",
      "Fetching details for fivetran/dbt_facebook_pages\n",
      "Fetching details for wjhrdy/dbt-trino-utils-fix\n",
      "Fetching details for SameC137/SensorDataELT\n",
      "Fetching details for darshan601/DBT-snowflake\n",
      "Fetching details for AdamChan-ML/dbt-snowflake\n",
      "Fetching details for gujarathijay/dbt\n",
      "Fetching details for ma2ciek/empty-readme-2\n",
      "Fetching details for ma2ciek/readme-test-2\n",
      "Fetching details for z333d/y42-demo\n",
      "Fetching details for caiocvelasco/project02-docker-dbt-DuckDB-medallion-kimball-postgres\n",
      "Fetching details for caiocvelasco/project03-docker-dbt-from-s3-parquet-to-Snowflake-external-tables\n",
      "Fetching details for SupansaMSDS/dw-and-bi\n",
      "Fetching details for kkeeratis/DW-and-BI\n",
      "Fetching details for Nongnoot456/dw-and-bi\n",
      "Fetching details for tonthanakorn/dw-and-bi\n",
      "Fetching details for bytecodeio/google_data_services_template\n",
      "Fetching details for giu-ferreira-cientista/dbt-test\n",
      "Fetching details for jonasmulticloudiac/dbtdatamodernstack\n",
      "Fetching details for joaopedrobrb/dbt-model\n",
      "Fetching details for MackRhine/dbt_demo1\n",
      "Fetching details for lillythamolwan/swu-ds525\n",
      "Fetching details for kzajaczkowski/dbt-learn-on-demand2\n",
      "Fetching details for a-bro-0227/dbt-labs-refactoring-sql\n",
      "Fetching details for codep-ai/dbt-demo\n",
      "Fetching details for AndMoleiro/learning-repo\n",
      "Fetching details for ArielS-Simplex/dbt_ods\n",
      "Fetching details for OHDSI/Broadsea-Dbt\n",
      "Fetching details for bcbi/atlas-broadsea-v3\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Education-Model\n",
      "Fetching details for BrenoFariasdaSilva/Dagster-Template\n",
      "Fetching details for akashhsapkal/ist722dbt\n",
      "Fetching details for jmjustis/ist722dbt\n",
      "Fetching details for pranaliredij/ist722dbt\n",
      "Fetching details for rdubwiley09/mi-warehouse\n",
      "Fetching details for kmiller96/menagerie\n",
      "Fetching details for martino-lucas95/Practico3.1_BigData\n",
      "Fetching details for domingosjuan/dbt-aula-05-santander-coders-ada\n",
      "Fetching details for mage-ai/mage-ai\n",
      "Fetching details for dagster-io/dagster\n",
      "Fetching details for d-tree-org/Airbyte\n",
      "Fetching details for fabriq-labs/fabriq-platform\n",
      "Fetching details for calum-mcg/dbt-fuzzy-text\n",
      "Fetching details for OpenLineage/OpenLineage\n",
      "Fetching details for paradime-io/paradime-dagster-dbt\n",
      "Fetching details for dbt-labs/spark-utils\n",
      "Fetching details for infinitelambda/dbt-central-app\n",
      "Fetching details for bismofunyuns99/dbt-cloud-snowflake-demo-template\n",
      "Fetching details for Ghifariezra/digitalskola-dataeng\n",
      "Fetching details for FlipsideCrypto/blast-models\n",
      "Fetching details for rubenmunozpardo/Rubenmunozpardo\n",
      "Fetching details for cjho0316/dbt-pipeline\n",
      "Fetching details for LewisDavies/upstream-prod\n",
      "Fetching details for owsmatheus/sp_engenharia_dados_dbt\n",
      "Fetching details for dlt-hub/dlt\n",
      "Fetching details for ahfay/Hands-On-dbt\n",
      "Fetching details for lbk-fishtown/dbt-aws-demo\n",
      "Fetching details for joe-bryan/dbt-business-intelligence\n",
      "Fetching details for dbt-labs/dbt-meshify\n",
      "Fetching details for itcouldwork/dbt_flights_data\n",
      "Fetching details for snowplow-incubator/analytics-engineer-tech-test\n",
      "Fetching details for emmy-1/subscriber_cancellations\n",
      "Fetching details for kforeman/covid19-scraper\n",
      "Fetching details for DataSentics/sap-order-to-cash\n",
      "Fetching details for xiangivyli/data_engineering_zoomcamp_learning\n",
      "Fetching details for laurenceandrews/carbon-project-rater\n",
      "Fetching details for spiced-academy/gcp-dbt-test\n",
      "Fetching details for digitalcityscience/agora-dbt\n",
      "Fetching details for tiagog-dev/data-engineer-zoomcamp-project\n",
      "Fetching details for Murataydinunimi/AIRFLOW_DBT_SNOWFLAKE_DOCKER\n",
      "Fetching details for billybillysss/spark-lakehouse\n",
      "Fetching details for ndrewwm/spotify-tracks\n",
      "Fetching details for lightdash/databricks-demo\n",
      "Fetching details for xaviercollribas/data_management_backbone_template\n",
      "Fetching details for dmateusp/dbt-steampipe\n",
      "Fetching details for transform-data/transform-dbt-sync\n",
      "Fetching details for ZhiwenSong1/LINTING-WITH-GITHUB-ACTIONS1\n",
      "Fetching details for migasar/snow-pond\n",
      "Fetching details for archie-cm/final-project-credit-card-fraud-pipeline\n",
      "Fetching details for mazayayumna/DE-Zoomcamp\n",
      "Fetching details for adityapawarx/ist722dbt\n",
      "Fetching details for amoghhasotkar/amoghhasotkar\n",
      "Fetching details for Gengsu07/DEGengsuProject\n",
      "Fetching details for mberneaud/us-weather-alerts\n",
      "Fetching details for ptrwn/dezoomcamp\n",
      "Fetching details for KatrinL13/gz-dbt-repository\n",
      "Fetching details for BorisQuanLi/northwind-olap-dbt-bigquery\n",
      "Fetching details for anuragmgithub/POC\n",
      "Fetching details for rockerben/dec_capstone\n",
      "Fetching details for mrgimenezcosta/tptd7_parte2\n",
      "Fetching details for bruno-ribeirodasilva/sumup-ae-tt\n",
      "Fetching details for fabhlc/dbt\n",
      "Fetching details for michiel-de-muynck/minerva_training\n",
      "Fetching details for ChungWasawat/dtc_de_project\n",
      "Fetching details for foghlaimeoir/data-engineering\n",
      "Fetching details for votr-me/votr_dbt\n",
      "Fetching details for phil-a10/dbt_databricks_demo\n",
      "Fetching details for salbifaza/digital-skola-project-4-dbt-bigquery\n",
      "Fetching details for thepaulalin/dbt-learn\n",
      "Fetching details for zsombor-flds/dsmr-data-flow\n",
      "Fetching details for lawansubba/dbt_vsfg_gdri\n",
      "Fetching details for etaboada-sanford/sandbt-hris\n",
      "Fetching details for jaswanth333/azure-dbt-spark-modeling\n",
      "Fetching details for EJOOSTEROP/mimodast\n",
      "Fetching details for igormartins0301/olist_dbt\n",
      "Fetching details for aboyalejandro/data-projects\n",
      "Fetching details for conradbez/dbt-factory\n",
      "Fetching details for ritabratasaha/dbt-test-framework\n",
      "Fetching details for ken-10/poketcgdata-workflows\n",
      "Fetching details for ashmithalaxmi/ProjectAA\n",
      "Fetching details for GalaxyStar678/dbt\n",
      "Fetching details for DanCorley/orange_theory_data_stack\n",
      "Fetching details for Opetushallitus/ovara\n",
      "Fetching details for geekquest/gq_chat_analytics\n",
      "Fetching details for fivetran-zehaoli/dbt-demo\n",
      "Fetching details for edgarrmondragon/meltano-dogfood\n",
      "Fetching details for PranauvShanmuganathan/Swiggy-Snowflake-DBT\n",
      "Fetching details for lucasudar/data-engineering-project\n",
      "Fetching details for mrichardson-ch/dbt-example\n",
      "Fetching details for IntegrityEnergy/integrity-energy-dbt\n",
      "Fetching details for LucasO21/dbt_jaffle_shop\n",
      "Fetching details for alumno19civ/curso_data_engineering\n",
      "Fetching details for FAD95/dbt_fundamentals\n",
      "Fetching details for amschwinn/dbt-fundamentals\n",
      "Fetching details for FlipsideCrypto/arbitrum-models\n",
      "Fetching details for AntonioTepsich/recommender-system-mlops\n",
      "Fetching details for pgoslatara/medium_scraper\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Fetching details for ujjawal-mandhani/dbt-containerized-projects\n",
      "Fetching details for coding-is-for-losers/ga-conversion-index\n",
      "Fetching details for DenisMurynka/data-engineering\n",
      "Fetching details for Nintorac/s4_dx7\n",
      "Fetching details for nagampere/horkew\n",
      "Fetching details for kzzzr/airbyte_lab\n",
      "Fetching details for dell-datascience/Data_Engineering\n",
      "Fetching details for durvalcarvalho/dbt-project-example\n",
      "Fetching details for dbt-labs/bigquery_dbt_partner_demo\n",
      "Fetching details for Levers-Labs/SOMA-B2B-SaaS\n",
      "Fetching details for dreams-labs/etl-pipelines\n",
      "Fetching details for Nuvalence/hackdata-ecommerce-example\n",
      "Fetching details for brocolidata/dbt_packages\n",
      "Fetching details for sredfern/dbt-language-server\n",
      "Fetching details for Rihab114/Velib-Steaming\n",
      "Fetching details for mwojtyczka/dbt-demo\n",
      "Fetching details for G-Chandramani/aws_code_deploy_test\n",
      "Fetching details for OpenLineage/metrics\n",
      "Fetching details for ChrisKornaros/NFL_Big_Data_Bowl_2025\n",
      "Fetching details for Hlsorrells/dbt-learn-on-demand\n",
      "Fetching details for Sumeagar1222/Dbt-repository\n",
      "Fetching details for boxysean/dbt-overdue\n",
      "Fetching details for geoHeil/dagster-asset-demo\n",
      "Fetching details for daguito81/dbt-kube-projectA\n",
      "Fetching details for jessrobbin/dbt\n",
      "Fetching details for Us3rname/webshop-scraper\n",
      "Fetching details for dbt-msft/ado_pipelines_example\n",
      "Fetching details for nicolascorchuelo/portfolio\n",
      "Fetching details for colin-thornburg/Insurance-Analytics\n",
      "Fetching details for Shindora/data-stack-dagster-dbt-superset\n",
      "Fetching details for devarber-beep/curso_data_engeneering\n",
      "Fetching details for bramreinders97/view_selection_experiments\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Fetching details for dbt-labs/python-snowpark-formula1\n",
      "Fetching details for jaanli/american-community-survey\n",
      "Fetching details for snowplow/dbt-snowplow-attribution\n",
      "Fetching details for zzstoatzz/blogs\n",
      "Fetching details for ryu1kn/sandbox--data-modelling\n",
      "Fetching details for Amature123/Datathon2023\n",
      "Fetching details for ldnicolasmay/dbt-fundamentals\n",
      "Fetching details for Mormur22/pruebas_intrgracion\n",
      "Fetching details for Amine-RT/DEZ-2024\n",
      "Fetching details for seacevedo/Solana-Pipeline\n",
      "Fetching details for mehd-io/pypi-duck-flow\n",
      "Fetching details for nicolasjonck/restaurant-analytics\n",
      "Fetching details for Avi-2509/profileDbt_Airbyte\n",
      "Fetching details for zecakpm/dbtlearn\n",
      "Fetching details for emtinanseo/Traffic-Data-Warehousing\n",
      "Fetching details for MaxHilsdorf/billboard-analytics-pipeline\n",
      "Fetching details for dbt-labs/dbt-Explorer-on-demand-course-platform\n",
      "Fetching details for 211-Connect/ndp_uwsc_dbt_wrapper\n",
      "Fetching details for chuannarongvat/at3_dbt\n",
      "Fetching details for Azmi-21/342-Project\n",
      "Fetching details for alexott/databricks-dbt-playground\n",
      "Fetching details for attaponsayod/Team_vard\n",
      "Fetching details for CXTV/git_co_develop_demo\n",
      "Fetching details for mfarik21/Global-Suicide-Trends-Data-Project\n",
      "Fetching details for strafe27/malaysia-film-finas-dag\n",
      "Fetching details for jairus-m/dbt-strava\n",
      "Fetching details for fortoajit/airbyte\n",
      "Fetching details for codedeamon/growthleads_assignment\n",
      "Fetching details for rydercodes/dbt-core\n",
      "Fetching details for samuelzhang1201/customer_case\n",
      "Fetching details for louielimranola/coinbase_project\n",
      "Fetching details for lbk-fishtown/db-ft-dbt-bc\n",
      "Fetching details for ferrap/dbt_transformations_gbq\n",
      "Fetching details for anuragmgithub/POC\n",
      "Fetching details for wdxtub/mds-note\n",
      "Fetching details for pedrambadiee/demo_dbt\n",
      "Fetching details for sanyassyed/DataEngineering_SFO_Eviction_Data_ETL_Pipeline\n",
      "Fetching details for world50/dbt-learn-testing-v1\n",
      "Fetching details for penquin501/capstone_data_pipeline_reddit\n",
      "Fetching details for ethereum-optimism/op-analytics\n",
      "Fetching details for Light2Dark/quality-of-life\n",
      "Fetching details for begussimo/my_dbt\n",
      "Fetching details for skghosh95/fivetrandbt\n",
      "Fetching details for JuanDiego18/DBT_basic_project\n",
      "Fetching details for justinkolpak-databricks/dbt_tpch\n",
      "Fetching details for Mohanaaravind/dbt_hands_on_1\n",
      "Fetching details for quannm2k2/dbt_snowflake_supermarket\n",
      "Fetching details for muditsinghal1/dbt_manual_repo\n",
      "Fetching details for ysjjovo/dbtcicd\n",
      "Fetching details for tranthe170/dbt-demo\n",
      "Fetching details for Midory3103/gz-dbt-repository\n",
      "Fetching details for JROdeyemi/MSSQL-Snowflake-ELT\n",
      "Failed to fetch repo details for JROdeyemi/MSSQL-Snowflake-ELT\n",
      "Fetching details for lucapug/nyc-bike-analytics\n",
      "Failed to fetch repo details for lucapug/nyc-bike-analytics\n",
      "Fetching details for ChiQuang98/dbt-snowflake-poc\n",
      "Failed to fetch repo details for ChiQuang98/dbt-snowflake-poc\n",
      "Fetching details for idenrai/GroceryMate\n",
      "Failed to fetch repo details for idenrai/GroceryMate\n",
      "Fetching details for mharty3/energy_data_capstone\n",
      "Failed to fetch repo details for mharty3/energy_data_capstone\n",
      "Fetching details for rwscheng/career-skills\n",
      "Failed to fetch repo details for rwscheng/career-skills\n",
      "Fetching details for mycielski/zsbd\n",
      "Failed to fetch repo details for mycielski/zsbd\n",
      "Fetching details for moeabbas6/dbt_analytics_engine\n",
      "Failed to fetch repo details for moeabbas6/dbt_analytics_engine\n",
      "Fetching details for victor9212/dbt-fundamentals\n",
      "Failed to fetch repo details for victor9212/dbt-fundamentals\n",
      "Fetching details for fagrahmed/financial\n",
      "Failed to fetch repo details for fagrahmed/financial\n",
      "Fetching details for koy0208/dbt-crime-visualizer\n",
      "Failed to fetch repo details for koy0208/dbt-crime-visualizer\n",
      "Fetching details for Idriss-Abidi/data_pipeline\n",
      "Failed to fetch repo details for Idriss-Abidi/data_pipeline\n",
      "Fetching details for cgastellor/dbt\n",
      "Failed to fetch repo details for cgastellor/dbt\n",
      "Fetching details for gabriellealves2015/lh_aw_grabrielle_alves\n",
      "Failed to fetch repo details for gabriellealves2015/lh_aw_grabrielle_alves\n",
      "Fetching details for luciams11/curso_data_engineering\n",
      "Failed to fetch repo details for luciams11/curso_data_engineering\n",
      "Fetching details for Merihun/mydbtlearn\n",
      "Failed to fetch repo details for Merihun/mydbtlearn\n",
      "Fetching details for adrianburusdbt/dbtcloudtestrepo\n",
      "Failed to fetch repo details for adrianburusdbt/dbtcloudtestrepo\n",
      "Fetching details for OpenSourcePolitics/dbt-processing\n",
      "Failed to fetch repo details for OpenSourcePolitics/dbt-processing\n",
      "Fetching details for z3z1ma/dbt-core-interface\n",
      "Failed to fetch repo details for z3z1ma/dbt-core-interface\n",
      "Fetching details for OpenSourcePolitics/dbt-processing\n",
      "Failed to fetch repo details for OpenSourcePolitics/dbt-processing\n",
      "Fetching details for ZhDmitriy/dbt_snowflake\n",
      "Failed to fetch repo details for ZhDmitriy/dbt_snowflake\n",
      "Fetching details for OHDSI/dbt-synthea\n",
      "Failed to fetch repo details for OHDSI/dbt-synthea\n",
      "Fetching details for gabriellopes013/dbt\n",
      "Failed to fetch repo details for gabriellopes013/dbt\n",
      "Fetching details for kiliantscherny/Fun-With-LEGO-data\n",
      "Failed to fetch repo details for kiliantscherny/Fun-With-LEGO-data\n",
      "Fetching details for yasinsharaf/Bike_Sales_Analytics-Using_Enterprise_Data_Models\n",
      "Failed to fetch repo details for yasinsharaf/Bike_Sales_Analytics-Using_Enterprise_Data_Models\n",
      "Fetching details for mouldyjon/ra_data_warehouse\n",
      "Failed to fetch repo details for mouldyjon/ra_data_warehouse\n",
      "Fetching details for digitranslab/bigbytes\n",
      "Failed to fetch repo details for digitranslab/bigbytes\n",
      "Fetching details for kostasgaridis/dbt-playground\n",
      "Failed to fetch repo details for kostasgaridis/dbt-playground\n",
      "Fetching details for gouline/duck-jaffle-ai\n",
      "Failed to fetch repo details for gouline/duck-jaffle-ai\n",
      "Fetching details for Aiden0/local-data-eng-workshop\n",
      "Failed to fetch repo details for Aiden0/local-data-eng-workshop\n",
      "Fetching details for ShubhamGupta505/dbs_airbyte_s3\n",
      "Failed to fetch repo details for ShubhamGupta505/dbs_airbyte_s3\n",
      "Fetching details for ShubhamGupta505/Airbyte_CDC\n",
      "Failed to fetch repo details for ShubhamGupta505/Airbyte_CDC\n",
      "Fetching details for cmccar01/airbyte\n",
      "Failed to fetch repo details for cmccar01/airbyte\n",
      "Fetching details for BrahianVT/airByte\n",
      "Failed to fetch repo details for BrahianVT/airByte\n",
      "Fetching details for dagster-io/dagster\n",
      "Failed to fetch repo details for dagster-io/dagster\n",
      "Fetching details for dagster-io/airlift-tutorial\n",
      "Failed to fetch repo details for dagster-io/airlift-tutorial\n",
      "Fetching details for gibrantwp20/Project3\n",
      "Failed to fetch repo details for gibrantwp20/Project3\n",
      "Fetching details for Ghifariezra/digitalskola-dataeng\n",
      "Failed to fetch repo details for Ghifariezra/digitalskola-dataeng\n",
      "Fetching details for mmotl/dbt-repo\n",
      "Failed to fetch repo details for mmotl/dbt-repo\n",
      "Fetching details for ZhiwenSong1/LINTING-WITH-GITHUB-ACTIONS\n",
      "Failed to fetch repo details for ZhiwenSong1/LINTING-WITH-GITHUB-ACTIONS\n",
      "Fetching details for adityapawarx/ist722dbt\n",
      "Failed to fetch repo details for adityapawarx/ist722dbt\n",
      "Fetching details for lucasfpineram/data_flow_architecture\n",
      "Failed to fetch repo details for lucasfpineram/data_flow_architecture\n",
      "Fetching details for fede-pereira/CineMark-DB\n",
      "Failed to fetch repo details for fede-pereira/CineMark-DB\n",
      "Fetching details for ujjawal-mandhani/airflow-dbt-spark-repo\n",
      "Failed to fetch repo details for ujjawal-mandhani/airflow-dbt-spark-repo\n",
      "Fetching details for elenasamsonenko/data_warehouse_analyst\n",
      "Failed to fetch repo details for elenasamsonenko/data_warehouse_analyst\n",
      "Fetching details for hari8924/dbt-code\n",
      "Failed to fetch repo details for hari8924/dbt-code\n",
      "Fetching details for G-Chandramani/dbt-code\n",
      "Failed to fetch repo details for G-Chandramani/dbt-code\n",
      "Fetching details for Ridwanabdusalam/dbt_data_pipeline\n",
      "Failed to fetch repo details for Ridwanabdusalam/dbt_data_pipeline\n",
      "Fetching details for petehunt/dagster-gitpod\n",
      "Failed to fetch repo details for petehunt/dagster-gitpod\n",
      "Fetching details for fivetran/dbt_asana_source\n",
      "Failed to fetch repo details for fivetran/dbt_asana_source\n",
      "Fetching details for dbt-labs/dbt-python-hands-on-lab-snowpark\n",
      "Failed to fetch repo details for dbt-labs/dbt-python-hands-on-lab-snowpark\n",
      "Fetching details for Mormur22/pruebas_intrgracion_dbt-test\n",
      "Failed to fetch repo details for Mormur22/pruebas_intrgracion_dbt-test\n",
      "Fetching details for jb-cardiel/dbt-Explorer-tutorial\n",
      "Failed to fetch repo details for jb-cardiel/dbt-Explorer-tutorial\n",
      "Fetching details for PAC-People/dbt-Explorer-on-demand-course-platform\n",
      "Failed to fetch repo details for PAC-People/dbt-Explorer-on-demand-course-platform\n",
      "Fetching details for abhiiiiiiijit/dbt-Explorer-on-demand-course-platform\n",
      "Failed to fetch repo details for abhiiiiiiijit/dbt-Explorer-on-demand-course-platform\n",
      "Fetching details for marklindsey11/AirByte\n",
      "Failed to fetch repo details for marklindsey11/AirByte\n",
      "Fetching details for jaynyoni/localAirbyte\n",
      "Failed to fetch repo details for jaynyoni/localAirbyte\n",
      "Fetching details for ehmadzubair/airbyte-github-connection-task\n",
      "Failed to fetch repo details for ehmadzubair/airbyte-github-connection-task\n",
      "Fetching details for techindicium/airbyte-indicium-training\n",
      "Failed to fetch repo details for techindicium/airbyte-indicium-training\n",
      "Fetching details for Teradata/dbt-teradata\n",
      "Failed to fetch repo details for Teradata/dbt-teradata\n",
      "Fetching details for Gemma-Analytics/ewah\n",
      "Failed to fetch repo details for Gemma-Analytics/ewah\n",
      "Fetching details for the-prairie/source-alberta-re\n",
      "Failed to fetch repo details for the-prairie/source-alberta-re\n",
      "Fetching details for Jumbleberry/snowplow\n",
      "Failed to fetch repo details for Jumbleberry/snowplow\n",
      "Fetching details for paulf-999/airflow_templates\n",
      "Failed to fetch repo details for paulf-999/airflow_templates\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Failed to fetch repo details for xlang-ai/Spider2\n",
      "Fetching details for dbt-labs/dbt_metrics\n",
      "Failed to fetch repo details for dbt-labs/dbt_metrics\n",
      "Fetching details for omnata-labs/dbt-ml-preprocessing\n",
      "Failed to fetch repo details for omnata-labs/dbt-ml-preprocessing\n",
      "Fetching details for drashtii27/Data-pipeline-project\n",
      "Failed to fetch repo details for drashtii27/Data-pipeline-project\n",
      "Fetching details for TEAMSchools/teamster\n",
      "Failed to fetch repo details for TEAMSchools/teamster\n",
      "Fetching details for EqualExperts/dbt-unit-testing\n",
      "Failed to fetch repo details for EqualExperts/dbt-unit-testing\n",
      "Fetching details for em-jones/custom-move-transformations\n",
      "Failed to fetch repo details for em-jones/custom-move-transformations\n",
      "Fetching details for sudo-donya/versioned_scd\n",
      "Failed to fetch repo details for sudo-donya/versioned_scd\n",
      "Fetching details for fivetran/dbt_greenhouse_source\n",
      "Failed to fetch repo details for fivetran/dbt_greenhouse_source\n",
      "Fetching details for dagster-io/dagster\n",
      "Failed to fetch repo details for dagster-io/dagster\n",
      "Fetching details for dbt-msft/tsql-utils\n",
      "Failed to fetch repo details for dbt-msft/tsql-utils\n",
      "Fetching details for ex-mahiro-kajiyama/test\n",
      "Failed to fetch repo details for ex-mahiro-kajiyama/test\n",
      "Fetching details for vsmoleevskii/dbt-scooters\n",
      "Failed to fetch repo details for vsmoleevskii/dbt-scooters\n",
      "Fetching details for Karthikadevi20171/dbt-project\n",
      "Failed to fetch repo details for Karthikadevi20171/dbt-project\n",
      "Fetching details for benbel376/end-to-end-ad-unit-data-MLOps-project\n",
      "Failed to fetch repo details for benbel376/end-to-end-ad-unit-data-MLOps-project\n",
      "Fetching details for HermanMeier/data-eng-project\n",
      "Failed to fetch repo details for HermanMeier/data-eng-project\n",
      "Fetching details for fivetran/dbt_ad_reporting\n",
      "Failed to fetch repo details for fivetran/dbt_ad_reporting\n",
      "Fetching details for DominikGolebiewski/dbt-macro-polo\n",
      "Failed to fetch repo details for DominikGolebiewski/dbt-macro-polo\n",
      "Fetching details for dinagalevska/cryptocurrency-insights\n",
      "Failed to fetch repo details for dinagalevska/cryptocurrency-insights\n",
      "Fetching details for kreuzwerker/kreuzlaker\n",
      "Failed to fetch repo details for kreuzwerker/kreuzlaker\n",
      "Fetching details for brocolidata/dataplatform_lab\n",
      "Failed to fetch repo details for brocolidata/dataplatform_lab\n",
      "Fetching details for mikegfuller/sa-demo\n",
      "Failed to fetch repo details for mikegfuller/sa-demo\n",
      "Fetching details for viniciuspc/data-engieneering-zoomcamp\n",
      "Failed to fetch repo details for viniciuspc/data-engieneering-zoomcamp\n",
      "Fetching details for rberenguel/dbt-sqlite-playground\n",
      "Failed to fetch repo details for rberenguel/dbt-sqlite-playground\n",
      "Fetching details for Das42/netsuite_dbt\n",
      "Failed to fetch repo details for Das42/netsuite_dbt\n",
      "Fetching details for pedro-brigido/fea_academy\n",
      "Failed to fetch repo details for pedro-brigido/fea_academy\n",
      "Fetching details for jimmysinnan/my_dbt_projet\n",
      "Failed to fetch repo details for jimmysinnan/my_dbt_projet\n",
      "Fetching details for danielmschaves/dbt-duckdb-air-quality\n",
      "Failed to fetch repo details for danielmschaves/dbt-duckdb-air-quality\n",
      "Fetching details for Datavault-UK/automate-dv\n",
      "Failed to fetch repo details for Datavault-UK/automate-dv\n",
      "Fetching details for truongvude/dezoomcamp\n",
      "Failed to fetch repo details for truongvude/dezoomcamp\n",
      "Fetching details for netbek/dw-analytics\n",
      "Failed to fetch repo details for netbek/dw-analytics\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Failed to fetch repo details for xlang-ai/Spider2\n",
      "Fetching details for GoogleCloudPlatform/terraform-google-dbt-composer-blueprint\n",
      "Failed to fetch repo details for GoogleCloudPlatform/terraform-google-dbt-composer-blueprint\n",
      "Fetching details for checkers-dev/checkers\n",
      "Failed to fetch repo details for checkers-dev/checkers\n",
      "Fetching details for mlincon/coursera-courses\n",
      "Failed to fetch repo details for mlincon/coursera-courses\n",
      "Fetching details for shahidmalik4/dbt-snowflake-data-pipeline\n",
      "Failed to fetch repo details for shahidmalik4/dbt-snowflake-data-pipeline\n",
      "Fetching details for AG-Wagon/gz-dbt-repository\n",
      "Failed to fetch repo details for AG-Wagon/gz-dbt-repository\n",
      "Fetching details for JohanTokarskij/warehouse_lifecycle_project_by_projektledare\n",
      "Failed to fetch repo details for JohanTokarskij/warehouse_lifecycle_project_by_projektledare\n",
      "Fetching details for Bl3f/dbt-loom-example\n",
      "Failed to fetch repo details for Bl3f/dbt-loom-example\n",
      "Fetching details for yu-iskw/dbt-airflow-macros\n",
      "Failed to fetch repo details for yu-iskw/dbt-airflow-macros\n",
      "Fetching details for mcannamela/dbt-doctools\n",
      "Failed to fetch repo details for mcannamela/dbt-doctools\n",
      "Fetching details for Dilip1100/DBTWORKSHOP101\n",
      "Failed to fetch repo details for Dilip1100/DBTWORKSHOP101\n",
      "Fetching details for techindicium/dbt-bigquery-analytics\n",
      "Failed to fetch repo details for techindicium/dbt-bigquery-analytics\n",
      "Fetching details for aec-self-study/ae-club-vivien\n",
      "Failed to fetch repo details for aec-self-study/ae-club-vivien\n",
      "Fetching details for Jezandre/eng_dados_cnes\n",
      "Failed to fetch repo details for Jezandre/eng_dados_cnes\n",
      "Fetching details for cciliayang/dbt-project\n",
      "Failed to fetch repo details for cciliayang/dbt-project\n",
      "Fetching details for dbt-labs/dbt-project-evaluator\n",
      "Failed to fetch repo details for dbt-labs/dbt-project-evaluator\n",
      "Fetching details for haithamhamad2/Zoomcamp2024\n",
      "Failed to fetch repo details for haithamhamad2/Zoomcamp2024\n",
      "Fetching details for coding-is-for-losers/shopify-buyer-segmentation\n",
      "Failed to fetch repo details for coding-is-for-losers/shopify-buyer-segmentation\n",
      "Fetching details for gabo-hacStyle/adventure-works-ea\n",
      "Failed to fetch repo details for gabo-hacStyle/adventure-works-ea\n",
      "Fetching details for karuniaperjuangan/ETL-DVD-Rental-Medallion-Architecture\n",
      "Failed to fetch repo details for karuniaperjuangan/ETL-DVD-Rental-Medallion-Architecture\n",
      "Fetching details for TomSchiro/dbt_default_sample_project-\n",
      "Failed to fetch repo details for TomSchiro/dbt_default_sample_project-\n",
      "Fetching details for Cocoon-Data-Transformation/cocoon\n",
      "Failed to fetch repo details for Cocoon-Data-Transformation/cocoon\n",
      "Fetching details for gborodrigues/ingestao-dados\n",
      "Failed to fetch repo details for gborodrigues/ingestao-dados\n",
      "Fetching details for leadang312/dbt-demo\n",
      "Failed to fetch repo details for leadang312/dbt-demo\n",
      "Fetching details for jayakrishna146/ClearBreeze2017\n",
      "Failed to fetch repo details for jayakrishna146/ClearBreeze2017\n",
      "Fetching details for dbyzero/dbt-jira-unified-api\n",
      "Failed to fetch repo details for dbyzero/dbt-jira-unified-api\n",
      "Fetching details for iamlucasmateo/dbt_eg\n",
      "Failed to fetch repo details for iamlucasmateo/dbt_eg\n",
      "Fetching details for fatimaconteh/dbt-learn-fconteh\n",
      "Failed to fetch repo details for fatimaconteh/dbt-learn-fconteh\n",
      "Fetching details for rafael-murcia-scopely/dbt-tutorial\n",
      "Failed to fetch repo details for rafael-murcia-scopely/dbt-tutorial\n",
      "Fetching details for rpmcdougall/dbt_airbnb\n",
      "Failed to fetch repo details for rpmcdougall/dbt_airbnb\n",
      "Fetching details for bbouretdev/dbt_training\n",
      "Failed to fetch repo details for bbouretdev/dbt_training\n",
      "Fetching details for Kruts-Mykhailo/NYT_Books_Analysis\n",
      "Failed to fetch repo details for Kruts-Mykhailo/NYT_Books_Analysis\n",
      "Fetching details for sparsh-ai/recohut\n",
      "Failed to fetch repo details for sparsh-ai/recohut\n",
      "Fetching details for dbt-labs/dbt-package-testing\n",
      "Failed to fetch repo details for dbt-labs/dbt-package-testing\n",
      "Fetching details for christineberger/dbt-demo-snowflake-public\n",
      "Failed to fetch repo details for christineberger/dbt-demo-snowflake-public\n",
      "Fetching details for DataWarehousePractice/20240714_dbt_jaffle-shop\n",
      "Failed to fetch repo details for DataWarehousePractice/20240714_dbt_jaffle-shop\n",
      "Fetching details for DurianDan/Xaola\n",
      "Failed to fetch repo details for DurianDan/Xaola\n",
      "Fetching details for sgmarius/dbt-core-presentation\n",
      "Failed to fetch repo details for sgmarius/dbt-core-presentation\n",
      "Fetching details for Abhijeet1026/MTG_DE\n",
      "Failed to fetch repo details for Abhijeet1026/MTG_DE\n",
      "Fetching details for larryloi/dbt-tutorial\n",
      "Failed to fetch repo details for larryloi/dbt-tutorial\n",
      "Fetching details for gk-fp/foodpanda-test-submission\n",
      "Failed to fetch repo details for gk-fp/foodpanda-test-submission\n",
      "Fetching details for cyao59/dbt-project\n",
      "Failed to fetch repo details for cyao59/dbt-project\n",
      "Fetching details for jose8delrio/curso_data_engineering\n",
      "Failed to fetch repo details for jose8delrio/curso_data_engineering\n",
      "Fetching details for saraleon1/databricks-meta-tests\n",
      "Failed to fetch repo details for saraleon1/databricks-meta-tests\n",
      "Fetching details for nttniels/gxpdf-dbt\n",
      "Failed to fetch repo details for nttniels/gxpdf-dbt\n",
      "Fetching details for oguzgogus/learning-dbt\n",
      "Failed to fetch repo details for oguzgogus/learning-dbt\n",
      "Fetching details for bgarcevic/danish-democracy-data\n",
      "Failed to fetch repo details for bgarcevic/danish-democracy-data\n",
      "Fetching details for KOlofinmoyin/dbt-demo\n",
      "Failed to fetch repo details for KOlofinmoyin/dbt-demo\n",
      "Fetching details for mahdimostafa/jlr_interview\n",
      "Failed to fetch repo details for mahdimostafa/jlr_interview\n",
      "Fetching details for Wang-Yong2018/pgai_rag_cookbook\n",
      "Failed to fetch repo details for Wang-Yong2018/pgai_rag_cookbook\n",
      "Fetching details for anmarabb/floranow-dbt\n",
      "Failed to fetch repo details for anmarabb/floranow-dbt\n",
      "Fetching details for heysweet/dbt-learn-asweet\n",
      "Failed to fetch repo details for heysweet/dbt-learn-asweet\n",
      "Fetching details for knotknull/data_engineering_pro_cert\n",
      "Failed to fetch repo details for knotknull/data_engineering_pro_cert\n",
      "Fetching details for artificialfintelligence/dbt-quickstart\n",
      "Failed to fetch repo details for artificialfintelligence/dbt-quickstart\n",
      "Fetching details for Karlobyo/demo_dbt_project\n",
      "Failed to fetch repo details for Karlobyo/demo_dbt_project\n",
      "Fetching details for haojunsng/data_voyager\n",
      "Failed to fetch repo details for haojunsng/data_voyager\n",
      "Fetching details for sfc-gh-pjain/DBT_HOL\n",
      "Failed to fetch repo details for sfc-gh-pjain/DBT_HOL\n",
      "Fetching details for rf-santos/airbyte_dbt_test\n",
      "Failed to fetch repo details for rf-santos/airbyte_dbt_test\n",
      "Fetching details for wnhoj/dbt_olist\n",
      "Failed to fetch repo details for wnhoj/dbt_olist\n",
      "Fetching details for anhbkpro/dbt_mysql_top_sql_50\n",
      "Failed to fetch repo details for anhbkpro/dbt_mysql_top_sql_50\n",
      "Fetching details for Daanielux/gz-dbt-repository\n",
      "Failed to fetch repo details for Daanielux/gz-dbt-repository\n",
      "Fetching details for harksodje/datatalk-assessment\n",
      "Failed to fetch repo details for harksodje/datatalk-assessment\n",
      "Fetching details for dotmesh-io/meltano\n",
      "Failed to fetch repo details for dotmesh-io/meltano\n",
      "Fetching details for devguilhermecarvalho/git-gcp-challenge-ipnet\n",
      "Failed to fetch repo details for devguilhermecarvalho/git-gcp-challenge-ipnet\n",
      "Fetching details for lucasdastre/Projeto-Commodities\n",
      "Failed to fetch repo details for lucasdastre/Projeto-Commodities\n",
      "Fetching details for airbytehq/airbyte-dbt-models\n",
      "Failed to fetch repo details for airbytehq/airbyte-dbt-models\n",
      "Fetching details for fivetran-DavidBenco/dbt-tutorial\n",
      "Failed to fetch repo details for fivetran-DavidBenco/dbt-tutorial\n",
      "Fetching details for jackiehearns/dbt-tutorial\n",
      "Failed to fetch repo details for jackiehearns/dbt-tutorial\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Failed to fetch repo details for xlang-ai/Spider2\n",
      "Fetching details for mortie23/dbt-postgres-nfl\n",
      "Failed to fetch repo details for mortie23/dbt-postgres-nfl\n",
      "Fetching details for Benjamin0313/dbt\n",
      "Failed to fetch repo details for Benjamin0313/dbt\n",
      "Fetching details for asadsid95/demo_dbt\n",
      "Failed to fetch repo details for asadsid95/demo_dbt\n",
      "Fetching details for khal4d87/Abdul_Demo\n",
      "Failed to fetch repo details for khal4d87/Abdul_Demo\n",
      "Fetching details for steve-yuan-8276/dbt_learning\n",
      "Failed to fetch repo details for steve-yuan-8276/dbt_learning\n",
      "Fetching details for marcobinda/SC23_AE_dbt_databricks\n",
      "Failed to fetch repo details for marcobinda/SC23_AE_dbt_databricks\n",
      "Fetching details for eugeneh101/mwaa-practice\n",
      "Failed to fetch repo details for eugeneh101/mwaa-practice\n",
      "Fetching details for ryandataengineergit/snowflake-dbt-demo\n",
      "Failed to fetch repo details for ryandataengineergit/snowflake-dbt-demo\n",
      "Fetching details for iradio/de-pro4\n",
      "Failed to fetch repo details for iradio/de-pro4\n",
      "Fetching details for OpenSourcePolitics/dbt-processing\n",
      "Failed to fetch repo details for OpenSourcePolitics/dbt-processing\n",
      "Fetching details for esirhal/ist722dbt\n",
      "Failed to fetch repo details for esirhal/ist722dbt\n",
      "Fetching details for dhruvip/de-end2end-project\n",
      "Failed to fetch repo details for dhruvip/de-end2end-project\n",
      "Fetching details for ohitsmekatie/dbt-sample-project\n",
      "Failed to fetch repo details for ohitsmekatie/dbt-sample-project\n",
      "Fetching details for alex-kolmakov/divesite-species-analytics\n",
      "Failed to fetch repo details for alex-kolmakov/divesite-species-analytics\n",
      "Fetching details for OllieClarke/ds44-dbt\n",
      "Failed to fetch repo details for OllieClarke/ds44-dbt\n",
      "Fetching details for agnivchtj/dbt-pipeline-project\n",
      "Failed to fetch repo details for agnivchtj/dbt-pipeline-project\n",
      "Fetching details for chelseyp/mage-zoomcamp-ckp\n",
      "Failed to fetch repo details for chelseyp/mage-zoomcamp-ckp\n",
      "Fetching details for vmose/Resource_Manager\n",
      "Failed to fetch repo details for vmose/Resource_Manager\n",
      "Fetching details for DatameshJulia/dw-dbt-test\n",
      "Failed to fetch repo details for DatameshJulia/dw-dbt-test\n",
      "Fetching details for jaehyeon-kim/dbt-on-aws\n",
      "Failed to fetch repo details for jaehyeon-kim/dbt-on-aws\n",
      "Fetching details for euanjohnston-dev/idealista_dbt_pipeline\n",
      "Failed to fetch repo details for euanjohnston-dev/idealista_dbt_pipeline\n",
      "Fetching details for buriihenry/Data-Engineering\n",
      "Failed to fetch repo details for buriihenry/Data-Engineering\n",
      "Fetching details for candedekoy/ELT_pipeline\n",
      "Failed to fetch repo details for candedekoy/ELT_pipeline\n",
      "Fetching details for TobikoData/sqlmesh\n",
      "Failed to fetch repo details for TobikoData/sqlmesh\n",
      "Fetching details for datarootsio/your-best-bet\n",
      "Failed to fetch repo details for datarootsio/your-best-bet\n",
      "Fetching details for oysterhr/oyster_dbt_project_evaluator\n",
      "Failed to fetch repo details for oysterhr/oyster_dbt_project_evaluator\n",
      "Fetching details for Romanmc72/dbt\n",
      "Failed to fetch repo details for Romanmc72/dbt\n",
      "Fetching details for Ayoub-Talbi1/Reddit-ELT\n",
      "Failed to fetch repo details for Ayoub-Talbi1/Reddit-ELT\n",
      "Fetching details for mashiike/dbt-methods\n",
      "Failed to fetch repo details for mashiike/dbt-methods\n",
      "Fetching details for navikt/vdl-okonomimodell\n",
      "Failed to fetch repo details for navikt/vdl-okonomimodell\n",
      "Fetching details for pardub/archive_github\n",
      "Failed to fetch repo details for pardub/archive_github\n",
      "Fetching details for Zenlytic/demo-dbt-data-model\n",
      "Failed to fetch repo details for Zenlytic/demo-dbt-data-model\n",
      "Fetching details for sbecker11/noisy_dbt\n",
      "Failed to fetch repo details for sbecker11/noisy_dbt\n",
      "Fetching details for analyticsengineersclub/coffee-shop-maher\n",
      "Failed to fetch repo details for analyticsengineersclub/coffee-shop-maher\n",
      "Fetching details for chironcodes/bix_challenge\n",
      "Failed to fetch repo details for chironcodes/bix_challenge\n",
      "Fetching details for lauravilaseca/dbt-tutorial\n",
      "Failed to fetch repo details for lauravilaseca/dbt-tutorial\n",
      "Fetching details for tomer958/Play-Perfect-Test\n",
      "Failed to fetch repo details for tomer958/Play-Perfect-Test\n",
      "Fetching details for simontroedsson/datawarehouse_project_D23_Data\n",
      "Failed to fetch repo details for simontroedsson/datawarehouse_project_D23_Data\n",
      "Fetching details for Deepanshuigtm/dbt-nyc_parking_data\n",
      "Failed to fetch repo details for Deepanshuigtm/dbt-nyc_parking_data\n",
      "Fetching details for Obrotoks/dbt_expectations_example\n",
      "Failed to fetch repo details for Obrotoks/dbt_expectations_example\n",
      "Fetching details for fnery/fitbit-steps\n",
      "Failed to fetch repo details for fnery/fitbit-steps\n",
      "Fetching details for yuxiangl6/zoomcamp24\n",
      "Failed to fetch repo details for yuxiangl6/zoomcamp24\n",
      "Fetching details for phemmmie/databricks-dbt\n",
      "Failed to fetch repo details for phemmmie/databricks-dbt\n",
      "Fetching details for sahilkotak/bde3-dbt\n",
      "Failed to fetch repo details for sahilkotak/bde3-dbt\n",
      "Fetching details for AdhamAlQatawy/airbnb-dbt\n",
      "Failed to fetch repo details for AdhamAlQatawy/airbnb-dbt\n",
      "Fetching details for Captainmango/elt-playground\n",
      "Failed to fetch repo details for Captainmango/elt-playground\n",
      "Fetching details for exefncs2/demo_2023_10\n",
      "Failed to fetch repo details for exefncs2/demo_2023_10\n",
      "Fetching details for Oushesh/Airbyte_LLM\n",
      "Failed to fetch repo details for Oushesh/Airbyte_LLM\n",
      "Fetching details for angelo-oviedo/BlueMar\n",
      "Failed to fetch repo details for angelo-oviedo/BlueMar\n",
      "Fetching details for samurai-py/olist-dda-etl\n",
      "Failed to fetch repo details for samurai-py/olist-dda-etl\n",
      "Fetching details for BigDataIA-Spring2024-Sec2-Team4/Assignment4\n",
      "Failed to fetch repo details for BigDataIA-Spring2024-Sec2-Team4/Assignment4\n",
      "Fetching details for Isaac-Ndirangu-Muturi-749/End_to_end_data_pipeline--Optimizing_Online_Retail_Analytics_with_Data_and_Analytics_Engineering\n",
      "Failed to fetch repo details for Isaac-Ndirangu-Muturi-749/End_to_end_data_pipeline--Optimizing_Online_Retail_Analytics_with_Data_and_Analytics_Engineering\n",
      "Fetching details for arranzignacio/course-dbt-nacho\n",
      "Failed to fetch repo details for arranzignacio/course-dbt-nacho\n",
      "Fetching details for sfc-gh-evenlet/dbt_orchestration_demo\n",
      "Failed to fetch repo details for sfc-gh-evenlet/dbt_orchestration_demo\n",
      "Fetching details for samuelzhang1201/customer_case\n",
      "Failed to fetch repo details for samuelzhang1201/customer_case\n",
      "Fetching details for jaehyeon-kim/dbt-on-aws\n",
      "Failed to fetch repo details for jaehyeon-kim/dbt-on-aws\n",
      "Fetching details for Taisunnn/wonderland\n",
      "Failed to fetch repo details for Taisunnn/wonderland\n",
      "Fetching details for RevanthVe/Nyc_taxi_dbt\n",
      "Failed to fetch repo details for RevanthVe/Nyc_taxi_dbt\n",
      "Fetching details for exequiel-santucho/data-engineering-zoomcamp-homework\n",
      "Failed to fetch repo details for exequiel-santucho/data-engineering-zoomcamp-homework\n",
      "Fetching details for wusixuan0/dbt_project\n",
      "Failed to fetch repo details for wusixuan0/dbt_project\n",
      "Fetching details for alessandracruz/AnalyticsEngineering\n",
      "Failed to fetch repo details for alessandracruz/AnalyticsEngineering\n",
      "Fetching details for alainissa/cerba_research_test\n",
      "Failed to fetch repo details for alainissa/cerba_research_test\n",
      "Fetching details for douenergy/Spodbtify\n",
      "Failed to fetch repo details for douenergy/Spodbtify\n",
      "Fetching details for mirnawong1/dbt-learn-zero\n",
      "Failed to fetch repo details for mirnawong1/dbt-learn-zero\n",
      "Fetching details for shivamsaryar/Mojo-F1\n",
      "Failed to fetch repo details for shivamsaryar/Mojo-F1\n",
      "Fetching details for ticuss/dbt\n",
      "Failed to fetch repo details for ticuss/dbt\n",
      "Fetching details for nicoaws/dbt-sample-project\n",
      "Failed to fetch repo details for nicoaws/dbt-sample-project\n",
      "Fetching details for bakerbryce/dbt_snowtools\n",
      "Failed to fetch repo details for bakerbryce/dbt_snowtools\n",
      "Fetching details for fsnovais/dbt_development\n",
      "Failed to fetch repo details for fsnovais/dbt_development\n",
      "Fetching details for astronomer/cosmos-example\n",
      "Failed to fetch repo details for astronomer/cosmos-example\n",
      "Fetching details for mahmoud-farah/databricks-dbt\n",
      "Failed to fetch repo details for mahmoud-farah/databricks-dbt\n",
      "Fetching details for martartigot/Proyecto_Final_Civica\n",
      "Failed to fetch repo details for martartigot/Proyecto_Final_Civica\n",
      "Fetching details for GuinsooLab/stealthward\n",
      "Failed to fetch repo details for GuinsooLab/stealthward\n",
      "Fetching details for IanDanielM/ZoomCamp-DataEngineer\n",
      "Failed to fetch repo details for IanDanielM/ZoomCamp-DataEngineer\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Failed to fetch repo details for xlang-ai/Spider2\n",
      "Fetching details for MohamedSy1/ELT-pipeline-olympic-dataset\n",
      "Failed to fetch repo details for MohamedSy1/ELT-pipeline-olympic-dataset\n",
      "Fetching details for Dumontsophiewagon/gz-dbt-repository\n",
      "Failed to fetch repo details for Dumontsophiewagon/gz-dbt-repository\n",
      "Fetching details for dluftspring/dans-dbt-diary\n",
      "Failed to fetch repo details for dluftspring/dans-dbt-diary\n",
      "Fetching details for Bishal123456789/dummy_airbnb\n",
      "Failed to fetch repo details for Bishal123456789/dummy_airbnb\n",
      "Fetching details for jvanbuel/dbt-tutorial\n",
      "Failed to fetch repo details for jvanbuel/dbt-tutorial\n",
      "Fetching details for raphaelabenom/database-north\n",
      "Failed to fetch repo details for raphaelabenom/database-north\n",
      "Fetching details for vincentclaes/dbt_showcase\n",
      "Failed to fetch repo details for vincentclaes/dbt_showcase\n",
      "Fetching details for Vijaykrishna94/Dema-E-Commerece\n",
      "Failed to fetch repo details for Vijaykrishna94/Dema-E-Commerece\n",
      "Fetching details for supermonkey62/Team2ADO\n",
      "Failed to fetch repo details for supermonkey62/Team2ADO\n",
      "Fetching details for bootsielon/dbt\n",
      "Failed to fetch repo details for bootsielon/dbt\n",
      "Fetching details for mwhitaker/dbt-action-sample\n",
      "Failed to fetch repo details for mwhitaker/dbt-action-sample\n",
      "Fetching details for olga12401/postgres-azure-dbt\n",
      "Failed to fetch repo details for olga12401/postgres-azure-dbt\n",
      "Fetching details for aec-self-study/aec-discepoli\n",
      "Failed to fetch repo details for aec-self-study/aec-discepoli\n",
      "Fetching details for jaehyeon-kim/general-demos\n",
      "Failed to fetch repo details for jaehyeon-kim/general-demos\n",
      "Fetching details for sidataplus/ETL-Synthea-dbt\n",
      "Failed to fetch repo details for sidataplus/ETL-Synthea-dbt\n",
      "Fetching details for zkan/metaplane-dbt\n",
      "Failed to fetch repo details for zkan/metaplane-dbt\n",
      "Fetching details for DanielPDWalker/Theme-Parks-Project\n",
      "Failed to fetch repo details for DanielPDWalker/Theme-Parks-Project\n",
      "Fetching details for GinaGrg1/all_that_dbt\n",
      "Failed to fetch repo details for GinaGrg1/all_that_dbt\n",
      "Fetching details for cloudquery/policies\n",
      "Failed to fetch repo details for cloudquery/policies\n",
      "Fetching details for saddalaaws/snowflakebdtfivetran\n",
      "Failed to fetch repo details for saddalaaws/snowflakebdtfivetran\n",
      "Fetching details for the-prairie/source-alberta-re\n",
      "Failed to fetch repo details for the-prairie/source-alberta-re\n",
      "Fetching details for Cidro/snowplow\n",
      "Failed to fetch repo details for Cidro/snowplow\n",
      "Fetching details for paulf-999/dbt_projects\n",
      "Failed to fetch repo details for paulf-999/dbt_projects\n",
      "Fetching details for paulf-999/dbt_projects\n",
      "Failed to fetch repo details for paulf-999/dbt_projects\n",
      "Fetching details for paulf-999/dbt_projects\n",
      "Failed to fetch repo details for paulf-999/dbt_projects\n",
      "Fetching details for paulf-999/dbt_projects\n",
      "Failed to fetch repo details for paulf-999/dbt_projects\n",
      "Fetching details for frothkoetter/dbt_airlinedata_demo\n",
      "Failed to fetch repo details for frothkoetter/dbt_airlinedata_demo\n",
      "Fetching details for cmj123/mastering_dbt\n",
      "Failed to fetch repo details for cmj123/mastering_dbt\n",
      "Fetching details for marycharow/dec_northwind\n",
      "Failed to fetch repo details for marycharow/dec_northwind\n",
      "Fetching details for el-grudge/money-diaries\n",
      "Failed to fetch repo details for el-grudge/money-diaries\n",
      "Fetching details for mahevarma/DBT\n",
      "Failed to fetch repo details for mahevarma/DBT\n",
      "Fetching details for jazzDung/financial\n",
      "Failed to fetch repo details for jazzDung/financial\n",
      "Fetching details for codep-ai/dbt-demo\n",
      "Failed to fetch repo details for codep-ai/dbt-demo\n",
      "Fetching details for Elolewis/sloop\n",
      "Failed to fetch repo details for Elolewis/sloop\n",
      "Fetching details for Androidown/dagster\n",
      "Failed to fetch repo details for Androidown/dagster\n",
      "Fetching details for dbeatty10/dbt-duckdb-playground\n",
      "Failed to fetch repo details for dbeatty10/dbt-duckdb-playground\n",
      "Fetching details for Katrindenek/Cloud-Data-Vault-with-dbt-and-Airbyte\n",
      "Failed to fetch repo details for Katrindenek/Cloud-Data-Vault-with-dbt-and-Airbyte\n",
      "Fetching details for Anton-Melin/warehouse_anton_m\n",
      "Failed to fetch repo details for Anton-Melin/warehouse_anton_m\n",
      "Fetching details for kmiller96/menagerie\n",
      "Failed to fetch repo details for kmiller96/menagerie\n",
      "Fetching details for VyPhammm/dbt_game_data_project\n",
      "Failed to fetch repo details for VyPhammm/dbt_game_data_project\n",
      "Fetching details for oysterhr/oyster_dbt_project_evaluator\n",
      "Failed to fetch repo details for oysterhr/oyster_dbt_project_evaluator\n",
      "Fetching details for aaalexlit/data-engineering-zoomcamp-alex\n",
      "Failed to fetch repo details for aaalexlit/data-engineering-zoomcamp-alex\n",
      "Fetching details for AmaliaTemneanu/data-engineering-zoomcamp-assignments\n",
      "Failed to fetch repo details for AmaliaTemneanu/data-engineering-zoomcamp-assignments\n",
      "Fetching details for GrentinaK/dbt-databricks\n",
      "Failed to fetch repo details for GrentinaK/dbt-databricks\n",
      "Fetching details for Abhijeet1026/DataEngineering\n",
      "Failed to fetch repo details for Abhijeet1026/DataEngineering\n",
      "Fetching details for zunayedology/foodpanda-test-submission\n",
      "Failed to fetch repo details for zunayedology/foodpanda-test-submission\n",
      "Fetching details for zwfang-031/foodpanda-test-submission\n",
      "Failed to fetch repo details for zwfang-031/foodpanda-test-submission\n",
      "Fetching details for syedamaann/LearningLog\n",
      "Failed to fetch repo details for syedamaann/LearningLog\n",
      "Fetching details for razorcd/data-engineering-training\n",
      "Failed to fetch repo details for razorcd/data-engineering-training\n",
      "Fetching details for Technocolabs100/Data-Engineer-Training-Materials\n",
      "Failed to fetch repo details for Technocolabs100/Data-Engineer-Training-Materials\n",
      "Fetching details for MurtazaNaiyar/Data-Engineering\n",
      "Failed to fetch repo details for MurtazaNaiyar/Data-Engineering\n",
      "Fetching details for ssoj1/dbt-tutorial\n",
      "Failed to fetch repo details for ssoj1/dbt-tutorial\n",
      "Fetching details for protechanalysis/cde_capstone_project_24\n",
      "Failed to fetch repo details for protechanalysis/cde_capstone_project_24\n",
      "Fetching details for cllaud99/olist-data\n",
      "Failed to fetch repo details for cllaud99/olist-data\n",
      "Fetching details for niits/homelab-finance\n",
      "Failed to fetch repo details for niits/homelab-finance\n",
      "Fetching details for jjbokan3/sports_betting_pipeline\n",
      "Failed to fetch repo details for jjbokan3/sports_betting_pipeline\n",
      "Fetching details for dagster-io/dagster\n",
      "Failed to fetch repo details for dagster-io/dagster\n",
      "Fetching details for dbt-labs/dbt-jsonschema\n",
      "Failed to fetch repo details for dbt-labs/dbt-jsonschema\n",
      "Fetching details for XamtacConsulting/Airbyte-docker-test\n",
      "Failed to fetch repo details for XamtacConsulting/Airbyte-docker-test\n",
      "Fetching details for EqualExperts/dbt-unit-testing\n",
      "Failed to fetch repo details for EqualExperts/dbt-unit-testing\n",
      "Fetching details for tellery/tellery\n",
      "Failed to fetch repo details for tellery/tellery\n",
      "Fetching details for infinitelambda/dbt-central-app\n",
      "Failed to fetch repo details for infinitelambda/dbt-central-app\n",
      "Fetching details for TasmanAnalytics/tasman-dbt-revenuecat\n",
      "Failed to fetch repo details for TasmanAnalytics/tasman-dbt-revenuecat\n",
      "Fetching details for fivetran/dbt_google_ads_source\n",
      "Failed to fetch repo details for fivetran/dbt_google_ads_source\n",
      "Fetching details for phdata/dbt_coalesce_2022_actionable_insights\n",
      "Failed to fetch repo details for phdata/dbt_coalesce_2022_actionable_insights\n",
      "Fetching details for Matts52/dbt-ml-inline-preprocessing\n",
      "Failed to fetch repo details for Matts52/dbt-ml-inline-preprocessing\n",
      "Fetching details for mxk3mbr3/epic_nautilus\n",
      "Failed to fetch repo details for mxk3mbr3/epic_nautilus\n",
      "Fetching details for tomekzakrzewski/nfj_elt_pipeline\n",
      "Failed to fetch repo details for tomekzakrzewski/nfj_elt_pipeline\n",
      "Fetching details for fivetran/dbt_marketo_source\n",
      "Failed to fetch repo details for fivetran/dbt_marketo_source\n",
      "Fetching details for TomSchiro/opi-dbtvault\n",
      "Failed to fetch repo details for TomSchiro/opi-dbtvault\n",
      "Fetching details for jazzDung/retail-banking-customer360\n",
      "Failed to fetch repo details for jazzDung/retail-banking-customer360\n",
      "Fetching details for BobProphecy/snow\n",
      "Failed to fetch repo details for BobProphecy/snow\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Failed to fetch repo details for xlang-ai/Spider2\n",
      "Fetching details for pcrespoo/data-engineering-bootcamp\n",
      "Failed to fetch repo details for pcrespoo/data-engineering-bootcamp\n",
      "Fetching details for josephmachado/simple_dbt_project\n",
      "Failed to fetch repo details for josephmachado/simple_dbt_project\n",
      "Fetching details for NiranjanRao07/EndToEnd_Data_Engineering_Project\n",
      "Failed to fetch repo details for NiranjanRao07/EndToEnd_Data_Engineering_Project\n",
      "Fetching details for wearemojo/mojo-hiring-de-task\n",
      "Failed to fetch repo details for wearemojo/mojo-hiring-de-task\n",
      "Fetching details for invictus2010/ae-club-jwith\n",
      "Failed to fetch repo details for invictus2010/ae-club-jwith\n",
      "Fetching details for giftHEALTH/dbt_zendesk_source\n",
      "Failed to fetch repo details for giftHEALTH/dbt_zendesk_source\n",
      "Fetching details for xlang-ai/Spider2\n",
      "Failed to fetch repo details for xlang-ai/Spider2\n",
      "Fetching details for salmah52/Chicago_Taxitrips009\n",
      "Failed to fetch repo details for salmah52/Chicago_Taxitrips009\n",
      "Fetching details for frederikstevens1997/dbt_learning\n",
      "Failed to fetch repo details for frederikstevens1997/dbt_learning\n",
      "Fetching details for tom-juntunen/demo_dbt_semantic_layer\n",
      "Failed to fetch repo details for tom-juntunen/demo_dbt_semantic_layer\n",
      "Fetching details for divijkulshrestha/gdelt-data-pipeline\n",
      "Failed to fetch repo details for divijkulshrestha/gdelt-data-pipeline\n",
      "Fetching details for MahmoudMahdy448/Football-Data-Analytics\n",
      "Failed to fetch repo details for MahmoudMahdy448/Football-Data-Analytics\n",
      "Fetching details for luisferreira97/foreman\n",
      "Failed to fetch repo details for luisferreira97/foreman\n",
      "Fetching details for Abdallah0799/data-pipelines-elite-drip\n",
      "Failed to fetch repo details for Abdallah0799/data-pipelines-elite-drip\n",
      "Fetching details for analyticsengineersclub/coffee-shop-timdene\n",
      "Failed to fetch repo details for analyticsengineersclub/coffee-shop-timdene\n",
      "Fetching details for gregorywmorris/oaken-spirits\n",
      "Failed to fetch repo details for gregorywmorris/oaken-spirits\n",
      "Fetching details for z3z1ma/dbt-osmosis\n",
      "Failed to fetch repo details for z3z1ma/dbt-osmosis\n",
      "Fetching details for ImpulsoGov/saude-mental-indicadores\n",
      "Failed to fetch repo details for ImpulsoGov/saude-mental-indicadores\n",
      "Fetching details for binste/dbt-ibis\n",
      "Failed to fetch repo details for binste/dbt-ibis\n",
      "Fetching details for matt-winkler/demo-tpch-bigquery\n",
      "Failed to fetch repo details for matt-winkler/demo-tpch-bigquery\n",
      "Fetching details for kahramanmurat/ny_taxi_rides\n",
      "Failed to fetch repo details for kahramanmurat/ny_taxi_rides\n",
      "Fetching details for nawaf91maqbali/testdbt\n",
      "Failed to fetch repo details for nawaf91maqbali/testdbt\n",
      "Fetching details for Dimasega18/DBT-Game-Sales-ELT\n",
      "Failed to fetch repo details for Dimasega18/DBT-Game-Sales-ELT\n",
      "Fetching details for fivetran/dbt_klaviyo\n",
      "Failed to fetch repo details for fivetran/dbt_klaviyo\n",
      "Fetching details for kristin-bagnall/sfdc_testing\n",
      "Failed to fetch repo details for kristin-bagnall/sfdc_testing\n",
      "Fetching details for teraearlywine/analytics\n",
      "Failed to fetch repo details for teraearlywine/analytics\n",
      "Fetching details for GabrielBossardi/GabrielBossardi-data-coding-interview\n",
      "Failed to fetch repo details for GabrielBossardi/GabrielBossardi-data-coding-interview\n",
      "Fetching details for bobbyiliev/materialize-dbt-action\n",
      "Failed to fetch repo details for bobbyiliev/materialize-dbt-action\n",
      "Fetching details for dhruvcpatel95/DataengineeringZC2024\n",
      "Failed to fetch repo details for dhruvcpatel95/DataengineeringZC2024\n",
      "Fetching details for vfdesouza/etl-adventure-works\n",
      "Failed to fetch repo details for vfdesouza/etl-adventure-works\n",
      "Fetching details for urevoleg/master-ozon-api\n",
      "Failed to fetch repo details for urevoleg/master-ozon-api\n",
      "Fetching details for Sean-Liu-GitHub/taxi_rides_ny\n",
      "Failed to fetch repo details for Sean-Liu-GitHub/taxi_rides_ny\n",
      "Fetching details for PhilaeRoetta/DBT_016\n",
      "Failed to fetch repo details for PhilaeRoetta/DBT_016\n",
      "Fetching details for kasivisu4/dbt_airflow\n",
      "Failed to fetch repo details for kasivisu4/dbt_airflow\n",
      "Fetching details for ShaalanMarwan/data-engineering-DataTalksClub\n",
      "Failed to fetch repo details for ShaalanMarwan/data-engineering-DataTalksClub\n",
      "Fetching details for omjirapat/data-engineering-zoomcamp\n",
      "Failed to fetch repo details for omjirapat/data-engineering-zoomcamp\n",
      "Fetching details for splitgraph/sgr\n",
      "Failed to fetch repo details for splitgraph/sgr\n",
      "Fetching details for jainsahil77/dbt-postgres-sj1\n",
      "Failed to fetch repo details for jainsahil77/dbt-postgres-sj1\n",
      "Fetching details for bachkaxyz/bread\n",
      "Failed to fetch repo details for bachkaxyz/bread\n",
      "Fetching details for jhannan13/corise-jan-23\n",
      "Failed to fetch repo details for jhannan13/corise-jan-23\n",
      "Fetching details for sharanair29/ELT_Project\n",
      "Failed to fetch repo details for sharanair29/ELT_Project\n",
      "Fetching details for christophermanning/data-workspace\n",
      "Failed to fetch repo details for christophermanning/data-workspace\n",
      "Fetching details for vicky9536/DATA226-Group-Project\n",
      "Failed to fetch repo details for vicky9536/DATA226-Group-Project\n",
      "Fetching details for brajajain/stellar_health_take_home\n",
      "Failed to fetch repo details for brajajain/stellar_health_take_home\n",
      "Fetching details for mavost/urban-dbt\n",
      "Failed to fetch repo details for mavost/urban-dbt\n",
      "Fetching details for fivetran/dbt_netsuite_source\n",
      "Failed to fetch repo details for fivetran/dbt_netsuite_source\n",
      "Fetching details for alfredzou/Zoomcamp2024\n",
      "Failed to fetch repo details for alfredzou/Zoomcamp2024\n",
      "Fetching details for drjodyannjones/customer_behaviour_pipeline\n",
      "Failed to fetch repo details for drjodyannjones/customer_behaviour_pipeline\n",
      "Fetching details for Kev1606/Data_Engineering\n",
      "Failed to fetch repo details for Kev1606/Data_Engineering\n",
      "Fetching details for weidir/sqlmesh-dbt-demo\n",
      "Failed to fetch repo details for weidir/sqlmesh-dbt-demo\n",
      "Fetching details for rittmananalytics/ra_attribution\n",
      "Failed to fetch repo details for rittmananalytics/ra_attribution\n",
      "Fetching details for viniciussalinas/northwind_dw\n",
      "Failed to fetch repo details for viniciussalinas/northwind_dw\n",
      "Fetching details for mbertrand/dbt-ny-taxi-rides\n",
      "Failed to fetch repo details for mbertrand/dbt-ny-taxi-rides\n",
      "Fetching details for scalio/epoch8-dbt-package-ga4\n",
      "Failed to fetch repo details for scalio/epoch8-dbt-package-ga4\n",
      "Fetching details for hnawaz007/dbt-dw\n",
      "Failed to fetch repo details for hnawaz007/dbt-dw\n",
      "Fetching details for Teradata/jaffle_shop_teradata\n",
      "Failed to fetch repo details for Teradata/jaffle_shop_teradata\n",
      "Fetching details for jonasberlinma/dbt-test\n",
      "Failed to fetch repo details for jonasberlinma/dbt-test\n",
      "Fetching details for Rafael-Ribeiro-de-Lima/northwind\n",
      "Failed to fetch repo details for Rafael-Ribeiro-de-Lima/northwind\n",
      "Fetching details for jhannan13/spotify-analysis\n",
      "Failed to fetch repo details for jhannan13/spotify-analysis\n",
      "Fetching details for joecbloom/asana_scrum_dashboard\n",
      "Failed to fetch repo details for joecbloom/asana_scrum_dashboard\n",
      "Fetching details for JeremyLG/open-data-stack\n",
      "Failed to fetch repo details for JeremyLG/open-data-stack\n",
      "Fetching details for devseunggwan/scratchs\n",
      "Failed to fetch repo details for devseunggwan/scratchs\n",
      "Fetching details for Nehal3437/dbt-personal\n",
      "Failed to fetch repo details for Nehal3437/dbt-personal\n",
      "Fetching details for amychen1776/vhol-test\n",
      "Failed to fetch repo details for amychen1776/vhol-test\n",
      "Data saved to github_search_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        \n",
    "        print(f\"Requesting: {url} with params: {params}\")\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch repository details\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\")\n",
    "    }\n",
    "\n",
    "# Load queries from Excel file\n",
    "input_file = \"DataQualityToolsList1.xlsx\"  # Replace with your Excel file name\n",
    "df_queries = pd.read_excel(input_file)\n",
    "queries = df_queries[\"search string\"].dropna().tolist()  # Assume column name is 'search_query'\n",
    "\n",
    "# Process each query\n",
    "all_repositories = []\n",
    "for query in queries:\n",
    "    print(f\"Processing query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            all_repositories.append(repo_details)\n",
    "\n",
    "# Save results to Excel\n",
    "output_file = \"github_search_results.xlsx\"\n",
    "df_output = pd.DataFrame(all_repositories)\n",
    "df_output.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05290fe-3bc0-426c-b7d0-4b34c7032ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"  # Replace with your GitHub t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5cdc8-9bd9-4952-97ef-404e68a37011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: filename:great_expectations extension:.yml size:<500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:<500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 9 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:<500', 'per_page': 100, 'page': 2}\n",
      "Fetching details for jacopotagliabue/you-dont-need-a-bigger-boat\n",
      "Fetching details for oss-compass/compass-projects-information\n",
      "Fetching details for erwinpaillacan/kedro-great-expectations-example\n",
      "Fetching details for CardosoJr/bootcamp\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Processing query: filename:great_expectations extension:.yml size:500..1000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:500..1000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 8 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:500..1000', 'per_page': 100, 'page': 2}\n",
      "Fetching details for vnugny/DataModel-GXX\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Processing query: filename:great_expectations extension:.yml size:1000..1500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:1000..1500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 7 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:1000..1500', 'per_page': 100, 'page': 2}\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Processing query: filename:great_expectations extension:.yml size:1500..2000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:1500..2000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 12 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:great_expectations extension:.yml size:1500..2000', 'per_page': 100, 'page': 2}\n",
      "Fetching details for Sage-Bionetworks/recover\n",
      "Fetching details for julian-west/e4ds-snippets\n",
      "Fetching details for scoyne2/data-manager\n",
      "Fetching details for dunghoang369/feature-store\n",
      "Fetching details for meredithoopis/Capstone_DE_project\n",
      "Fetching details for mhabedank/dwd-weather-data-ingestion\n",
      "Fetching details for syalanuj/youtube\n",
      "Fetching details for samurai-py/weather-traffic-etl\n",
      "Fetching details for freireoliveira/great_expectations_project\n",
      "Fetching details for great-expectations/great_expectations\n",
      "Fetching details for bartosz25/python-playground\n",
      "Fetching details for naveeagrawal/mlops\n",
      "Processing query: filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:<500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:<500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 24 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:<500', 'per_page': 100, 'page': 2}\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:<500', 'per_page': 100, 'page': 3}\n",
      "Fetching details for alipay/alipay-sdk-python-all\n",
      "Fetching details for zBrainiac/streaming-flink\n",
      "Fetching details for robson14br/Python-AI-Backend-Developer\n",
      "Fetching details for himnsuk/Python-Practice\n",
      "Fetching details for HERBETON25/AtividadeDe-Casa\n",
      "Fetching details for Sun-Zhiyuan/GVL\n",
      "Fetching details for leeFonzo/Practica-POO\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for several27/ProphecyDataQualityDemo\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for SourceryAI/pydeequ3\n",
      "Fetching details for siddhant-deepsource/pydeequ3\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for EloneSampaio/cmms-manutencao\n",
      "Fetching details for koking0/LuffyCity\n",
      "Fetching details for koking0/LuffyCity\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Processing query: filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:500..1000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:500..1000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 47 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:500..1000', 'per_page': 100, 'page': 2}\n",
      "Fetching details for data-science-on-aws/data-science-on-aws\n",
      "Fetching details for alipay/alipay-sdk-python-all\n",
      "Fetching details for xplot/imeet\n",
      "Fetching details for D7S5/_Spark\n",
      "Fetching details for krishnainfoblox/data_dreamers\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for natetongwong/SparkPythonDeequ\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for canimus/cuallee\n",
      "Fetching details for several27/ProphecyDataQualityDemo\n",
      "Fetching details for bhaveshbendale/data-science-on-aws\n",
      "Fetching details for MADS508/labs\n",
      "Fetching details for lucentcosmos/awsds\n",
      "Fetching details for artemponomarevjetski/aws-ml-data-pipeline-deep-learniing\n",
      "Fetching details for oonisim/books-datascience-on-aws\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2021-06-28\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2021-05-15\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200425\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200523\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200822\n",
      "Fetching details for ChubaOraka/data-science-on-aws-workshop-2020-09-26\n",
      "Fetching details for Chuba-Oraka/workshop-ML-20200919\n",
      "Fetching details for vijay-khanna/data-science-on-aws-sagemaker\n",
      "Fetching details for leoalexand/aws-ds-NLP-BERT\n",
      "Fetching details for YushinJung-ATG/SageMaker-Workshop\n",
      "Fetching details for weharris/data-science-on-AWS\n",
      "Fetching details for HadiMaqbool/workshp29\n",
      "Fetching details for bwcx-aws/data-science-on-aws-workshop\n",
      "Fetching details for anupam3693/data-science-on-aws\n",
      "Fetching details for sureindia-in/data-science-on-aws-workshop\n",
      "Fetching details for sureindia-in/aws-sagemaker-workshop-jun2021\n",
      "Fetching details for sureindia-in/data-science-on-aws-latest-2021\n",
      "Fetching details for ichen20/oreilly_book\n",
      "Fetching details for hydrobot004/datascience-on-aws\n",
      "Fetching details for esenebenjamin/SageMaker\n",
      "Fetching details for shumshersubashgautam/DataScience-AWS\n",
      "Fetching details for mike-maclaverty/data-science-on-aws\n",
      "Fetching details for bhupen/aws_datascience_workshop\n",
      "Fetching details for flavia-sosa/data-science-on-aws-pipelineai\n",
      "Fetching details for vijay-khanna/data-science-on-aws-workshop\n",
      "Fetching details for Chalcym/AWS_datascience_workshop\n",
      "Fetching details for aditya-chaturvedi/pipeline.ai-workshop\n",
      "Fetching details for sureindia-in/data-science-on-aws-archive-good\n",
      "Fetching details for Arijit-datascience/aws_workshop\n",
      "Fetching details for demonhawk007/AWS-BERT-Workshop\n",
      "Fetching details for cyourth-cognonic/aws-workshop\n",
      "Processing query: filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:1000..1500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:1000..1500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 9 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:1000..1500', 'per_page': 100, 'page': 2}\n",
      "Fetching details for xplot/imeet\n",
      "Fetching details for CamiloMWizeline/arcxp-interview\n",
      "Fetching details for pflun/advancedAlgorithms\n",
      "Fetching details for depaulatiago/bootcampDIOPython\n",
      "Fetching details for yanvoi/leetcode_solutions\n",
      "Fetching details for alexbprr/Modelagem-Computacional\n",
      "Fetching details for Yazdish/PyLearnAssignments\n",
      "Fetching details for kcirtapfromspace/database_thing\n",
      "Fetching details for krishnainfoblox/data_dreamers\n",
      "Processing query: filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:1500..2000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:1500..2000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 8 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'filename:deequ extension:.jar,deequ extension:.sh,pydeequ extension:.py size:1500..2000', 'per_page': 100, 'page': 2}\n",
      "Fetching details for databand-ai/dbnd\n",
      "Fetching details for bssrdf/pyleet\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for mldlwizard/Evaluating-the-proficiency-of-LLM-s-in-DSA-An-analysis-of-standardized-test-performance\n",
      "Fetching details for Rafael-ST/rede\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Fetching details for Roshan-Here/leetcode-scrapper\n",
      "Processing query: pandera extension:.py,pandera extension:.ipynb size:<500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:<500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 2 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:<500', 'per_page': 100, 'page': 2}\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for DrShushen/practice_py\n",
      "Processing query: pandera extension:.py,pandera extension:.ipynb size:500..1000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:500..1000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 2 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:500..1000', 'per_page': 100, 'page': 2}\n",
      "Fetching details for DrShushen/practice_py\n",
      "Fetching details for noklam/kedro-pandera-iris\n",
      "Processing query: pandera extension:.py,pandera extension:.ipynb size:1000..1500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:1000..1500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 1 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:1000..1500', 'per_page': 100, 'page': 2}\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:1000..1500', 'per_page': 100, 'page': 3}\n",
      "Fetching details for Taecpy/validate_exim\n",
      "Processing query: pandera extension:.py,pandera extension:.ipynb size:1500..2000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:1500..2000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 1 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pandera extension:.py,pandera extension:.ipynb size:1500..2000', 'per_page': 100, 'page': 2}\n",
      "Fetching details for lgarzia/library_explorations\n",
      "Processing query: apachegriffin extension:.yml size:<500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml size:<500', 'per_page': 100, 'page': 1}\n",
      "Processing query: apachegriffin extension:.yml size:500..1000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml size:500..1000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 1 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml size:500..1000', 'per_page': 100, 'page': 2}\n",
      "Fetching details for tanxinzheng/docker-image-demo\n",
      "Processing query: apachegriffin extension:.yml size:1000..1500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml size:1000..1500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 7 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml size:1000..1500', 'per_page': 100, 'page': 2}\n",
      "Fetching details for apache/griffin\n",
      "Fetching details for dershov173/Spark-Streaming-Project\n",
      "Fetching details for zcswl7961/apache-griffin-expand\n",
      "Fetching details for ep-infosec/33_apache_griffin\n",
      "Fetching details for 15802519394/griffin\n",
      "Fetching details for rohan-flutterint/griffin\n",
      "Fetching details for rohan-flutterint/griffin-scala\n",
      "Processing query: apachegriffin extension:.yml size:1500..2000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml size:1500..2000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 6 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'apachegriffin extension:.yml size:1500..2000', 'per_page': 100, 'page': 2}\n",
      "Rate limit exceeded. Waiting for reset...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        \n",
    "        print(f\"Requesting: {url} with params: {params}\")\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch repository details\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    \n",
    "    Args:\n",
    "        owner (str): Repository owner (user/org).\n",
    "        repo (str): Repository name.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    # Fetch basic repo details\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "    \n",
    "    # Return detailed metadata\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\")\n",
    "    }\n",
    "\n",
    "# Load queries from Excel file\n",
    "input_file = \"DataQualityToolsList1.xlsx\"  # Replace with your Excel file name\n",
    "df_queries = pd.read_excel(input_file)\n",
    "queries = df_queries[\"search string\"].dropna().tolist()\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "size_ranges = [\n",
    "    \"<500\",\n",
    "    \"500..1000\",\n",
    "    \"1000..1500\",\n",
    "    \"1500..2000\"\n",
    "]\n",
    "\n",
    "# Process each query with size-based partitioning and fetch metadata\n",
    "all_repositories = []\n",
    "for query in queries:\n",
    "    for size in size_ranges:\n",
    "        full_query = f\"{query} size:{size}\"\n",
    "        print(f\"Processing query: {full_query}\")\n",
    "        search_results = search_github(full_query, per_page=100, max_pages=10)\n",
    "        \n",
    "        for item in search_results:\n",
    "            owner = item['repository']['owner']['login']\n",
    "            repo = item['repository']['name']\n",
    "            print(f\"Fetching details for {owner}/{repo}\")\n",
    "            repo_details = fetch_repository_details(owner, repo)\n",
    "            if repo_details:\n",
    "                all_repositories.append(repo_details)\n",
    "\n",
    "# Save results to Excel\n",
    "output_file = \"github_search_results_with_metadata.xlsx\"\n",
    "df_output = pd.DataFrame(all_repositories)\n",
    "df_output.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899df038-7ff5-4df2-897c-08a44e11d771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with size range: <500\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml size:<500', 'per_page': 100, 'page': 1}\n",
      "Page 1: 1 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml size:<500', 'per_page': 100, 'page': 2}\n",
      "Searching with size range: 500..1000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml size:500..1000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 39 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml size:500..1000', 'per_page': 100, 'page': 2}\n",
      "Searching with size range: 1000..5000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml size:1000..5000', 'per_page': 100, 'page': 1}\n",
      "Page 1: 8 results\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml size:1000..5000', 'per_page': 100, 'page': 2}\n",
      "Searching with size range: >5000\n",
      "Requesting: https://api.github.com/search/code with params: {'q': 'pumba filename:pumba extension:.yaml size:>5000', 'per_page': 100, 'page': 1}\n",
      "Fetching details for ledgifi/currencies\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        \n",
    "        print(f\"Requesting: {url} with params: {params}\")\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    \n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "    \n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json()\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Function to search GitHub with size filtering\n",
    "def partitioned_search(query, size_ranges, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with size-based partitioning.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The base search query for the GitHub API.\n",
    "        size_ranges (list): List of size ranges (e.g., [\"<500\", \"500..1000\", \">1000\"]).\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch per size range.\n",
    "        \n",
    "    Returns:\n",
    "        list: A combined list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for size_range in size_ranges:\n",
    "        print(f\"Searching with size range: {size_range}\")\n",
    "        size_query = f\"{query} size:{size_range}\"\n",
    "        results = search_github(size_query, per_page=per_page, max_pages=max_pages)\n",
    "        all_results.extend(results)\n",
    "    return all_results\n",
    "\n",
    "# Define the query and size ranges\n",
    "query = \"pumba filename:pumba extension:.yaml\"\n",
    "size_ranges = [\"<500\", \"500..1000\", \"1000..5000\", \">5000\"]\n",
    "\n",
    "# Partitioned search for repositories\n",
    "results = partitioned_search(query, size_ranges, per_page=100, max_pages=10)\n",
    "\n",
    "# Collect metadata\n",
    "repositories = []\n",
    "for item in results:\n",
    "    owner = item['repository']['owner']['login']\n",
    "    repo = item['repository']['name']\n",
    "    print(f\"Fetching details for {owner}/{repo}\")\n",
    "    repo_details = fetch_repository_details(owner, repo)\n",
    "    if repo_details:\n",
    "        repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"partitioned_results.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25fbc8e-5062-478a-be26-db7170486073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: pumba filename:pumba extension:.yaml size:0..49999\n",
      "Page 1: 48 results\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for openebs-archive/e2e-tests\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for chaosiq/chaosiq\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for VadimShtukan/otus_homework\n",
      "Fetching details for chaosiq/chaosiq\n",
      "Fetching details for ledgifi/currencies\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for openebs/dynamic-localpv-provisioner\n",
      "Fetching details for openebs-archive/cstor-operators\n",
      "Fetching details for openebs-archive/jiva-operator\n",
      "Fetching details for abhinavjha126/openebs\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:50000..99999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:100000..149999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:150000..199999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:200000..249999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:250000..299999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:300000..349999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:350000..399999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:400000..449999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:450000..499999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:500000..549999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:550000..599999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:600000..649999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:650000..699999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:700000..749999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:750000..799999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:800000..849999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:850000..899999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:900000..949999\n",
      "Searching with query: pumba filename:pumba extension:.yaml size:950000..999999\n",
      "Data saved to repositories_by_size.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"pumba filename:pumba extension:.yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"repositories_by_size.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d57b5ae5-f695-46ba-95e0-935bf9963dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:0..49999\n",
      "Page 1: 26 results\n",
      "Fetching details for chaostoolkit/chaostoolkit\n",
      "Fetching details for DevLearnOps/tutorials\n",
      "Fetching details for chaostoolkit/chaostoolkit-bundler\n",
      "Fetching details for reliablyhq/cli\n",
      "Fetching details for chaostoolkit/chaostoolkit\n",
      "Fetching details for mcastellin/chaostoolkit-terraform\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-service-fabric\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-google-cloud-platform\n",
      "Fetching details for chaostoolkit/chaostoolkit-lib\n",
      "Fetching details for friki-io/chaostoolkit-kafka\n",
      "Fetching details for chaostoolkit/chaostoolkit-reporting\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-reliably\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-datadog\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-aws\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for reliablyhq/cli\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-istio\n",
      "Fetching details for cdsre/chaostoolkit-consul\n",
      "Fetching details for reliablyhq/actions\n",
      "Fetching details for deceptiveChaos/chaosToken\n",
      "Fetching details for chaostoolkit/chaostoolkit-extension-template\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-honeycomb\n",
      "Fetching details for chaostoolkit/chaostoolkit-addons\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-opentracing\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-humio\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:50000..99999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:100000..149999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:150000..199999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:200000..249999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:250000..299999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:300000..349999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:350000..399999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:400000..449999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:450000..499999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:550000..599999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:600000..649999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:650000..699999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:700000..749999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:750000..799999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:800000..849999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:850000..899999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:900000..949999\n",
      "Searching with query: chaostoolkit filename:pyproject extension:toml size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to chaos_toolkit_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaostoolkit filename:pyproject extension:toml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_toolkit_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d09280-1530-456a-8af8-1555be4a4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaostoolkit filename:actions extension:py size:0..49999\n",
      "Page 1: 37 results\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-aws\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-saltstack\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for friki-io/chaostoolkit-kafka\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-google-cloud-platform\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-honeycomb\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for friki-io/chaostoolkit-kafka\n",
      "Fetching details for jpmorganchase/kallisti-core\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-aws\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-service-fabric\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-aws\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-aws\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-aws\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-google-cloud-platform\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-google-cloud-platform\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-google-cloud-platform\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-reliably\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for chaostoolkit/chaostoolkit-kubernetes\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-google-cloud-platform\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-google-cloud-platform\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-reliably\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-azure\n",
      "Fetching details for chaostoolkit-incubator/chaostoolkit-aws\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:50000..99999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:100000..149999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:150000..199999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:200000..249999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:250000..299999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:300000..349999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:350000..399999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:400000..449999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:450000..499999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:550000..599999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:600000..649999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:650000..699999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:700000..749999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:750000..799999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:800000..849999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:850000..899999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:900000..949999\n",
      "Searching with query: chaostoolkit filename:actions extension:py size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to chaos_toolkit_102.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaostoolkit filename:actions extension:py\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_toolkit_102.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb3ae131-e228-43d3-a1f8-5a72baf78b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaostoolkit filename:dockerfile size:0..49999\n",
      "Page 1: 40 results\n",
      "Fetching details for chaostoolkit/chaostoolkit\n",
      "Fetching details for chaostoolkit-incubator/community-playground\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for litmuschaos/litmus-python\n",
      "Fetching details for caruccio/chaostoolkit-k8s-demo\n",
      "Fetching details for chaostoolkit/chaostoolkit\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for elastest/elastest-instrumentation-manager\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for chaostoolkit/chaostoolkit\n",
      "Fetching details for reliablyhq/cli\n",
      "Fetching details for govau/cga-cf-bosh-cli-docker\n",
      "Fetching details for jecnua/docker-chaos\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for ThomVanL/blog-azure-chaos-engineering\n",
      "Fetching details for ianalderman/chaostoolkit-azure-demo\n",
      "Fetching details for TGPSKI/chaostoolkit-demo\n",
      "Fetching details for vfarcic/chaostoolkit-container-image\n",
      "Fetching details for mayurpawar/chaostoolkit-aws-batch-example\n",
      "Fetching details for MrBW/chaos-monkey-spring-boot-demo\n",
      "Fetching details for jguipi/custom_dockerfile\n",
      "Fetching details for yuvalom/chaos-engineering\n",
      "Fetching details for chaostoolkit-incubator/kubernetes-crd\n",
      "Fetching details for GoogleCloudPlatform/chaos-engineering\n",
      "Fetching details for spyroot/dpdk-pktgen-k8s-cycling-bench\n",
      "Fetching details for chaostoolkit-incubator/kubernetes-job\n",
      "Fetching details for reliablyhq/cli\n",
      "Fetching details for caruccio/chaostoolkit-k8s-demo\n",
      "Fetching details for TGPSKI/chaostoolkit-demo\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for oumkale/test-python\n",
      "Fetching details for oumkale/aws-az-experiment\n",
      "Fetching details for uditgaurav/litmus-py\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Searching with query: chaostoolkit filename:dockerfile size:50000..99999\n",
      "Page 1: 1 results\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: chaostoolkit filename:dockerfile size:100000..149999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:150000..199999\n",
      "Page 1: 1 results\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: chaostoolkit filename:dockerfile size:200000..249999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:250000..299999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:300000..349999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:350000..399999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:400000..449999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaostoolkit filename:dockerfile size:450000..499999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:500000..549999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:550000..599999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:600000..649999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:650000..699999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:700000..749999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:750000..799999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:800000..849999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:850000..899999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaostoolkit filename:dockerfile size:900000..949999\n",
      "Searching with query: chaostoolkit filename:dockerfile size:950000..999999\n",
      "Data saved to chaos_toolkit_103.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaostoolkit filename:dockerfile\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_toolkit_103.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7a87281-0ac7-44a5-bec0-db1077f07f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:toxiproxy extension:json size:0..49999\n",
      "Page 1: 75 results\n",
      "Fetching details for dgiot/dgiot\n",
      "Fetching details for stackrox/stackrox\n",
      "Fetching details for proto-at-block/bitkey\n",
      "Fetching details for emqx/emqx\n",
      "Fetching details for QaBound/toxiproxy-demo\n",
      "Fetching details for anderlli0053/AppArchive\n",
      "Fetching details for fla-sil/PyTorrent\n",
      "Fetching details for yexingzhe/SPJson4C\n",
      "Fetching details for webofmars/chaos-meetup\n",
      "Fetching details for jonatan-ivanov/local-services\n",
      "Fetching details for twingly/twingly-http\n",
      "Fetching details for StuAtKong/horse-chestnut\n",
      "Fetching details for DonKodiyan/chaos-camp\n",
      "Fetching details for AdamRSuth/AZS_Research_Files\n",
      "Fetching details for digital-asset/daml\n",
      "Fetching details for aherrmann/bazel-remote-retry-testing\n",
      "Fetching details for fla-sil/PyTorrent\n",
      "Fetching details for hoilc/scoop-lemon\n",
      "Fetching details for josephwasily/Defragile\n",
      "Fetching details for jmaycon/test-kafka-cluster\n",
      "Fetching details for PlugFox/ws\n",
      "Fetching details for anderlli0053/DEV-tools\n",
      "Fetching details for jxsl13/amqpx\n",
      "Fetching details for fla-sil/PyTorrent\n",
      "Fetching details for open-policy-agent/contrib\n",
      "Fetching details for GoogleCloudPlatform/serverless-production-readiness-java-gcp\n",
      "Fetching details for okibcn/ScoopMaster\n",
      "Fetching details for emqx/pulsar-client-erl\n",
      "Fetching details for yexingzhe/SPJson4C\n",
      "Fetching details for robbizorg/github_msd_project\n",
      "Fetching details for wang-song/scoopMain\n",
      "Fetching details for anatomic/zero-to-live-fp-js-workshop\n",
      "Fetching details for jonatan-ivanov/dn23-boot3-workshop\n",
      "Fetching details for mgazanayi/neo4j-toxiproxy-cluster\n",
      "Fetching details for timvaillancourt/ghostbuster\n",
      "Fetching details for jonatan-ivanov/teahouse\n",
      "Fetching details for yexingzhe/SPJson4C\n",
      "Fetching details for cipherstash/cipherstash-playground\n",
      "Fetching details for aquasecurity/vuln-list-debian\n",
      "Fetching details for fla-sil/PyTorrent\n",
      "Fetching details for dhis2-sre/rabbitmq-client\n",
      "Fetching details for seanghay/starred-repos-collection\n",
      "Fetching details for donquixote-rosinante/circuit-breaker-toxiproxy\n",
      "Fetching details for smoogan/docker-toxiproxy-dotnet\n",
      "Fetching details for proto-at-block/bitkey\n",
      "Fetching details for keynslug/emqx-cradle\n",
      "Fetching details for bamorim/postion\n",
      "Fetching details for Automaat/missing_prometheus_metrics_example\n",
      "Fetching details for faderskd/kafka-playground\n",
      "Fetching details for hstreamdb/hstreamdb_erl\n",
      "Fetching details for dream-x/chaos_engineering_sre\n",
      "Fetching details for epodegrid/kubegenie-dataset\n",
      "Fetching details for aminfa/EncryptedMachineBenchmarks\n",
      "Fetching details for srinaath-sqsft/ntt-poc\n",
      "Fetching details for margostino/supersonic-session\n",
      "Fetching details for virtualtam/pyroscope-profiling-demo\n",
      "Fetching details for dcluna/dotfiles\n",
      "Fetching details for andrew-farries/typeorm-init-race\n",
      "Fetching details for Moonsong-Labs/storage-hub\n",
      "Fetching details for webofmars/chaos-meetup\n",
      "Fetching details for sirupsen/toxiproxy-rails-example\n",
      "Fetching details for seatgeek/amqp-dispatcher\n",
      "Fetching details for KevinCathcart/Orleans.Streams.RabbitMqStreamProvider\n",
      "Fetching details for thephpleague/flysystem\n",
      "Fetching details for eddumelendez/testcontainers-samples\n",
      "Fetching details for cloudamqp/amqproxy\n",
      "Fetching details for fastdgiot/emqx\n",
      "Fetching details for dentarg/ruby-2.4-http-timeout-issue\n",
      "Fetching details for linafx/digital-assets\n",
      "Fetching details for asaikali/spring-ai-zero-to-hero\n",
      "Fetching details for tzolov/ai-observability-demo\n",
      "Fetching details for joshlong-attic/springone-2024-spring-ai\n",
      "Fetching details for jonatan-ivanov/dn24-boot3-workshop\n",
      "Fetching details for khulnasoft-lab/vuln-list-debian\n",
      "Fetching details for eddumelendez/testcontainers-samples\n",
      "Searching with query: filename:toxiproxy extension:json size:50000..99999\n",
      "Page 1: 1 results\n",
      "Fetching details for on-nix/pkgs\n",
      "Searching with query: filename:toxiproxy extension:json size:100000..149999\n",
      "Searching with query: filename:toxiproxy extension:json size:150000..199999\n",
      "Searching with query: filename:toxiproxy extension:json size:200000..249999\n",
      "Searching with query: filename:toxiproxy extension:json size:250000..299999\n",
      "Searching with query: filename:toxiproxy extension:json size:300000..349999\n",
      "Searching with query: filename:toxiproxy extension:json size:350000..399999\n",
      "Searching with query: filename:toxiproxy extension:json size:400000..449999\n",
      "Searching with query: filename:toxiproxy extension:json size:450000..499999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:toxiproxy extension:json size:500000..549999\n",
      "Searching with query: filename:toxiproxy extension:json size:550000..599999\n",
      "Searching with query: filename:toxiproxy extension:json size:600000..649999\n",
      "Searching with query: filename:toxiproxy extension:json size:650000..699999\n",
      "Searching with query: filename:toxiproxy extension:json size:700000..749999\n",
      "Searching with query: filename:toxiproxy extension:json size:750000..799999\n",
      "Searching with query: filename:toxiproxy extension:json size:800000..849999\n",
      "Searching with query: filename:toxiproxy extension:json size:850000..899999\n",
      "Searching with query: filename:toxiproxy extension:json size:900000..949999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:toxiproxy extension:json size:950000..999999\n",
      "Data saved to toxiproxy_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:toxiproxy extension:json\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"toxiproxy_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79eb2276-8df5-422c-a01d-4e03b6c45946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: toxiproxy filename:toxic extension:go size:0..49999\n",
      "Page 1: 20 results\n",
      "Fetching details for IBM/sarama\n",
      "Fetching details for Shopify/toxiproxy\n",
      "Fetching details for Shopify/toxiproxy\n",
      "Fetching details for fakeNetflix/Shopify-repo-toxiproxy\n",
      "Fetching details for form3tech-oss/form3-toxies\n",
      "Fetching details for Shopify/toxiproxy\n",
      "Fetching details for badrootd/udp-crasher\n",
      "Fetching details for fakeNetflix/Shopify-repo-toxiproxy\n",
      "Fetching details for Shopify/toxiproxy\n",
      "Fetching details for Shopify/toxiproxy\n",
      "Fetching details for form3tech-oss/form3-toxies\n",
      "Fetching details for SolaceProducts/pubsubplus-opentelemetry-go-integration\n",
      "Fetching details for fakeNetflix/Shopify-repo-toxiproxy\n",
      "Fetching details for SolaceProducts/pubsubplus-go-client\n",
      "Fetching details for Shopify/toxiproxy\n",
      "Fetching details for deepin-community/golang-github-ibm-sarama\n",
      "Fetching details for zhwei820/kafka-desktop\n",
      "Fetching details for Chris-Hammer/toxiproxy\n",
      "Fetching details for Chris-Hammer/toxiproxy\n",
      "Fetching details for Chris-Hammer/toxiproxy\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:50000..99999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:100000..149999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:150000..199999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:200000..249999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:250000..299999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:300000..349999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:350000..399999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:400000..449999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:450000..499999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:550000..599999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:600000..649999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:650000..699999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:700000..749999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:750000..799999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:800000..849999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:850000..899999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:900000..949999\n",
      "Searching with query: toxiproxy filename:toxic extension:go size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to toxiproxy_102.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"toxiproxy filename:toxic extension:go\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"toxiproxy_102.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9c0bddd-3a84-4b3e-bebf-2ec0a84849b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: toxiproxy filename:makefile size:0..49999\n",
      "Page 1: 44 results\n",
      "Fetching details for testcontainers/testcontainers-go\n",
      "Fetching details for Shopify/toxiproxy\n",
      "Fetching details for openbsd/ports\n",
      "Fetching details for rabbitmq/rabbitmq-dotnet-client\n",
      "Fetching details for grubert65/AuthMock\n",
      "Fetching details for jonatan-ivanov/local-services\n",
      "Fetching details for grubert65/ToxiProxy\n",
      "Fetching details for qbit/improved-succotash\n",
      "Fetching details for freebsd/freebsd-ports\n",
      "Fetching details for jandelgado/rabtap\n",
      "Fetching details for php-amqplib/php-amqplib\n",
      "Fetching details for jmeagher/playground\n",
      "Fetching details for mateusfreira/nun-db\n",
      "Fetching details for jasperla/openbsd-wip\n",
      "Fetching details for jonatan-ivanov/teahouse\n",
      "Fetching details for DragonFlyBSD/DPorts\n",
      "Fetching details for seeker89/chaos-engineering-book\n",
      "Fetching details for richfitz/toxiproxyr\n",
      "Fetching details for fakeNetflix/Shopify-repo-toxiproxy\n",
      "Fetching details for jiahaog/rails-mysql-timeouts\n",
      "Fetching details for virtualtam/pyroscope-profiling-demo\n",
      "Fetching details for aledbf/leader-election-delay\n",
      "Fetching details for Jehops/freebsd-ports-legacy\n",
      "Fetching details for nameko/nameko-sqlalchemy\n",
      "Fetching details for erig0/pkgsrc-wip\n",
      "Fetching details for timvaillancourt/ghostbuster\n",
      "Fetching details for lgcosta/pfsense-ports\n",
      "Fetching details for qbit/improved-succotash\n",
      "Fetching details for jmeagher/playground\n",
      "Fetching details for lavabit/pahoehoe\n",
      "Fetching details for openportio/openport-go\n",
      "Fetching details for pfsense/FreeBSD-ports\n",
      "Fetching details for opnsense/ports\n",
      "Fetching details for freebsd/freebsd-ports-kde\n",
      "Fetching details for ghostbsd/ghostbsd-ports\n",
      "Fetching details for Zirias/zfbsd-ports\n",
      "Fetching details for HardenedBSD/ports\n",
      "Fetching details for yzgyyang/freebsd-ports\n",
      "Fetching details for evadot/freebsd-ports\n",
      "Fetching details for patmaddox/test-subtree\n",
      "Fetching details for truenas/ports\n",
      "Fetching details for Chris-Hammer/toxiproxy\n",
      "Fetching details for grab/blogs\n",
      "Fetching details for leapcode/vpnweb\n",
      "Searching with query: toxiproxy filename:makefile size:50000..99999\n",
      "Page 1: 1 results\n",
      "Fetching details for erig0/pkgsrc-wip\n",
      "Searching with query: toxiproxy filename:makefile size:100000..149999\n",
      "Searching with query: toxiproxy filename:makefile size:150000..199999\n",
      "Searching with query: toxiproxy filename:makefile size:200000..249999\n",
      "Searching with query: toxiproxy filename:makefile size:250000..299999\n",
      "Searching with query: toxiproxy filename:makefile size:300000..349999\n",
      "Searching with query: toxiproxy filename:makefile size:350000..399999\n",
      "Searching with query: toxiproxy filename:makefile size:400000..449999\n",
      "Searching with query: toxiproxy filename:makefile size:450000..499999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: toxiproxy filename:makefile size:500000..549999\n",
      "Searching with query: toxiproxy filename:makefile size:550000..599999\n",
      "Searching with query: toxiproxy filename:makefile size:600000..649999\n",
      "Searching with query: toxiproxy filename:makefile size:650000..699999\n",
      "Searching with query: toxiproxy filename:makefile size:700000..749999\n",
      "Searching with query: toxiproxy filename:makefile size:750000..799999\n",
      "Searching with query: toxiproxy filename:makefile size:800000..849999\n",
      "Searching with query: toxiproxy filename:makefile size:850000..899999\n",
      "Searching with query: toxiproxy filename:makefile size:900000..949999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: toxiproxy filename:makefile size:950000..999999\n",
      "Data saved to toxiproxy_103.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"toxiproxy filename:makefile\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"toxiproxy_103.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773e55b9-e7c1-4113-a924-8e06cc090c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: kube-monkey filename:configmap extension:yaml size:0..49999\n",
      "Page 1: 12 results\n",
      "Fetching details for aws-samples/kubernetes-for-java-developers\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for djkormo/k8s-AKS-primer\n",
      "Fetching details for ajgrande924/insight-project\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for dneff/ts2019-challenges\n",
      "Fetching details for ERNI-Academy/poc-chaos-preformance-resiliency-testing\n",
      "Fetching details for Rabosa616/NetConf2019_MicroservicesGeneratingChaos\n",
      "Fetching details for OctoConsulting/fedhipster-iac\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for shanaka-sky/k8s-java\n",
      "Fetching details for cloudbees-pyang/kubernetes-java\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:50000..99999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:100000..149999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:150000..199999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:200000..249999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:250000..299999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:300000..349999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:350000..399999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:400000..449999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:450000..499999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:500000..549999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:550000..599999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:600000..649999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:650000..699999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:700000..749999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:750000..799999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:800000..849999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:850000..899999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:900000..949999\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml size:950000..999999\n",
      "Data saved to kube_monkey_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"kube-monkey filename:configmap extension:yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"kube_monkey_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5303a6ca-7798-46a8-a882-02c0bf0feb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: kube-monkey filename:values extension:yaml size:0..49999\n",
      "Page 1: 18 results\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for vfarcic/go-demo-5\n",
      "Fetching details for startxfr/helm-repository\n",
      "Fetching details for polybase/polybase-db\n",
      "Fetching details for opencord/kind-voltha\n",
      "Fetching details for qu4lity/tid-kube5g\n",
      "Fetching details for makeomatic/helm-charts\n",
      "Fetching details for mgyong/mbtc\n",
      "Fetching details for startxfr/helm-repository\n",
      "Fetching details for startxfr/helm-repository\n",
      "Fetching details for J0hn-B/eshop\n",
      "Fetching details for mgyong/mbtc\n",
      "Fetching details for makeomatic/helm-charts\n",
      "Fetching details for nuuday/terraform-aws-eks-addons\n",
      "Fetching details for startxfr/helm-repository\n",
      "Fetching details for startxfr/helm-repository\n",
      "Fetching details for startxfr/helm-repository\n",
      "Fetching details for startxfr/helm-repository\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:50000..99999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:100000..149999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:150000..199999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:200000..249999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:250000..299999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:300000..349999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:350000..399999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:400000..449999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:450000..499999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:550000..599999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:600000..649999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:650000..699999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:700000..749999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:750000..799999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:800000..849999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:850000..899999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:900000..949999\n",
      "Searching with query: kube-monkey filename:values extension:yaml size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to kube_monkey_102.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"kube-monkey filename:values extension:yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"kube_monkey_102.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dee9eab9-66a7-44bf-a066-6828b5b99088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: kube-monkey filename:dockerfile size:0..49999\n",
      "Page 1: 18 results\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for essobi/dockerfiles\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for cleett/kube-monkey-demo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Fetching details for haoside666/learnDemo\n",
      "Searching with query: kube-monkey filename:dockerfile size:50000..99999\n",
      "Searching with query: kube-monkey filename:dockerfile size:100000..149999\n",
      "Searching with query: kube-monkey filename:dockerfile size:150000..199999\n",
      "Searching with query: kube-monkey filename:dockerfile size:200000..249999\n",
      "Searching with query: kube-monkey filename:dockerfile size:250000..299999\n",
      "Searching with query: kube-monkey filename:dockerfile size:300000..349999\n",
      "Searching with query: kube-monkey filename:dockerfile size:350000..399999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: kube-monkey filename:dockerfile size:400000..449999\n",
      "Searching with query: kube-monkey filename:dockerfile size:450000..499999\n",
      "Searching with query: kube-monkey filename:dockerfile size:500000..549999\n",
      "Searching with query: kube-monkey filename:dockerfile size:550000..599999\n",
      "Searching with query: kube-monkey filename:dockerfile size:600000..649999\n",
      "Searching with query: kube-monkey filename:dockerfile size:650000..699999\n",
      "Searching with query: kube-monkey filename:dockerfile size:700000..749999\n",
      "Searching with query: kube-monkey filename:dockerfile size:750000..799999\n",
      "Searching with query: kube-monkey filename:dockerfile size:800000..849999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: kube-monkey filename:dockerfile size:850000..899999\n",
      "Searching with query: kube-monkey filename:dockerfile size:900000..949999\n",
      "Searching with query: kube-monkey filename:dockerfile size:950000..999999\n",
      "Data saved to kube_monkey_103.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"kube-monkey filename:dockerfile\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"kube_monkey_103.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "692c52bb-3ba0-44b6-a72f-d02724f8e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaoskube filename:chart extension:yaml size:0..49999\n",
      "Page 1: 57 results\n",
      "Fetching details for linki/chaoskube\n",
      "Fetching details for cloudnativeapp/charts\n",
      "Fetching details for ppc64le/charts\n",
      "Fetching details for shelleg/msa-demo-app\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for giddyinc/charts\n",
      "Fetching details for Yunjuzhen/charts\n",
      "Fetching details for changqian9/helm-chart\n",
      "Fetching details for relliott-harness/caleno-lamp\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for huangyingw/helm-charts\n",
      "Fetching details for AbelGuti/trash\n",
      "Fetching details for ACloudGuru-Resources/charts\n",
      "Fetching details for huangyingw/helm_charts\n",
      "Fetching details for opspresso/helm-charts\n",
      "Fetching details for socialbase/charts\n",
      "Fetching details for Yunjuzhen/charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for wumingmin/helmchart\n",
      "Fetching details for mauilion/kind-chaoskube\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for liudaze/helm-chart\n",
      "Fetching details for lic17/charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for jogrodnik/kafka\n",
      "Fetching details for jogrodnik/kafka\n",
      "Fetching details for mberthos/helm\n",
      "Fetching details for kuberlab-catalog/chaoskube\n",
      "Fetching details for aemneina/testcatalog\n",
      "Fetching details for simpleusd/charts\n",
      "Fetching details for develoopio/charts-no-beta\n",
      "Fetching details for Sureya/airflow_k8s_executor\n",
      "Fetching details for SirObi/airflow_k8s_executor\n",
      "Fetching details for jamartinez-zodia/helm\n",
      "Fetching details for jonyjalfon94/helm-old-charts\n",
      "Fetching details for Arthur-B-DevOps/old_helm_charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:50000..99999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:100000..149999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:150000..199999\n",
      "Page 1: 1 results\n",
      "Fetching details for madeden/blogposts\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:200000..249999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:250000..299999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:300000..349999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:350000..399999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:400000..449999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:450000..499999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:500000..549999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:550000..599999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:600000..649999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:650000..699999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:700000..749999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:750000..799999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:800000..849999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:850000..899999\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:900000..949999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaoskube filename:chart extension:yaml size:950000..999999\n",
      "Data saved to chaos_kube_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaoskube filename:chart extension:yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_kube_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eb90a05-02ac-4aa1-bca6-d400e853b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaoskube filename:values extension:yaml size:0..49999\n",
      "Page 1: 83 results\n",
      "Fetching details for linki/chaoskube\n",
      "Fetching details for GPSDD/api-infrastructure\n",
      "Fetching details for cloudnativeapp/charts\n",
      "Fetching details for cloud-cds/cds-stack\n",
      "Fetching details for hugolhafner/kube-cluster-hcl\n",
      "Fetching details for cloud-cds/cds-stack\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for cloud-cds/cds-stack\n",
      "Fetching details for feliux/containers-lab\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for wescale/gcp-iac-sample\n",
      "Fetching details for shelleg/msa-demo-app\n",
      "Fetching details for mauilion/kind-chaoskube\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for cloud-cds/cds-stack\n",
      "Fetching details for huangyingw/helm_charts\n",
      "Fetching details for wumingmin/helmchart\n",
      "Fetching details for Yunjuzhen/charts\n",
      "Fetching details for giddyinc/charts\n",
      "Fetching details for ppc64le/charts\n",
      "Fetching details for kubesphere/helm-charts\n",
      "Fetching details for ksandermann/helm-chart-haproxy-exporter\n",
      "Fetching details for ksandermann/helm-charts\n",
      "Fetching details for Yunjuzhen/charts\n",
      "Fetching details for socialbase/charts\n",
      "Fetching details for changqian9/helm-chart\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for huangyingw/helm-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for opspresso/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for develoopio/charts-no-beta\n",
      "Fetching details for Sureya/airflow_k8s_executor\n",
      "Fetching details for relliott-harness/caleno-lamp\n",
      "Fetching details for jogrodnik/kafka\n",
      "Fetching details for SirObi/airflow_k8s_executor\n",
      "Fetching details for jogrodnik/kafka\n",
      "Fetching details for jamartinez-zodia/helm\n",
      "Fetching details for jonyjalfon94/helm-old-charts\n",
      "Fetching details for Arthur-B-DevOps/old_helm_charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for ACloudGuru-Resources/charts\n",
      "Fetching details for aemneina/testcatalog\n",
      "Fetching details for simpleusd/charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for shelleg/msa-demo-app\n",
      "Fetching details for liudaze/helm-chart\n",
      "Fetching details for lic17/charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for DoD-Platform-One/gitlab\n",
      "Fetching details for tnir/charts-gitlab\n",
      "Fetching details for tbowman-bah/gitlab\n",
      "Fetching details for eugeneswalker/gitlab-chart\n",
      "Fetching details for rahb3rt/gitlab\n",
      "Fetching details for maartenor/gitlab-helm\n",
      "Fetching details for nuttingd/gitlab-helm-chart\n",
      "Fetching details for paradyme-management/gitlab-chart\n",
      "Fetching details for imaginestack/kubelogic\n",
      "Fetching details for suomitek/suomitek-ai-charts\n",
      "Fetching details for tibosmn/phd-thesis\n",
      "Fetching details for tibosmn/phd-thesis\n",
      "Fetching details for Trigii/DevSecOpsArch\n",
      "Fetching details for BoulderES/gitlab_chart\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for ibuildthecloud/rancher-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for mberthos/helm\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for AbelGuti/trash\n",
      "Fetching details for marcus-sa/helm-charts\n",
      "Fetching details for kuberlab-catalog/chaoskube\n",
      "Searching with query: chaoskube filename:values extension:yaml size:50000..99999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:100000..149999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:150000..199999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:200000..249999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:250000..299999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:300000..349999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:350000..399999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:400000..449999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:450000..499999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaoskube filename:values extension:yaml size:550000..599999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:600000..649999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:650000..699999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:700000..749999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:750000..799999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:800000..849999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:850000..899999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:900000..949999\n",
      "Searching with query: chaoskube filename:values extension:yaml size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to chaos_kube_102.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaoskube filename:values extension:yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_kube_102.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcbf3b1c-2e88-4284-a215-4ca59846eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:chaoskube extension:yaml size:0..49999\n",
      "Page 1: 27 results\n",
      "Fetching details for linki/chaoskube\n",
      "Fetching details for openebs-archive/e2e-tests\n",
      "Fetching details for Theog75/chaosoperator\n",
      "Fetching details for cisco-sso/framework-deploy\n",
      "Fetching details for epodegrid/kubegenie-dataset\n",
      "Fetching details for GPSDD/api-infrastructure\n",
      "Fetching details for hugolhafner/kube-cluster-hcl\n",
      "Fetching details for kubetail-org/grpc-dispatcher-go\n",
      "Fetching details for Theog75/chaosoperator\n",
      "Fetching details for feliux/containers-lab\n",
      "Fetching details for openebs-archive/e2e-tests\n",
      "Fetching details for Theog75/chaosoperator\n",
      "Fetching details for avnes/flux-catalogue\n",
      "Fetching details for cisco-sso/framework-deploy\n",
      "Fetching details for linki/chaoskube\n",
      "Fetching details for Theog75/chaosoperator\n",
      "Fetching details for SII-Codelab-Chaos/Codelab-Chaos-TP\n",
      "Fetching details for kubetail-org/kubetail\n",
      "Fetching details for Theog75/chaosoperator\n",
      "Fetching details for moosh3/gitops-kustomize\n",
      "Fetching details for Theog75/chaosoperator\n",
      "Fetching details for Theog75/chaosoperator\n",
      "Fetching details for Sakib37/DevOps\n",
      "Fetching details for kazukousen/mesh-sandbox\n",
      "Fetching details for neo-technology/marmoset\n",
      "Fetching details for ce-rust/cerk\n",
      "Fetching details for avnes/flux-catalogue\n",
      "Searching with query: filename:chaoskube extension:yaml size:50000..99999\n",
      "Searching with query: filename:chaoskube extension:yaml size:100000..149999\n",
      "Searching with query: filename:chaoskube extension:yaml size:150000..199999\n",
      "Searching with query: filename:chaoskube extension:yaml size:200000..249999\n",
      "Searching with query: filename:chaoskube extension:yaml size:250000..299999\n",
      "Searching with query: filename:chaoskube extension:yaml size:300000..349999\n",
      "Searching with query: filename:chaoskube extension:yaml size:350000..399999\n",
      "Searching with query: filename:chaoskube extension:yaml size:400000..449999\n",
      "Searching with query: filename:chaoskube extension:yaml size:450000..499999\n",
      "Searching with query: filename:chaoskube extension:yaml size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:chaoskube extension:yaml size:550000..599999\n",
      "Searching with query: filename:chaoskube extension:yaml size:600000..649999\n",
      "Searching with query: filename:chaoskube extension:yaml size:650000..699999\n",
      "Searching with query: filename:chaoskube extension:yaml size:700000..749999\n",
      "Searching with query: filename:chaoskube extension:yaml size:750000..799999\n",
      "Searching with query: filename:chaoskube extension:yaml size:800000..849999\n",
      "Searching with query: filename:chaoskube extension:yaml size:850000..899999\n",
      "Searching with query: filename:chaoskube extension:yaml size:900000..949999\n",
      "Searching with query: filename:chaoskube extension:yaml size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to chaos_kube_103.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:chaoskube extension:yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_kube_103.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63054157-f000-4858-900c-6819da6faccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaosblade filename:makefile size:0..49999\n",
      "Page 1: 19 results\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade-exec-jvm\n",
      "Fetching details for 120742056/chaosblade-exec-test\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-exec-os\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cplus\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade-exec-middleware\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for 120742056/chaosblade-exec-test\n",
      "Fetching details for chaosblade-io/chaosblade-exec-docker\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-box\n",
      "Fetching details for chaosblade-io/chaosblade-box-agent\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cloud\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for caofujiang/chaosblade-exec-os\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Searching with query: chaosblade filename:makefile size:50000..99999\n",
      "Searching with query: chaosblade filename:makefile size:100000..149999\n",
      "Searching with query: chaosblade filename:makefile size:150000..199999\n",
      "Searching with query: chaosblade filename:makefile size:200000..249999\n",
      "Searching with query: chaosblade filename:makefile size:250000..299999\n",
      "Searching with query: chaosblade filename:makefile size:300000..349999\n",
      "Searching with query: chaosblade filename:makefile size:350000..399999\n",
      "Searching with query: chaosblade filename:makefile size:400000..449999\n",
      "Searching with query: chaosblade filename:makefile size:450000..499999\n",
      "Searching with query: chaosblade filename:makefile size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaosblade filename:makefile size:550000..599999\n",
      "Searching with query: chaosblade filename:makefile size:600000..649999\n",
      "Searching with query: chaosblade filename:makefile size:650000..699999\n",
      "Searching with query: chaosblade filename:makefile size:700000..749999\n",
      "Searching with query: chaosblade filename:makefile size:750000..799999\n",
      "Searching with query: chaosblade filename:makefile size:800000..849999\n",
      "Searching with query: chaosblade filename:makefile size:850000..899999\n",
      "Searching with query: chaosblade filename:makefile size:900000..949999\n",
      "Searching with query: chaosblade filename:makefile size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to chaosblade_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosblade filename:makefile\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaosblade_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10f3339d-a7cd-452e-b6b3-bc4ba18b368f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaosblade filename:executor extension:go size:0..49999\n",
      "Page 1: 45 results\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-exec-os\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade-exec-docker\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-exec-docker\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cloud\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade-exec-cri\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade-exec-docker\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for chaosblade-io/chaosblade-exec-middleware\n",
      "Fetching details for caofujiang/chaosblade-exec-os\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for caofujiang/chaosblade-exec-cri\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Fetching details for gridgentoo/chaosblade\n",
      "Searching with query: chaosblade filename:executor extension:go size:50000..99999\n",
      "Searching with query: chaosblade filename:executor extension:go size:100000..149999\n",
      "Searching with query: chaosblade filename:executor extension:go size:150000..199999\n",
      "Searching with query: chaosblade filename:executor extension:go size:200000..249999\n",
      "Searching with query: chaosblade filename:executor extension:go size:250000..299999\n",
      "Searching with query: chaosblade filename:executor extension:go size:300000..349999\n",
      "Searching with query: chaosblade filename:executor extension:go size:350000..399999\n",
      "Searching with query: chaosblade filename:executor extension:go size:400000..449999\n",
      "Searching with query: chaosblade filename:executor extension:go size:450000..499999\n",
      "Searching with query: chaosblade filename:executor extension:go size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: chaosblade filename:executor extension:go size:550000..599999\n",
      "Searching with query: chaosblade filename:executor extension:go size:600000..649999\n",
      "Searching with query: chaosblade filename:executor extension:go size:650000..699999\n",
      "Searching with query: chaosblade filename:executor extension:go size:700000..749999\n",
      "Searching with query: chaosblade filename:executor extension:go size:750000..799999\n",
      "Searching with query: chaosblade filename:executor extension:go size:800000..849999\n",
      "Searching with query: chaosblade filename:executor extension:go size:850000..899999\n",
      "Searching with query: chaosblade filename:executor extension:go size:900000..949999\n",
      "Searching with query: chaosblade filename:executor extension:go size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to chaosblade_102.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosblade filename:executor extension:go\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaosblade_102.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6937f5a7-3b1d-490e-b8a0-e3da25ae0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:chaosblade extension:yaml size:0..49999\n",
      "Page 1: 41 results\n",
      "Fetching details for opsre/awesome-ops\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for KindlingProject/space-capsule\n",
      "Fetching details for Huile3344/install-app\n",
      "Fetching details for chaosblade-io/chaosblade-box\n",
      "Fetching details for metio/kube-custom-resources-rs\n",
      "Fetching details for huanghj78/conan\n",
      "Fetching details for mvalarh/prod\n",
      "Fetching details for k8s-operatorhub/community-operators\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for KindlingProject/space-capsule\n",
      "Fetching details for chaosblade-io/chaosblade-box\n",
      "Fetching details for minkimcello/landscape3\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for Huile3344/install-app\n",
      "Fetching details for chaosblade-io/chaosblade-box\n",
      "Fetching details for mvalarh/prod\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for KindlingProject/space-capsule\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for kubevious/mock-data\n",
      "Fetching details for 120742056/chaosblade-exec-test\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for huanghj78/conan\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for k8s-operatorhub/community-operators\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for mvalarh/prod\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Fetching details for caofujiang/chaosblade-operator\n",
      "Searching with query: filename:chaosblade extension:yaml size:50000..99999\n",
      "Page 1: 6 results\n",
      "Fetching details for KindlingProject/space-capsule\n",
      "Fetching details for huanghj78/conan\n",
      "Fetching details for KindlingProject/space-capsule\n",
      "Fetching details for huanghj78/conan\n",
      "Fetching details for KindlingProject/space-capsule\n",
      "Fetching details for huanghj78/conan\n",
      "Searching with query: filename:chaosblade extension:yaml size:100000..149999\n",
      "Searching with query: filename:chaosblade extension:yaml size:150000..199999\n",
      "Searching with query: filename:chaosblade extension:yaml size:200000..249999\n",
      "Searching with query: filename:chaosblade extension:yaml size:250000..299999\n",
      "Page 1: 1 results\n",
      "Fetching details for KindlingProject/space-capsule\n",
      "Searching with query: filename:chaosblade extension:yaml size:300000..349999\n",
      "Searching with query: filename:chaosblade extension:yaml size:350000..399999\n",
      "Searching with query: filename:chaosblade extension:yaml size:400000..449999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:chaosblade extension:yaml size:450000..499999\n",
      "Searching with query: filename:chaosblade extension:yaml size:500000..549999\n",
      "Searching with query: filename:chaosblade extension:yaml size:550000..599999\n",
      "Searching with query: filename:chaosblade extension:yaml size:600000..649999\n",
      "Searching with query: filename:chaosblade extension:yaml size:650000..699999\n",
      "Searching with query: filename:chaosblade extension:yaml size:700000..749999\n",
      "Searching with query: filename:chaosblade extension:yaml size:750000..799999\n",
      "Searching with query: filename:chaosblade extension:yaml size:800000..849999\n",
      "Searching with query: filename:chaosblade extension:yaml size:850000..899999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:chaosblade extension:yaml size:900000..949999\n",
      "Searching with query: filename:chaosblade extension:yaml size:950000..999999\n",
      "Data saved to chaosblade_103.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:chaosblade extension:yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaosblade_103.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043501e-f913-4977-8bcd-bd1e25fb7a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:deploy_chaoskube extension:sh size:0..49999\n",
      "Page 1: 15 results\n",
      "Fetching details for kubesphere/helm-charts\n",
      "Fetching details for tnir/charts-gitlab\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:deploy_chaoskube extension:sh\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaosblade_104.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1a12c64-32e3-4884-9765-634f81a05ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:pumba extension:yaml size:0..49999\n",
      "Page 1: 48 results\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for openebs-archive/e2e-tests\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for chaosiq/chaosiq\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for VadimShtukan/otus_homework\n",
      "Fetching details for ledgifi/currencies\n",
      "Fetching details for chaosiq/chaosiq\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for ppgia-unifor/kubow\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for openebs/dynamic-localpv-provisioner\n",
      "Fetching details for openebs-archive/cstor-operators\n",
      "Fetching details for openebs-archive/jiva-operator\n",
      "Fetching details for abhinavjha126/openebs\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for ksatchit/randomData\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Searching with query: filename:pumba extension:yaml size:50000..99999\n",
      "Searching with query: filename:pumba extension:yaml size:100000..149999\n",
      "Searching with query: filename:pumba extension:yaml size:150000..199999\n",
      "Searching with query: filename:pumba extension:yaml size:200000..249999\n",
      "Searching with query: filename:pumba extension:yaml size:250000..299999\n",
      "Searching with query: filename:pumba extension:yaml size:300000..349999\n",
      "Searching with query: filename:pumba extension:yaml size:350000..399999\n",
      "Searching with query: filename:pumba extension:yaml size:400000..449999\n",
      "Searching with query: filename:pumba extension:yaml size:450000..499999\n",
      "Searching with query: filename:pumba extension:yaml size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:pumba extension:yaml size:550000..599999\n",
      "Searching with query: filename:pumba extension:yaml size:600000..649999\n",
      "Searching with query: filename:pumba extension:yaml size:650000..699999\n",
      "Searching with query: filename:pumba extension:yaml size:700000..749999\n",
      "Searching with query: filename:pumba extension:yaml size:750000..799999\n",
      "Searching with query: filename:pumba extension:yaml size:800000..849999\n",
      "Searching with query: filename:pumba extension:yaml size:850000..899999\n",
      "Searching with query: filename:pumba extension:yaml size:900000..949999\n",
      "Searching with query: filename:pumba extension:yaml size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to pumba_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:pumba extension:yaml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"pumba_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d591fac-1de1-481e-ab0b-e9ce59f889a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: pumba filename:makefile size:0..49999\n",
      "Page 1: 75 results\n",
      "Fetching details for alexei-led/pumba\n",
      "Fetching details for leviska/tcp_over_udp\n",
      "Fetching details for cyrinux/portscanner\n",
      "Fetching details for altaris/seat-navy-issue\n",
      "Fetching details for vitorenesduarte/exactly-once\n",
      "Fetching details for kabard/pumba\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for sacherjj/CasperLabs\n",
      "Fetching details for altaris/pumba\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for Crazybus/pumbaku\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for eerimoq/pumbaa\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Fetching details for OS-Q/Y511\n",
      "Searching with query: pumba filename:makefile size:50000..99999\n",
      "Searching with query: pumba filename:makefile size:100000..149999\n",
      "Searching with query: pumba filename:makefile size:150000..199999\n",
      "Searching with query: pumba filename:makefile size:200000..249999\n",
      "Searching with query: pumba filename:makefile size:250000..299999\n",
      "Searching with query: pumba filename:makefile size:300000..349999\n",
      "Searching with query: pumba filename:makefile size:350000..399999\n",
      "Searching with query: pumba filename:makefile size:400000..449999\n",
      "Searching with query: pumba filename:makefile size:450000..499999\n",
      "Searching with query: pumba filename:makefile size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: pumba filename:makefile size:550000..599999\n",
      "Searching with query: pumba filename:makefile size:600000..649999\n",
      "Searching with query: pumba filename:makefile size:650000..699999\n",
      "Searching with query: pumba filename:makefile size:700000..749999\n",
      "Searching with query: pumba filename:makefile size:750000..799999\n",
      "Searching with query: pumba filename:makefile size:800000..849999\n",
      "Searching with query: pumba filename:makefile size:850000..899999\n",
      "Searching with query: pumba filename:makefile size:900000..949999\n",
      "Searching with query: pumba filename:makefile size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to pumba_102.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"pumba filename:makefile\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"pumba_102.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bde91bcd-2690-4ff5-afde-d96656574f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:powerfulseal extension:yml size:0..49999\n",
      "Page 1: 5 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for seeker89/chaos-engineering-book\n",
      "Fetching details for litmuschaos/litmus-ansible\n",
      "Fetching details for uditgaurav/test-litmus-ansible\n",
      "Fetching details for uditgaurav/litmus-run-e2e\n",
      "Searching with query: filename:powerfulseal extension:yml size:50000..99999\n",
      "Searching with query: filename:powerfulseal extension:yml size:100000..149999\n",
      "Searching with query: filename:powerfulseal extension:yml size:150000..199999\n",
      "Searching with query: filename:powerfulseal extension:yml size:200000..249999\n",
      "Searching with query: filename:powerfulseal extension:yml size:250000..299999\n",
      "Searching with query: filename:powerfulseal extension:yml size:300000..349999\n",
      "Searching with query: filename:powerfulseal extension:yml size:350000..399999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:powerfulseal extension:yml size:400000..449999\n",
      "Searching with query: filename:powerfulseal extension:yml size:450000..499999\n",
      "Searching with query: filename:powerfulseal extension:yml size:500000..549999\n",
      "Searching with query: filename:powerfulseal extension:yml size:550000..599999\n",
      "Searching with query: filename:powerfulseal extension:yml size:600000..649999\n",
      "Searching with query: filename:powerfulseal extension:yml size:650000..699999\n",
      "Searching with query: filename:powerfulseal extension:yml size:700000..749999\n",
      "Searching with query: filename:powerfulseal extension:yml size:750000..799999\n",
      "Searching with query: filename:powerfulseal extension:yml size:800000..849999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:powerfulseal extension:yml size:850000..899999\n",
      "Searching with query: filename:powerfulseal extension:yml size:900000..949999\n",
      "Searching with query: filename:powerfulseal extension:yml size:950000..999999\n",
      "Data saved to powerfulseal_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:powerfulseal extension:yml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"powerfulseal_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55dea0dc-af0d-4510-bd97-3e0b2ae68240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:powerfulseal extension:yml size:0..49999\n",
      "Page 1: 5 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for seeker89/chaos-engineering-book\n",
      "Fetching details for litmuschaos/litmus-ansible\n",
      "Fetching details for uditgaurav/test-litmus-ansible\n",
      "Fetching details for uditgaurav/litmus-run-e2e\n",
      "Searching with query: filename:powerfulseal extension:yml size:50000..99999\n",
      "Searching with query: filename:powerfulseal extension:yml size:100000..149999\n",
      "Searching with query: filename:powerfulseal extension:yml size:150000..199999\n",
      "Searching with query: filename:powerfulseal extension:yml size:200000..249999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:powerfulseal extension:yml size:250000..299999\n",
      "Searching with query: filename:powerfulseal extension:yml size:300000..349999\n",
      "Searching with query: filename:powerfulseal extension:yml size:350000..399999\n",
      "Searching with query: filename:powerfulseal extension:yml size:400000..449999\n",
      "Searching with query: filename:powerfulseal extension:yml size:450000..499999\n",
      "Searching with query: filename:powerfulseal extension:yml size:500000..549999\n",
      "Searching with query: filename:powerfulseal extension:yml size:550000..599999\n",
      "Searching with query: filename:powerfulseal extension:yml size:600000..649999\n",
      "Searching with query: filename:powerfulseal extension:yml size:650000..699999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: filename:powerfulseal extension:yml size:700000..749999\n",
      "Searching with query: filename:powerfulseal extension:yml size:750000..799999\n",
      "Searching with query: filename:powerfulseal extension:yml size:800000..849999\n",
      "Searching with query: filename:powerfulseal extension:yml size:850000..899999\n",
      "Searching with query: filename:powerfulseal extension:yml size:900000..949999\n",
      "Searching with query: filename:powerfulseal extension:yml size:950000..999999\n",
      "Data saved to powerfulseal_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:powerfulseal extension:yml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"powerfulseal_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcd2b41d-39cf-419c-9f62-5bfdc2912818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: litmuschaos filename:config extension:yml size:0..49999\n",
      "Page 1: 20 results\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for oumkale/e2e-test.github.io\n",
      "Fetching details for Killpit/fault-tolerance-kubernetes\n",
      "Fetching details for Jonsy13/test-litmus-helm\n",
      "Fetching details for uditgaurav/litmuschaos-ci\n",
      "Fetching details for Killpit/fault-tolerance-kubernetes\n",
      "Fetching details for ispeakc0de/pages\n",
      "Fetching details for Jonsy13/Pipeline-Updates-Test\n",
      "Fetching details for ksatchit/chaos-helm-1\n",
      "Fetching details for amityt/api-docs\n",
      "Fetching details for uditgaurav/test-litmus-helm\n",
      "Fetching details for Killpit/fault-tolerance-kubernetes\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Killpit/fault-tolerance-kubernetes\n",
      "Fetching details for Killpit/fault-tolerance-kubernetes\n",
      "Fetching details for Killpit/fault-tolerance-kubernetes\n",
      "Fetching details for Jonsy13/chaos-frontend-builder\n",
      "Fetching details for Jonsy13/test-build-size\n",
      "Fetching details for Jonsy13/parallel-matrix-build\n",
      "Searching with query: litmuschaos filename:config extension:yml size:50000..99999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: litmuschaos filename:config extension:yml size:100000..149999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:150000..199999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:200000..249999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:250000..299999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:300000..349999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:350000..399999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:400000..449999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:450000..499999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: litmuschaos filename:config extension:yml size:550000..599999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:600000..649999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:650000..699999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:700000..749999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:750000..799999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:800000..849999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:850000..899999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:900000..949999\n",
      "Searching with query: litmuschaos filename:config extension:yml size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to litmuschaos_101.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"litmuschaos filename:config extension:yml\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"litmuschaos_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672c1df4-674c-4295-80f5-79f8b1f7301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: litmuschaos filename:dockerfile size:0..49999\n",
      "Page 1: 100 results\n",
      "Page 2: 36 results\n",
      "Fetching details for google/oss-fuzz\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for cncf/devstats-docker-images\n",
      "Fetching details for umamukkara/test-litmus-go\n",
      "Fetching details for uditgaurav/kubernetes-chaos\n",
      "Fetching details for litmuschaos/charthub.litmuschaos.io\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for TelkomIndonesia/litmus-node-fault\n",
      "Fetching details for cncf/devstats-docker-images\n",
      "Fetching details for ispeakc0de/cn-litmus-go\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for Adarshkumar14/litmus-upgrade-agent\n",
      "Fetching details for uditgaurav/azure-project\n",
      "Fetching details for cncf/devstats-docker-images\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for ispeakc0de/scheduler-gitaction-test\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for spyroot/dpdk-pktgen-k8s-cycling-bench\n",
      "Fetching details for litmuschaos/chaos-exporter\n",
      "Fetching details for Jonsy13/notes\n",
      "Fetching details for litmuschaos/litmus-go\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for Jonsy13/command-chaos-test\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for oumkale/notes\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for uditgaurav/k8s-actions\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-chaos-charts\n",
      "Fetching details for litmuschaos/chaos-runner\n",
      "Fetching details for oumkale/test-python\n",
      "Fetching details for litmuschaos/admission-controllers\n",
      "Fetching details for litmuschaos/chaos-ci-lib\n",
      "Fetching details for uditgaurav/new-chaos-runner\n",
      "Fetching details for uditgaurav/test-litmus-go\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for umamukkara/test-litmus-go\n",
      "Fetching details for uditgaurav/test-litmus-ansible\n",
      "Fetching details for uditgaurav/raas\n",
      "Fetching details for uditgaurav/litmus-run-e2e\n",
      "Fetching details for Jonsy13/chaos-frontend-builder\n",
      "Fetching details for litmuschaos/chaos-scheduler\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for litmuschaos/litmus-python\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for litmuschaos/litmus-ansible\n",
      "Fetching details for cncf/devstats-docker-images\n",
      "Fetching details for litmuschaos/github-chaos-actions\n",
      "Fetching details for uditgaurav/litmus-py\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for uditgaurav/run-command-chaos\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for uditgaurav/logs\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for Jonsy13/litmus-test\n",
      "Fetching details for litmuschaos/chaos-operator\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for uditgaurav/logs\n",
      "Fetching details for oumkale/pre-defined-workflow\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for ajeshbaby/my-lit-test\n",
      "Fetching details for uditgaurav/raas\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for oumkale/pre-defined-workflow\n",
      "Fetching details for Jonsy13/test-build-size\n",
      "Fetching details for uditgaurav/github-test\n",
      "Fetching details for uditgaurav/logs\n",
      "Fetching details for litmuschaos/litmus\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for uditgaurav/logs\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for litmuschaos/test-tools\n",
      "Fetching details for limberguti/Actividad3_Grupo1\n",
      "Fetching details for actions-marketplace-validations/uditgaurav_kubernetes-chaos\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Jonsy13/litmus-test\n",
      "Fetching details for Jonsy13/e2e-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Jonsy13/parallel-matrix-build\n",
      "Fetching details for actions-marketplace-validations/uditgaurav_k8s-actions\n",
      "Fetching details for oumkale/aws-az-experiment\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for mayadata-io/chaos-ci-lib\n",
      "Fetching details for uditgaurav/test-chaos-ci-lib\n",
      "Fetching details for uditgaurav/test-chaos-runner\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-litmus\n",
      "Fetching details for Jonsy13/parallel-matrix-build\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Jonsy13/litmus-test\n",
      "Fetching details for Jonsy13/parallel-matrix-build\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for actions-marketplace-validations/litmuschaos_github-chaos-actions\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for uditgaurav/logs\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Jonsy13/litmus-test\n",
      "Fetching details for Jonsy13/parallel-matrix-build\n",
      "Fetching details for Jonsy13/parallel-matrix-build\n",
      "Fetching details for uditgaurav/logs\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Jonsy13/litmus-test\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for Jonsy13/litmus-test\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for uditgaurav/logs\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Fetching details for uditgaurav/test-test-tools\n",
      "Fetching details for Jonsy13/test-e2e-build\n",
      "Searching with query: litmuschaos filename:dockerfile size:50000..99999\n",
      "Searching with query: litmuschaos filename:dockerfile size:100000..149999\n",
      "Searching with query: litmuschaos filename:dockerfile size:150000..199999\n",
      "Searching with query: litmuschaos filename:dockerfile size:200000..249999\n",
      "Searching with query: litmuschaos filename:dockerfile size:250000..299999\n",
      "Searching with query: litmuschaos filename:dockerfile size:300000..349999\n",
      "Searching with query: litmuschaos filename:dockerfile size:350000..399999\n",
      "Searching with query: litmuschaos filename:dockerfile size:400000..449999\n",
      "Searching with query: litmuschaos filename:dockerfile size:450000..499999\n",
      "Searching with query: litmuschaos filename:dockerfile size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: litmuschaos filename:dockerfile size:550000..599999\n",
      "Searching with query: litmuschaos filename:dockerfile size:600000..649999\n",
      "Searching with query: litmuschaos filename:dockerfile size:650000..699999\n",
      "Searching with query: litmuschaos filename:dockerfile size:700000..749999\n",
      "Searching with query: litmuschaos filename:dockerfile size:750000..799999\n",
      "Searching with query: litmuschaos filename:dockerfile size:800000..849999\n",
      "Searching with query: litmuschaos filename:dockerfile size:850000..899999\n",
      "Searching with query: litmuschaos filename:dockerfile size:900000..949999\n",
      "Searching with query: litmuschaos filename:dockerfile size:950000..999999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Data saved to litmuschaos_102.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"litmuschaos filename:dockerfile\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"litmuschaos_102.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5378a-0e19-454b-b938-2124260dbbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaosmonkey filename:makefile size:0..49999\n",
      "Page 1: 14 results\n",
      "Fetching details for Netflix/chaosmonkey\n",
      "Fetching details for STFleming/StitchUp\n",
      "Fetching details for mlafeldt/chaosmonkey\n",
      "Fetching details for fossabot/webserver\n",
      "Fetching details for shinesolutions/aem-test-suite\n",
      "Fetching details for Lukas-De-Angelis-Riva/Amazon-Books-Analyzer\n",
      "Fetching details for fossabot/webserver\n",
      "Fetching details for massix/chaos-monkey\n",
      "Fetching details for shinesolutions/inspec-aem-aws\n",
      "Fetching details for mlafeldt/homebrew-formulas\n",
      "Fetching details for fossabot/webserver\n",
      "Fetching details for junaidk/podchaosmonkey\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for 2lambda123/chaosmonkey\n",
      "Searching with query: chaosmonkey filename:makefile size:50000..99999\n",
      "Searching with query: chaosmonkey filename:makefile size:100000..149999\n",
      "Searching with query: chaosmonkey filename:makefile size:150000..199999\n",
      "Searching with query: chaosmonkey filename:makefile size:200000..249999\n",
      "Searching with query: chaosmonkey filename:makefile size:250000..299999\n",
      "Searching with query: chaosmonkey filename:makefile size:300000..349999\n",
      "Searching with query: chaosmonkey filename:makefile size:350000..399999\n",
      "Searching with query: chaosmonkey filename:makefile size:400000..449999\n",
      "Searching with query: chaosmonkey filename:makefile size:450000..499999\n",
      "Searching with query: chaosmonkey filename:makefile size:500000..549999\n",
      "Rate limit exceeded. Waiting for reset...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosmonkey filename:makefile\"  # Replace with your base query\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaosmonkey_101.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c64945c-6f8b-44ac-bb2c-05ea56ba79ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: powerfulseal filename:dockerfile size:0..49999\n",
      "Page 1: 9 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for essobi/dockerfiles\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for dgzlopes/powerfulseal-agent\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: powerfulseal filename:dockerfile size:50000..99999\n",
      "Page 1: 1 results\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: powerfulseal filename:dockerfile size:100000..149999\n",
      "Searching with query: powerfulseal filename:dockerfile size:150000..199999\n",
      "Page 1: 1 results\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: powerfulseal filename:dockerfile size:200000..249999\n",
      "Searching with query: powerfulseal filename:dockerfile size:250000..299999\n",
      "Searching with query: powerfulseal filename:dockerfile size:300000..349999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: powerfulseal filename:dockerfile size:350000..399999\n",
      "Searching with query: powerfulseal filename:dockerfile size:400000..449999\n",
      "Searching with query: powerfulseal filename:dockerfile size:450000..499999\n",
      "Searching with query: powerfulseal filename:dockerfile size:500000..549999\n",
      "Searching with query: powerfulseal filename:dockerfile size:550000..599999\n",
      "Searching with query: powerfulseal filename:dockerfile size:600000..649999\n",
      "Searching with query: powerfulseal filename:dockerfile size:650000..699999\n",
      "Searching with query: powerfulseal filename:dockerfile size:700000..749999\n",
      "Searching with query: powerfulseal filename:dockerfile size:750000..799999\n",
      "Rate limit exceeded. Waiting for reset...\n",
      "Searching with query: powerfulseal filename:dockerfile size:800000..849999\n",
      "Searching with query: powerfulseal filename:dockerfile size:850000..899999\n",
      "Searching with query: powerfulseal filename:dockerfile size:900000..949999\n",
      "Searching with query: powerfulseal filename:dockerfile size:950000..999999\n",
      "Data saved to chaos.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"  # For fetching topics\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    \"\"\"\n",
    "    Searches GitHub Code Search API with pagination.\n",
    "    Args:\n",
    "        query (str): The search query for the GitHub API.\n",
    "        per_page (int): Number of results per page (max 100).\n",
    "        max_pages (int): Number of pages to fetch.\n",
    "    Returns:\n",
    "        list: A list of search result items.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:  # Handle rate limiting\n",
    "            print(\"Rate limit exceeded. Waiting for reset...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break  # No more results\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    \"\"\"\n",
    "    Fetches detailed repository metadata from the GitHub API.\n",
    "    Args:\n",
    "        owner (str): Owner (user/org) of the repository.\n",
    "        repo (str): Name of the repository.\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    # Fetch languages\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    # Fetch contributors count\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    # Fetch commits\n",
    "    commits_url = f\"{url}/commits\"\n",
    "    commits_response = requests.get(commits_url, headers=HEADERS)\n",
    "    commits = commits_response.json() if commits_response.status_code == 200 else []\n",
    "    last_commit = commits[0]['commit']['committer']['date'] if commits else None\n",
    "    first_commit = commits[-1]['commit']['committer']['date'] if commits else None\n",
    "    total_commits = len(commits) if commits else 0\n",
    "\n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"lastCommit\": last_commit,\n",
    "        \"firstCommit\": first_commit,\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    \"\"\"\n",
    "    Creates size ranges for partitioning the search query.\n",
    "    Args:\n",
    "        min_size (int): Minimum file size in bytes.\n",
    "        max_size (int): Maximum file size in bytes.\n",
    "        step (int): Step size for partitioning.\n",
    "    Returns:\n",
    "        list: List of (min_size, max_size) tuples.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0  # Minimum file size in bytes\n",
    "max_size = 1000000  # Maximum file size in bytes (adjust as needed)\n",
    "size_step = 50000  # Partition size range (e.g., 50KB)\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"powerfulseal filename:dockerfile\"  # Replace with your base query\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71241774-c660-41ef-81aa-9e1a42ab71b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:chaosmonkey extension:go size:0..49999\n",
      "Page 1: 100 results\n",
      "Page 2: 100 results\n",
      "Page 3: 49 results\n",
      "Fetching details for Netflix/chaosmonkey\n",
      "Fetching details for adrianco/spigo\n",
      "Fetching details for Kostov6/Chaosmonkey\n",
      "Fetching details for mlafeldt/chaosmonkey\n",
      "Fetching details for Kostov6/Chaosmonkey\n",
      "Fetching details for chenhy97/sysu_VWR\n",
      "Fetching details for Kostov6/Chaosmonkey\n",
      "Fetching details for Netflix/chaosmonkey\n",
      "Fetching details for eznd-otus-msa/hw3\n",
      "Fetching details for kubernetes/kubernetes\n",
      "Fetching details for CentaurusInfra/arktos\n",
      "Fetching details for PelionIoT/edge-kubelet\n",
      "Fetching details for kubernetes/kubernetes\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:294\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m reraise(\u001b[38;5;28mtype\u001b[39m(error), error, _stacktrace)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:294\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m repo \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepository\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching details for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mowner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m repo_details \u001b[38;5;241m=\u001b[39m fetch_repository_details(owner, repo)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_details:\n\u001b[0;32m    141\u001b[0m     repositories\u001b[38;5;241m.\u001b[39mappend(repo_details)\n",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m, in \u001b[0;36mfetch_repository_details\u001b[1;34m(owner, repo)\u001b[0m\n\u001b[0;32m     82\u001b[0m contributors_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(contributors_url, headers\u001b[38;5;241m=\u001b[39mHEADERS)\n\u001b[0;32m     83\u001b[0m contributors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(contributors_response\u001b[38;5;241m.\u001b[39mjson()) \u001b[38;5;28;01mif\u001b[39;00m contributors_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 85\u001b[0m total_commits \u001b[38;5;241m=\u001b[39m fetch_total_commits(owner, repo)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepositoryID\u001b[39m\u001b[38;5;124m\"\u001b[39m: repo_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepositoryName\u001b[39m\u001b[38;5;124m\"\u001b[39m: repo_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumberOfCommits\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_commits\n\u001b[0;32m    106\u001b[0m }\n",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m, in \u001b[0;36mfetch_total_commits\u001b[1;34m(owner, repo)\u001b[0m\n\u001b[0;32m     43\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     46\u001b[0m         commits_url, \n\u001b[0;32m     47\u001b[0m         headers\u001b[38;5;241m=\u001b[39mHEADERS, \n\u001b[0;32m     48\u001b[0m         params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_page\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: page}\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m     51\u001b[0m         reset_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-RateLimit-Reset\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m60\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:chaosmonkey extension:go\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_monkey_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf280e-dde8-46ed-82d9-1015f39bde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosmonkey filename:makefile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_monkey_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fae50-43ef-4f75-936c-d325191d53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"toxiproxy filename:toxic extension:go\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"toxiproxy_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab7f5c-93e5-4439-a4b5-cdfe94332f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"toxiproxy filename:Makefile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"toxiproxy_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d74740-8ab4-4965-b600-11e693f00de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:toxiproxy extension:json\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"toxiproxy_103\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4b5c2-9c41-4c9f-adb2-57142552c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaos-mesh filename:config extension:yml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_mesh_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0219c8-d2ec-4acc-8fae-260ea99cc074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaos-mesh filename:install extension:sh\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_mesh_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8bb405-c761-4641-a364-930d35f944ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaos-mesh filename:rbac extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_mesh_103\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b617286-676e-4814-a059-b3b11ed3a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaos-mesh filename:values extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_mesh_104\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a101b-5575-4b3f-88fb-a2f51e1301d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaos-mesh filename:chaos extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_mesh_105\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5174b7-83d4-4fac-a36c-197cb22861d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:chaos-mesh extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_mesh_106\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb56ce7-0599-4558-b50c-d6224c1b50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosblade filename:Dockerfile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_blade_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d503d2-022a-4aa4-8975-8b3e5e7c69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosblade filename:Makefile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_blade_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4944f-1642-4f47-bfbb-91ec070e58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:chaosblade extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_blade_103\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd71c43-6fc0-4775-9477-62e081e12d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosblade filename:executor extension:go\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_blade_104\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8058d-597c-4af0-a520-668a4db79963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"litmuschaos filename:config extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"litmus_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b9e8d-4f28-4fa2-86d2-2f80f20f928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"litmuschaos filename:dockerfile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"litmus_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b79a4c-e9c6-4adf-a933-a9cf8826da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"litmuschaos filename:makefile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"litmus_103\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e431a1-ed56-400b-9e85-3ce90a64d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"kube-monkey filename:configmap extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"kube_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804808c8-c6e1-4a33-8dfd-befc583ce51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"kube-monkey filename:values extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"kube_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6848a7fa-dfb1-4719-a6ec-1e9408d3896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaostoolkit extension:json size:0..49999\n",
      "Page 1: 100 results\n",
      "Page 2: 23 results\n",
      "Fetching details for fla-sil/PyTorrent\n",
      "Fetching details for sola-st/PythonTypeAnnotationStudy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m repo \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepository\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching details for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mowner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m repo_details \u001b[38;5;241m=\u001b[39m fetch_repository_details(owner, repo)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_details:\n\u001b[0;32m    141\u001b[0m     repositories\u001b[38;5;241m.\u001b[39mappend(repo_details)\n",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m, in \u001b[0;36mfetch_repository_details\u001b[1;34m(owner, repo)\u001b[0m\n\u001b[0;32m     82\u001b[0m contributors_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(contributors_url, headers\u001b[38;5;241m=\u001b[39mHEADERS)\n\u001b[0;32m     83\u001b[0m contributors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(contributors_response\u001b[38;5;241m.\u001b[39mjson()) \u001b[38;5;28;01mif\u001b[39;00m contributors_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 85\u001b[0m total_commits \u001b[38;5;241m=\u001b[39m fetch_total_commits(owner, repo)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepositoryID\u001b[39m\u001b[38;5;124m\"\u001b[39m: repo_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepositoryName\u001b[39m\u001b[38;5;124m\"\u001b[39m: repo_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumberOfCommits\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_commits\n\u001b[0;32m    106\u001b[0m }\n",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m, in \u001b[0;36mfetch_total_commits\u001b[1;34m(owner, repo)\u001b[0m\n\u001b[0;32m     43\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     46\u001b[0m         commits_url, \n\u001b[0;32m     47\u001b[0m         headers\u001b[38;5;241m=\u001b[39mHEADERS, \n\u001b[0;32m     48\u001b[0m         params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_page\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: page}\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m     51\u001b[0m         reset_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-RateLimit-Reset\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m60\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaostoolkit extension:json\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"kube_103\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1ffb9-fe62-4844-be95-790499f8a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:pumba extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"pumba_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8b90c-4003-44c1-8f88-ebbed0fa60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"pumba filename:makefile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"pumba_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36829ad9-f401-4cd7-b2f3-2f1bdc01b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:pumba extension:sh\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"pumba_103\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75435802-8805-4124-a90d-bbe86e3dd38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaostoolkit filename:actions extension:py\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_toolkit_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d698ca-4810-4b18-87e4-6a3ec0f4932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaosmonkey extension:yaml size:0..49999\n",
      "Page 1: 100 results\n",
      "Page 2: 100 results\n",
      "Page 3: 12 results\n",
      "Fetching details for kubernetes/perf-tests\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for redhat-france-sa/rhforum-2019-quotegame\n",
      "Fetching details for massix/chaos-monkey\n",
      "Fetching details for Kostov6/Chaosmonkey\n",
      "Fetching details for deckhouse/deckhouse\n",
      "Fetching details for kangelos/devops_tools\n",
      "Fetching details for networknt/model-config\n",
      "Fetching details for shinesolutions/aem-aws-stack-builder\n",
      "Fetching details for aws/aws-k8s-tester\n",
      "Fetching details for kubernetes/test-infra\n",
      "Fetching details for chaosiq/demos\n",
      "Fetching details for deideirui/github-trending-crawler\n",
      "Fetching details for paul-hammant/mcmm\n",
      "Fetching details for ocadotechnology/kubernetes-chaosmonkey\n",
      "Fetching details for Saborni/Saborni-s-POCs\n",
      "Fetching details for redhat-na-ssa/demo-ai-gitops-catalog\n",
      "Fetching details for aflaxman/vivarium_dla\n",
      "Fetching details for mvdkleijn/homedash\n",
      "Fetching details for nilo19/kubemark-performance-tests-on-azure\n",
      "Fetching details for patrickmx/mailhog-operator\n",
      "Fetching details for haew0nsh1n/spring-cloud-k8s-config\n",
      "Fetching details for gridgentoo/uber-kubernetes-test-infra\n",
      "Fetching details for ryanjbaxter/s1-2021-config\n",
      "Fetching details for kubernetes/perf-tests\n",
      "Fetching details for ryanjbaxter/asae-demo\n",
      "Fetching details for FilipKatulski/k8s-performance-tests\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for kubernetes/perf-tests\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for junaidk/podchaosmonkey\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for mmorejon/podchaosmonkey\n",
      "Fetching details for andy-mcgrath/podchaosmonkey\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for onepiecelover/perf-tests\n",
      "Fetching details for gridgentoo/deckhouse\n",
      "Fetching details for mmorejon/podchaosmonkey\n",
      "Fetching details for AntsInMyEy3sJohnson/blog-examples\n",
      "Fetching details for Training360/javax-spcl2-public\n",
      "Fetching details for deideirui/github-trending-crawler\n",
      "Fetching details for deideirui/github-trending-crawler\n",
      "Fetching details for shinesolutions/aem-aws-stack-builder\n",
      "Fetching details for shinesolutions/aem-aws-stack-builder\n",
      "Fetching details for AntsInMyEy3sJohnson/blog-examples\n",
      "Fetching details for kangelos/devops_tools\n",
      "Fetching details for armory/quick-spin-k8s\n",
      "Fetching details for gridgentoo/deckhouse\n",
      "Fetching details for junaidk/podchaosmonkey\n",
      "Fetching details for Kostov6/Chaosmonkey\n",
      "Fetching details for kangelos/devops_tools\n",
      "Fetching details for shinesolutions/aem-aws-stack-builder\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for apache/camel-kamelets-examples\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for atomix/chaos-controller\n",
      "Fetching details for shinesolutions/aem-aws-stack-builder\n",
      "Fetching details for AntsInMyEy3sJohnson/blog-examples\n",
      "Fetching details for mmorejon/podchaosmonkey\n",
      "Fetching details for perithompson/podchaosmonkey\n",
      "Fetching details for FilipKatulski/k8s-performance-tests\n",
      "Fetching details for startxfr/helm-repository\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosmonkey extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_mon\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f102d-291f-45a5-9ccd-58201c9e4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaostoolkit filename:dockerfile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaos_toolkit_103\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac02191-c9e3-462b-94bf-90ed0bd4891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"filename:powerfulseal extension:yml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"powerfulseal_101\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6305b-8a66-4c31-b62d-376ee52be02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: powerfulseal filename:dockerfile size:0..49999\n",
      "Page 1: 9 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for essobi/dockerfiles\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for dgzlopes/powerfulseal-agent\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: powerfulseal filename:dockerfile size:50000..99999\n",
      "Page 1: 1 results\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: powerfulseal filename:dockerfile size:100000..149999\n",
      "Searching with query: powerfulseal filename:dockerfile size:150000..199999\n",
      "Page 1: 1 results\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: powerfulseal filename:dockerfile size:200000..249999\n",
      "Searching with query: powerfulseal filename:dockerfile size:250000..299999\n",
      "Searching with query: powerfulseal filename:dockerfile size:300000..349999\n",
      "Rate limit exceeded. Waiting for 18 seconds...\n",
      "Searching with query: powerfulseal filename:dockerfile size:350000..399999\n",
      "Searching with query: powerfulseal filename:dockerfile size:400000..449999\n",
      "Searching with query: powerfulseal filename:dockerfile size:450000..499999\n",
      "Searching with query: powerfulseal filename:dockerfile size:500000..549999\n",
      "Searching with query: powerfulseal filename:dockerfile size:550000..599999\n",
      "Searching with query: powerfulseal filename:dockerfile size:600000..649999\n",
      "Searching with query: powerfulseal filename:dockerfile size:650000..699999\n",
      "Searching with query: powerfulseal filename:dockerfile size:700000..749999\n",
      "Searching with query: powerfulseal filename:dockerfile size:750000..799999\n",
      "Rate limit exceeded. Waiting for 59 seconds...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"powerfulseal filename:dockerfile\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"powerfulseal_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74064e2-5901-487c-8eac-d6d4ee9e56ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaosblade extension:yaml size:0..49999\n",
      "Page 1: 100 results\n",
      "Page 2: 89 results\n",
      "Fetching details for opsre/awesome-ops\n",
      "Fetching details for chaosblade-io/chaosblade-operator\n",
      "Fetching details for 0x727/FingerprintHub\n",
      "Fetching details for kubevela/catalog\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "#query_base = \"pumba filename:pumba extension:json\"\n",
    "#query_base = \"toxiproxy extension:py\"\n",
    "\n",
    "query_base = \"chaosblade extension:yaml\"\n",
    "\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"Toxiproxy_101_yml\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fe91b-06f1-474d-a30f-5898a5a6793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaoskube filename:chart extension:yaml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"chaoskube_102\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e902469-c1c5-4d8f-b948-1c3b20a2b2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: chaosmonkey extension:toml size:0..49999\n",
      "Page 1: 2 results\n",
      "Fetching details for Young-ook/terraform-aws-spinnaker\n",
      "Fetching details for codecentric/chaos-monkey-spring-boot-demo\n",
      "Searching with query: chaosmonkey extension:toml size:50000..99999\n",
      "Searching with query: chaosmonkey extension:toml size:100000..149999\n",
      "Searching with query: chaosmonkey extension:toml size:150000..199999\n",
      "Searching with query: chaosmonkey extension:toml size:200000..249999\n",
      "Searching with query: chaosmonkey extension:toml size:250000..299999\n",
      "Searching with query: chaosmonkey extension:toml size:300000..349999\n",
      "Searching with query: chaosmonkey extension:toml size:350000..399999\n",
      "Searching with query: chaosmonkey extension:toml size:400000..449999\n",
      "Rate limit exceeded. Waiting for 51 seconds...\n",
      "Searching with query: chaosmonkey extension:toml size:450000..499999\n",
      "Searching with query: chaosmonkey extension:toml size:500000..549999\n",
      "Searching with query: chaosmonkey extension:toml size:550000..599999\n",
      "Searching with query: chaosmonkey extension:toml size:600000..649999\n",
      "Searching with query: chaosmonkey extension:toml size:650000..699999\n",
      "Searching with query: chaosmonkey extension:toml size:700000..749999\n",
      "Searching with query: chaosmonkey extension:toml size:750000..799999\n",
      "Searching with query: chaosmonkey extension:toml size:800000..849999\n",
      "Searching with query: chaosmonkey extension:toml size:850000..899999\n",
      "Rate limit exceeded. Waiting for 59 seconds...\n",
      "Searching with query: chaosmonkey extension:toml size:900000..949999\n",
      "Searching with query: chaosmonkey extension:toml size:950000..999999\n",
      "Data saved to sawert\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Define size ranges for partitioning\n",
    "def create_size_ranges(min_size, max_size, step):\n",
    "    ranges = []\n",
    "    current_min = min_size\n",
    "    while current_min < max_size:\n",
    "        current_max = current_min + step - 1\n",
    "        if current_max > max_size:\n",
    "            current_max = max_size\n",
    "        ranges.append((current_min, current_max))\n",
    "        current_min = current_max + 1\n",
    "    return ranges\n",
    "\n",
    "# Main process\n",
    "min_size = 0\n",
    "max_size = 1000000\n",
    "size_step = 50000\n",
    "size_ranges = create_size_ranges(min_size, max_size, size_step)\n",
    "\n",
    "query_base = \"chaosmonkey extension:toml\"\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for min_size, max_size in size_ranges:\n",
    "    query = f\"{query_base} size:{min_size}..{max_size}\"\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"sawert\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eecc22d-6921-4cde-8678-2f5941bf49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\20236044\\appdata\\local\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f63e09-4453-4aed-9c1b-d7eef5e885a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: litmus filename:makefile\n",
      "Page 1: 100 results\n",
      "Page 2: 100 results\n",
      "Page 3: 100 results\n",
      "Page 4: 22 results\n",
      "Fetching details for NetBSD/pkgsrc\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to check rate limits\n",
    "def check_rate_limit():\n",
    "    url = f\"{GITHUB_API_URL}/rate_limit\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        rate_limit_data = response.json()\n",
    "        remaining = rate_limit_data['rate']['remaining']\n",
    "        reset_time = rate_limit_data['rate']['reset']\n",
    "        print(f\"Rate limit remaining: {remaining}. Reset at {time.ctime(reset_time)}\")\n",
    "    else:\n",
    "        print(\"Failed to fetch rate limit status.\")\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch repository details with retries\n",
    "def fetch_repository_details(owner, repo, retries=3):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()\n",
    "\n",
    "            # Fetch languages\n",
    "            languages_url = f\"{url}/languages\"\n",
    "            languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "            all_languages = \", \".join(languages_response.json().keys()) if languages_response.status_code == 200 else None\n",
    "\n",
    "            # Fetch contributors\n",
    "            contributors_url = f\"{url}/contributors\"\n",
    "            contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "            contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "            # Fetch total commits\n",
    "            total_commits = fetch_total_commits(owner, repo)\n",
    "\n",
    "            return {\n",
    "                \"repositoryID\": repo_data.get(\"id\"),\n",
    "                \"repositoryName\": repo_data.get(\"name\"),\n",
    "                \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "                \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "                \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "                \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "                \"URL\": repo_data.get(\"html_url\"),\n",
    "                \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "                \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "                \"allLanguages\": all_languages,\n",
    "                \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "                \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "                \"contributors\": contributors,\n",
    "                \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "                \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "                \"forks\": repo_data.get(\"forks_count\"),\n",
    "                \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "                \"numberOfCommits\": total_commits\n",
    "            }\n",
    "        elif response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Failed to fetch repo details for {owner}/{repo} - Status Code: {response.status_code}, Attempt: {attempt + 1}\")\n",
    "    return None\n",
    "\n",
    "# Main process\n",
    "# query_list = [\n",
    "#     \"query1\",  # Replace with your GitHub search queries\n",
    "#     \"query2\",\n",
    "#     \"query3\"\n",
    "# ]\n",
    "\n",
    "query_list = [\"litmus filename:makefile\"]\n",
    "repositories = []\n",
    "\n",
    "for query in query_list:\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = \"Litmus_101.xlsx\"  # Ensure the file has an .xlsx extension\n",
    "try:\n",
    "    df = pd.DataFrame(repositories)\n",
    "    df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save Excel file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8414c3d6-d306-46e0-a0ac-5b1d9eb55aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: powerfulseal filename:dockerfile\n",
      "Page 1: 11 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for essobi/dockerfiles\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for dgzlopes/powerfulseal-agent\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: filename:powerfulseal extension:yml\n",
      "Page 1: 5 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for seeker89/chaos-engineering-book\n",
      "Fetching details for litmuschaos/litmus-ansible\n",
      "Fetching details for uditgaurav/test-litmus-ansible\n",
      "Fetching details for uditgaurav/litmus-run-e2e\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml\n",
      "Page 1: 12 results\n",
      "Fetching details for aws-samples/kubernetes-for-java-developers\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for djkormo/k8s-AKS-primer\n",
      "Fetching details for ajgrande924/insight-project\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for dneff/ts2019-challenges\n",
      "Fetching details for ERNI-Academy/poc-chaos-preformance-resiliency-testing\n",
      "Fetching details for Rabosa616/NetConf2019_MicroservicesGeneratingChaos\n",
      "Fetching details for OctoConsulting/fedhipster-iac\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for shanaka-sky/k8s-java\n",
      "Fetching details for cloudbees-pyang/kubernetes-java\n",
      "Searching with query: apachegriffin extension:.yml\n",
      "Page 1: 14 results\n",
      "Fetching details for apache/griffin\n",
      "Fetching details for tanxinzheng/docker-image-demo\n",
      "Fetching details for apache/griffin\n",
      "Fetching details for dershov173/Spark-Streaming-Project\n",
      "Fetching details for zcswl7961/apache-griffin-expand\n",
      "Fetching details for ep-infosec/33_apache_griffin\n",
      "Fetching details for 15802519394/griffin\n",
      "Fetching details for rohan-flutterint/griffin\n",
      "Fetching details for rohan-flutterint/griffin-scala\n",
      "Fetching details for zcswl7961/apache-griffin-expand\n",
      "Fetching details for ep-infosec/33_apache_griffin\n",
      "Fetching details for 15802519394/griffin\n",
      "Fetching details for rohan-flutterint/griffin\n",
      "Fetching details for rohan-flutterint/griffin-scala\n",
      "Searching with query: chaos-mesh filename:config extension:yml\n",
      "Page 1: 9 results\n",
      "Fetching details for FudanSELab/train-ticket-experiment\n",
      "Fetching details for FudanSELab/train-ticket-experiment\n",
      "Fetching details for FudanSELab/train-ticket-experiment\n",
      "Fetching details for chaos-mesh/chaos-mesh\n",
      "Fetching details for um33/github-actions-trends-analysis\n",
      "Fetching details for ztk1996/TF-RCA\n",
      "Fetching details for ztk1996/TF-RCA\n",
      "Fetching details for ztk1996/TF-RCA\n",
      "Fetching details for wngjia/ece573-offline\n",
      "Searching with query: powerfulseal filename:dockerfile\n",
      "Page 1: 11 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for essobi/dockerfiles\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for dgzlopes/powerfulseal-agent\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Data saved to All\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Main process\n",
    "# Load queries from Excel file\n",
    "query_file = \"queries1.xlsx\"  # Replace with your Excel file\n",
    "df_queries = pd.read_excel(query_file)\n",
    "query_list = df_queries['query']  # Assuming the Excel column header is 'query'\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for query in query_list:\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(repositories)\n",
    "output_file = \"All\"\n",
    "df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e57ca5-ef5e-4d27-8225-c348b5430487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: powerfulseal filename:dockerfile\n",
      "Page 1: 11 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for essobi/dockerfiles\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for dgzlopes/powerfulseal-agent\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Searching with query: filename:powerfulseal extension:yml\n",
      "Page 1: 5 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for seeker89/chaos-engineering-book\n",
      "Fetching details for litmuschaos/litmus-ansible\n",
      "Fetching details for uditgaurav/test-litmus-ansible\n",
      "Fetching details for uditgaurav/litmus-run-e2e\n",
      "Searching with query: kube-monkey filename:configmap extension:yaml\n",
      "Page 1: 12 results\n",
      "Fetching details for aws-samples/kubernetes-for-java-developers\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for djkormo/k8s-AKS-primer\n",
      "Fetching details for ajgrande924/insight-project\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for dneff/ts2019-challenges\n",
      "Fetching details for ERNI-Academy/poc-chaos-preformance-resiliency-testing\n",
      "Fetching details for Rabosa616/NetConf2019_MicroservicesGeneratingChaos\n",
      "Fetching details for OctoConsulting/fedhipster-iac\n",
      "Fetching details for asobti/kube-monkey\n",
      "Fetching details for shanaka-sky/k8s-java\n",
      "Fetching details for cloudbees-pyang/kubernetes-java\n",
      "Searching with query: apachegriffin extension:.yml\n",
      "Page 1: 14 results\n",
      "Fetching details for apache/griffin\n",
      "Fetching details for tanxinzheng/docker-image-demo\n",
      "Fetching details for apache/griffin\n",
      "Fetching details for dershov173/Spark-Streaming-Project\n",
      "Fetching details for zcswl7961/apache-griffin-expand\n",
      "Fetching details for ep-infosec/33_apache_griffin\n",
      "Fetching details for 15802519394/griffin\n",
      "Fetching details for rohan-flutterint/griffin\n",
      "Fetching details for rohan-flutterint/griffin-scala\n",
      "Fetching details for zcswl7961/apache-griffin-expand\n",
      "Fetching details for ep-infosec/33_apache_griffin\n",
      "Fetching details for 15802519394/griffin\n",
      "Fetching details for rohan-flutterint/griffin\n",
      "Fetching details for rohan-flutterint/griffin-scala\n",
      "Searching with query: chaos-mesh filename:config extension:yml\n",
      "Page 1: 9 results\n",
      "Fetching details for FudanSELab/train-ticket-experiment\n",
      "Fetching details for FudanSELab/train-ticket-experiment\n",
      "Fetching details for FudanSELab/train-ticket-experiment\n",
      "Fetching details for chaos-mesh/chaos-mesh\n",
      "Fetching details for um33/github-actions-trends-analysis\n",
      "Fetching details for ztk1996/TF-RCA\n",
      "Fetching details for ztk1996/TF-RCA\n",
      "Fetching details for ztk1996/TF-RCA\n",
      "Fetching details for wngjia/ece573-offline\n",
      "Searching with query: powerfulseal filename:dockerfile\n",
      "Page 1: 11 results\n",
      "Fetching details for powerfulseal/powerfulseal\n",
      "Fetching details for essobi/dockerfiles\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for dgzlopes/powerfulseal-agent\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Fetching details for VladCroitoru/dockerfile_smells_project\n",
      "Fetching details for irvin-s/docker_repair\n",
      "Fetching details for Zuquim/Identifying-Logging-Practices-in-Open-Source-Python-Containerized-Application-Projects\n",
      "Data saved to ChaosToolOutput.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch total commits for a repository\n",
    "def fetch_total_commits(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url, \n",
    "            headers=HEADERS, \n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = reset_time - int(time.time())\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits\n",
    "\n",
    "# Function to fetch additional repository metadata\n",
    "def fetch_repository_details(owner, repo):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch repo details for {owner}/{repo}\")\n",
    "        return None\n",
    "    repo_data = response.json()\n",
    "\n",
    "    languages_url = f\"{url}/languages\"\n",
    "    languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "    all_languages = \", \".join(languages_response.json().keys())\n",
    "\n",
    "    contributors_url = f\"{url}/contributors\"\n",
    "    contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "    contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "    total_commits = fetch_total_commits(owner, repo)\n",
    "    \n",
    "    return {\n",
    "        \"repositoryID\": repo_data.get(\"id\"),\n",
    "        \"repositoryName\": repo_data.get(\"name\"),\n",
    "        \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "        \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "        \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "        \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "        \"URL\": repo_data.get(\"html_url\"),\n",
    "        \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "        \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "        \"allLanguages\": all_languages,\n",
    "        \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "        \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "        \"contributors\": contributors,\n",
    "        \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "        \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "        \"forks\": repo_data.get(\"forks_count\"),\n",
    "        \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "        \"numberOfCommits\": total_commits\n",
    "    }\n",
    "\n",
    "# Main process\n",
    "# Load queries from Excel file\n",
    "query_file = \"queries1.xlsx\"  # Replace with your Excel file path\n",
    "df_queries = pd.read_excel(query_file)\n",
    "query_list = df_queries['query']  \n",
    "\n",
    "repositories = []\n",
    "\n",
    "for query in query_list:\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = \"ChaosToolOutput.xlsx\"  # Ensure the file has an .xlsx extension\n",
    "try:\n",
    "    df = pd.DataFrame(repositories)\n",
    "    #print(df.head())  # Debug: check the first few rows of the DataFrame\n",
    "    df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save Excel file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c30c2-9450-4a5a-a235-2fc1baf88a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: start_hour extension:toml\n",
      "Page 1: 100 results\n",
      "Page 2: 100 results\n",
      "Page 3: 100 results\n",
      "Page 4: 100 results\n",
      "Page 5: 76 results\n",
      "Fetching details for facile-it/rabbitmq-consumer\n",
      "Fetching details for TNO/ChaProEV\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "#t = \"\"  # Replace with your GitHub t\n",
    "t = \"\"  # Replace with your GitHub t\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to check rate limits\n",
    "def check_rate_limit():\n",
    "    url = f\"{GITHUB_API_URL}/rate_limit\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        rate_limit_data = response.json()\n",
    "        remaining = rate_limit_data['rate']['remaining']\n",
    "        reset_time = rate_limit_data['rate']['reset']\n",
    "        print(f\"Rate limit remaining: {remaining}. Reset at {time.ctime(reset_time)}\")\n",
    "    else:\n",
    "        print(\"Failed to fetch rate limit status.\")\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch the first and last commit dates for a repository\n",
    "def fetch_commit_dates(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    first_commit_date = None\n",
    "    last_commit_date = None\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url,\n",
    "            headers=HEADERS,\n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        if page == 1:\n",
    "            last_commit_date = commits[0]['commit']['committer']['date']\n",
    "        first_commit_date = commits[-1]['commit']['committer']['date']\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits, first_commit_date, last_commit_date\n",
    "\n",
    "# Function to fetch repository details with retries\n",
    "def fetch_repository_details(owner, repo, retries=3):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()\n",
    "\n",
    "            # Fetch languages\n",
    "            languages_url = f\"{url}/languages\"\n",
    "            languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "            all_languages = \", \".join(languages_response.json().keys()) if languages_response.status_code == 200 else None\n",
    "\n",
    "            # Fetch contributors\n",
    "            contributors_url = f\"{url}/contributors\"\n",
    "            contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "            contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "            # Fetch total commits and commit dates\n",
    "            total_commits, first_commit_date, last_commit_date = fetch_commit_dates(owner, repo)\n",
    "\n",
    "            # Fetch watchers count using subscribers endpoint\n",
    "            subscribers_url = f\"{url}/subscribers\"\n",
    "            subscribers_response = requests.get(subscribers_url, headers=HEADERS)\n",
    "            watchers_count = len(subscribers_response.json()) if subscribers_response.status_code == 200 else 0\n",
    "\n",
    "            return {\n",
    "                \"repositoryID\": repo_data.get(\"id\"),\n",
    "                \"repositoryName\": repo_data.get(\"name\"),\n",
    "                \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "                \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "                \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "                \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "                \"URL\": repo_data.get(\"html_url\"),\n",
    "                \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "                \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "                \"allLanguages\": all_languages,\n",
    "                \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "                \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "                \"contributors\": contributors,\n",
    "                \"watchers\": watchers_count,\n",
    "                \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "                \"forks\": repo_data.get(\"forks_count\"),\n",
    "                \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "                \"numberOfCommits\": total_commits,\n",
    "                \"firstCommitDate\": first_commit_date,\n",
    "                \"lastCommitDate\": last_commit_date\n",
    "            }\n",
    "        elif response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Failed to fetch repo details for {owner}/{repo} - Status Code: {response.status_code}, Attempt: {attempt + 1}\")\n",
    "    return None\n",
    "\n",
    "# Main process\n",
    "#query_list = [\"spinnaker AND chaosmonkey in:file,path\"]\n",
    "#query_list = [\"filename:chaosmonkey extension:go\"]\n",
    "#query_list = [\"chaosmonkey migrate\"]\n",
    "#query_list = [\"spinnaker AND chaosmonkey in:file,path\"]\n",
    "#query_list = [\"filename:toxiproxy extension:json\"]\n",
    "#query_list = [\"toxiproxy in:file,path\"]\n",
    "query_list = [\"start_hour extension:toml\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for query in query_list:\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = \"chaos_mesh_1101.xlsx\"  # Ensure the file has an .xlsx extension\n",
    "try:\n",
    "    df = pd.DataFrame(repositories)\n",
    "    df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b85471-6503-46f8-8b6f-3fc3e32a07c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with query: filename:netflix.spinnaker\n",
      "Page 1: 13 results\n",
      "Fetching details for GitHubSecurityLab/CodeQL-Community-Packs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 167\u001b[0m\n\u001b[0;32m    165\u001b[0m repo \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepository\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching details for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mowner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 167\u001b[0m repo_details \u001b[38;5;241m=\u001b[39m fetch_repository_details(owner, repo)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_details:\n\u001b[0;32m    169\u001b[0m     repositories\u001b[38;5;241m.\u001b[39mappend(repo_details)\n",
      "Cell \u001b[1;32mIn[1], line 103\u001b[0m, in \u001b[0;36mfetch_repository_details\u001b[1;34m(owner, repo, retries)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Fetch contributors\u001b[39;00m\n\u001b[0;32m    102\u001b[0m contributors_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/contributors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m contributors_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(contributors_url, headers\u001b[38;5;241m=\u001b[39mHEADERS)\n\u001b[0;32m    104\u001b[0m contributors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(contributors_response\u001b[38;5;241m.\u001b[39mjson()) \u001b[38;5;28;01mif\u001b[39;00m contributors_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Fetch total commits and commit dates\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1097\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1101\u001b[0m         (\n\u001b[0;32m   1102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconn\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1108\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_time_off:\n\u001b[0;32m    634\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    635\u001b[0m         (\n\u001b[0;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem time is way off (before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRECENT_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This will probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[0;32m    640\u001b[0m     )\n\u001b[1;32m--> 642\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[0;32m    643\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    644\u001b[0m     cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[0;32m    645\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[0;32m    646\u001b[0m     ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[0;32m    647\u001b[0m     ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[0;32m    648\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[0;32m    649\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[0;32m    650\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[0;32m    651\u001b[0m     cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[0;32m    652\u001b[0m     key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[0;32m    653\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[0;32m    654\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    655\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[0;32m    656\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    657\u001b[0m     assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[0;32m    658\u001b[0m     assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[0;32m    659\u001b[0m )\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39mis_verified\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:783\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[0;32m    781\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 783\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    784\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    785\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[0;32m    786\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[0;32m    787\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[0;32m    788\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[0;32m    789\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[0;32m    790\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[0;32m    791\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    792\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    793\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    794\u001b[0m )\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:446\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 446\u001b[0m         context\u001b[38;5;241m.\u001b[39mload_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# GitHub API and authentication\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "#t = \"\"  # Replace with your GitHub t\n",
    "#t = \"\"  # Replace with your GitHub t\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {t}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Function to check rate limits\n",
    "def check_rate_limit():\n",
    "    url = f\"{GITHUB_API_URL}/rate_limit\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        rate_limit_data = response.json()\n",
    "        remaining = rate_limit_data['rate']['remaining']\n",
    "        reset_time = rate_limit_data['rate']['reset']\n",
    "        print(f\"Rate limit remaining: {remaining}. Reset at {time.ctime(reset_time)}\")\n",
    "    else:\n",
    "        print(\"Failed to fetch rate limit status.\")\n",
    "\n",
    "# Function to search GitHub with pagination\n",
    "def search_github(query, per_page=100, max_pages=10):\n",
    "    all_results = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{GITHUB_API_URL}/search/code\"\n",
    "        params = {\"q\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        all_results.extend(items)\n",
    "        print(f\"Page {page}: {len(items)} results\")\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch the first and last commit dates for a repository\n",
    "def fetch_commit_dates(owner, repo):\n",
    "    commits_url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}/commits\"\n",
    "    first_commit_date = None\n",
    "    last_commit_date = None\n",
    "    total_commits = 0\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            commits_url,\n",
    "            headers=HEADERS,\n",
    "            params={\"per_page\": 100, \"page\": page}\n",
    "        )\n",
    "        if response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch commits: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        \n",
    "        if page == 1:\n",
    "            last_commit_date = commits[0]['commit']['committer']['date']\n",
    "        first_commit_date = commits[-1]['commit']['committer']['date']\n",
    "        \n",
    "        total_commits += len(commits)\n",
    "        page += 1\n",
    "    return total_commits, first_commit_date, last_commit_date\n",
    "\n",
    "# Function to fetch repository details with retries\n",
    "def fetch_repository_details(owner, repo, retries=3):\n",
    "    url = f\"{GITHUB_API_URL}/repos/{owner}/{repo}\"\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()\n",
    "\n",
    "            # Fetch languages\n",
    "            languages_url = f\"{url}/languages\"\n",
    "            languages_response = requests.get(languages_url, headers=HEADERS)\n",
    "            all_languages = \", \".join(languages_response.json().keys()) if languages_response.status_code == 200 else None\n",
    "\n",
    "            # Fetch contributors\n",
    "            contributors_url = f\"{url}/contributors\"\n",
    "            contributors_response = requests.get(contributors_url, headers=HEADERS)\n",
    "            contributors = len(contributors_response.json()) if contributors_response.status_code == 200 else 0\n",
    "\n",
    "            # Fetch total commits and commit dates\n",
    "            total_commits, first_commit_date, last_commit_date = fetch_commit_dates(owner, repo)\n",
    "\n",
    "            # Fetch watchers count using subscribers endpoint\n",
    "            subscribers_url = f\"{url}/subscribers\"\n",
    "            subscribers_response = requests.get(subscribers_url, headers=HEADERS)\n",
    "            watchers_count = len(subscribers_response.json()) if subscribers_response.status_code == 200 else 0\n",
    "\n",
    "            return {\n",
    "                \"repositoryID\": repo_data.get(\"id\"),\n",
    "                \"repositoryName\": repo_data.get(\"name\"),\n",
    "                \"ownerLogin\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "                \"ownerType\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "                \"repositoryDescription\": repo_data.get(\"description\"),\n",
    "                \"topics\": \", \".join(repo_data.get(\"topics\", [])),\n",
    "                \"URL\": repo_data.get(\"html_url\"),\n",
    "                \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "                \"primaryLanguage\": repo_data.get(\"language\"),\n",
    "                \"allLanguages\": all_languages,\n",
    "                \"repositoryCreation\": repo_data.get(\"created_at\"),\n",
    "                \"repositoryLastUpdate\": repo_data.get(\"updated_at\"),\n",
    "                \"contributors\": contributors,\n",
    "                \"watchers\": watchers_count,\n",
    "                \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "                \"forks\": repo_data.get(\"forks_count\"),\n",
    "                \"issues\": repo_data.get(\"open_issues_count\"),\n",
    "                \"numberOfCommits\": total_commits,\n",
    "                \"firstCommitDate\": first_commit_date,\n",
    "                \"lastCommitDate\": last_commit_date\n",
    "            }\n",
    "        elif response.status_code == 403:\n",
    "            reset_time = int(response.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait_time = max(reset_time - int(time.time()), 0)\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Failed to fetch repo details for {owner}/{repo} - Status Code: {response.status_code}, Attempt: {attempt + 1}\")\n",
    "    return None\n",
    "\n",
    "# Main process\n",
    "#query_list = [\"spinnaker AND chaosmonkey in:file,path\"]\n",
    "#query_list = [\"filename:chaosmonkey extension:go\"]\n",
    "#query_list = [\"chaosmonkey migrate\"]\n",
    "#query_list = [\"spinnaker AND chaosmonkey in:file,path\"]\n",
    "#query_list = [\"filename:toxiproxy extension:json\"]\n",
    "#query_list = [\"toxiproxy in:file,path\"]\n",
    "query_list = [\"filename:netflix.spinnaker\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "repositories = []\n",
    "\n",
    "for query in query_list:\n",
    "    print(f\"Searching with query: {query}\")\n",
    "    results = search_github(query, per_page=100, max_pages=10)\n",
    "    \n",
    "    for item in results:\n",
    "        owner = item['repository']['owner']['login']\n",
    "        repo = item['repository']['name']\n",
    "        print(f\"Fetching details for {owner}/{repo}\")\n",
    "        repo_details = fetch_repository_details(owner, repo)\n",
    "        if repo_details:\n",
    "            repositories.append(repo_details)\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_file = \"chaosmonnn.xlsx\"  # Ensure the file has an .xlsx extension\n",
    "try:\n",
    "    df = pd.DataFrame(repositories)\n",
    "    df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save Excel file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
